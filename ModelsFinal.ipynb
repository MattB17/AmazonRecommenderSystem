{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Models\n",
    "\n",
    "In this notebook we look at some final candidate models and assess their performance on a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "random.seed(17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "\n",
    "We will subsample 50,000 of the 200,000 records for training by sampling 25% of the data from each of the five music categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = None\n",
    "with open(os.path.join('data', 'train.json'), 'r') as train_file:\n",
    "    data = [json.loads(row) for row in train_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame(data).drop(columns=['image'])\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>category</th>\n",
       "      <th>price</th>\n",
       "      <th>itemID</th>\n",
       "      <th>reviewHash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>08 24, 2010</td>\n",
       "      <td>u04428712</td>\n",
       "      <td>So is Katy Perry's new album \"Teenage Dream\" c...</td>\n",
       "      <td>Amazing that I Actually Bought This...More Ama...</td>\n",
       "      <td>1282608000</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$35.93</td>\n",
       "      <td>p70761125</td>\n",
       "      <td>85559980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>09 7, 2015</td>\n",
       "      <td>u07624734</td>\n",
       "      <td>o.k.</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1441584000</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$14.99</td>\n",
       "      <td>p78489708</td>\n",
       "      <td>23609516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.0</td>\n",
       "      <td>11 16, 2006</td>\n",
       "      <td>u40719083</td>\n",
       "      <td>Katchen's performace throughout this collectio...</td>\n",
       "      <td>Superb interpretations by Katchen</td>\n",
       "      <td>1163635200</td>\n",
       "      <td>Classical</td>\n",
       "      <td>$31.04</td>\n",
       "      <td>p63362921</td>\n",
       "      <td>40704096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.0</td>\n",
       "      <td>08 12, 2016</td>\n",
       "      <td>u66989964</td>\n",
       "      <td>She is my fave!</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1470960000</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$11.57</td>\n",
       "      <td>p83852395</td>\n",
       "      <td>05580669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.0</td>\n",
       "      <td>10 31, 2016</td>\n",
       "      <td>u34469863</td>\n",
       "      <td>Really Great</td>\n",
       "      <td>Really Great</td>\n",
       "      <td>1477872000</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$7.98</td>\n",
       "      <td>p51677500</td>\n",
       "      <td>28306462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199985</th>\n",
       "      <td>4.0</td>\n",
       "      <td>12 4, 2014</td>\n",
       "      <td>u65640560</td>\n",
       "      <td>Had to have in my collection..</td>\n",
       "      <td>Four Stars</td>\n",
       "      <td>1417651200</td>\n",
       "      <td>Jazz</td>\n",
       "      <td>$2.15</td>\n",
       "      <td>p76891834</td>\n",
       "      <td>67429536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199989</th>\n",
       "      <td>5.0</td>\n",
       "      <td>12 28, 2010</td>\n",
       "      <td>u92755262</td>\n",
       "      <td>I first heard Taking Back Sunday this summer w...</td>\n",
       "      <td>Great CD</td>\n",
       "      <td>1293494400</td>\n",
       "      <td>Alternative Rock</td>\n",
       "      <td>$12.97</td>\n",
       "      <td>p50896575</td>\n",
       "      <td>46693447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199994</th>\n",
       "      <td>4.0</td>\n",
       "      <td>09 3, 2014</td>\n",
       "      <td>u97165206</td>\n",
       "      <td>I love this lp this album is really good\\none ...</td>\n",
       "      <td>great song</td>\n",
       "      <td>1409702400</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$49.99</td>\n",
       "      <td>p58216418</td>\n",
       "      <td>07085315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>4.0</td>\n",
       "      <td>05 1, 2004</td>\n",
       "      <td>u68902609</td>\n",
       "      <td>With this, Mariah's third album, Mariah proved...</td>\n",
       "      <td>Well Done Mariah! You Show 'Em!</td>\n",
       "      <td>1083369600</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$7.98</td>\n",
       "      <td>p84118731</td>\n",
       "      <td>35077372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>5.0</td>\n",
       "      <td>03 20, 2016</td>\n",
       "      <td>u04485604</td>\n",
       "      <td>Get it now ! Right now ! I am partial. I am a ...</td>\n",
       "      <td>Our Poet</td>\n",
       "      <td>1458432000</td>\n",
       "      <td>Alternative Rock</td>\n",
       "      <td>$11.07</td>\n",
       "      <td>p19134748</td>\n",
       "      <td>27463540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50001 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        overall   reviewTime reviewerID  \\\n",
       "0           4.0  08 24, 2010  u04428712   \n",
       "5           5.0   09 7, 2015  u07624734   \n",
       "11          5.0  11 16, 2006  u40719083   \n",
       "14          5.0  08 12, 2016  u66989964   \n",
       "21          5.0  10 31, 2016  u34469863   \n",
       "...         ...          ...        ...   \n",
       "199985      4.0   12 4, 2014  u65640560   \n",
       "199989      5.0  12 28, 2010  u92755262   \n",
       "199994      4.0   09 3, 2014  u97165206   \n",
       "199995      4.0   05 1, 2004  u68902609   \n",
       "199998      5.0  03 20, 2016  u04485604   \n",
       "\n",
       "                                               reviewText  \\\n",
       "0       So is Katy Perry's new album \"Teenage Dream\" c...   \n",
       "5                                                    o.k.   \n",
       "11      Katchen's performace throughout this collectio...   \n",
       "14                                        She is my fave!   \n",
       "21                                           Really Great   \n",
       "...                                                   ...   \n",
       "199985                     Had to have in my collection..   \n",
       "199989  I first heard Taking Back Sunday this summer w...   \n",
       "199994  I love this lp this album is really good\\none ...   \n",
       "199995  With this, Mariah's third album, Mariah proved...   \n",
       "199998  Get it now ! Right now ! I am partial. I am a ...   \n",
       "\n",
       "                                                  summary  unixReviewTime  \\\n",
       "0       Amazing that I Actually Bought This...More Ama...      1282608000   \n",
       "5                                              Five Stars      1441584000   \n",
       "11                      Superb interpretations by Katchen      1163635200   \n",
       "14                                             Five Stars      1470960000   \n",
       "21                                           Really Great      1477872000   \n",
       "...                                                   ...             ...   \n",
       "199985                                         Four Stars      1417651200   \n",
       "199989                                           Great CD      1293494400   \n",
       "199994                                         great song      1409702400   \n",
       "199995                    Well Done Mariah! You Show 'Em!      1083369600   \n",
       "199998                                           Our Poet      1458432000   \n",
       "\n",
       "                category   price     itemID reviewHash  \n",
       "0                    Pop  $35.93  p70761125   85559980  \n",
       "5                    Pop  $14.99  p78489708   23609516  \n",
       "11             Classical  $31.04  p63362921   40704096  \n",
       "14                   Pop  $11.57  p83852395   05580669  \n",
       "21                   Pop   $7.98  p51677500   28306462  \n",
       "...                  ...     ...        ...        ...  \n",
       "199985              Jazz   $2.15  p76891834   67429536  \n",
       "199989  Alternative Rock  $12.97  p50896575   46693447  \n",
       "199994               Pop  $49.99  p58216418   07085315  \n",
       "199995               Pop   $7.98  p84118731   35077372  \n",
       "199998  Alternative Rock  $11.07  p19134748   27463540  \n",
       "\n",
       "[50001 rows x 10 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = data_df['category'].unique()\n",
    "dfs = []\n",
    "for category in categories:\n",
    "    dfs.append(data_df[data_df['category'] == category].sample(frac=0.25))\n",
    "data_df = pd.concat(dfs, axis=0)\n",
    "data_df = data_df.sort_index()\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing\n",
    "\n",
    "We apply feature cleaning as prototyped before and then split into a training and validation set, ensuring that the proportion of data points in training vs validation is consistent for each music category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_price(price):\n",
    "    \"\"\"Trims `price` to remove the $ sign.\n",
    "    \n",
    "    If the price variable does not have the format $x.xx\n",
    "    then the empty string is returned.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    price: str\n",
    "        A string representing a price.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A string representing `price` but with the $ sign removed,\n",
    "        or the empty string if `price` does not have the correct\n",
    "        format.\n",
    "    \n",
    "    \"\"\"\n",
    "    if (not pd.isnull(price) and isinstance(price, str) and\n",
    "        len(price) > 0 and price[0] == '$'):\n",
    "        return price[1:]\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-52-d3ba8e2e1b50>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_df['cleanedPrice'] = data_df['cleanedPrice'].astype('float')\n",
      "<ipython-input-52-d3ba8e2e1b50>:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_df['fixedReviewText'] = np.where(pd.isnull(data_df['reviewText']), \"\", data_df['reviewText'])\n",
      "<ipython-input-52-d3ba8e2e1b50>:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_df['fixedSummary'] = np.where(pd.isnull(data_df['summary']), \"\", data_df['summary'])\n",
      "<ipython-input-52-d3ba8e2e1b50>:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_df['fullReviewText'] = data_df['fixedSummary'] + \" \" + data_df['fixedReviewText']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>category</th>\n",
       "      <th>price</th>\n",
       "      <th>itemID</th>\n",
       "      <th>reviewHash</th>\n",
       "      <th>...</th>\n",
       "      <th>reviewHour</th>\n",
       "      <th>reviewMonthYear</th>\n",
       "      <th>cleanedPrice</th>\n",
       "      <th>fullReviewText</th>\n",
       "      <th>isPop</th>\n",
       "      <th>isClassical</th>\n",
       "      <th>isDanceElectronic</th>\n",
       "      <th>isAlternativeRock</th>\n",
       "      <th>isJazz</th>\n",
       "      <th>reviewWordCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>08 24, 2010</td>\n",
       "      <td>u04428712</td>\n",
       "      <td>So is Katy Perry's new album \"Teenage Dream\" c...</td>\n",
       "      <td>Amazing that I Actually Bought This...More Ama...</td>\n",
       "      <td>1282608000</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$35.93</td>\n",
       "      <td>p70761125</td>\n",
       "      <td>85559980</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>2010-08</td>\n",
       "      <td>35.93</td>\n",
       "      <td>Amazing that I Actually Bought This...More Ama...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>09 7, 2015</td>\n",
       "      <td>u07624734</td>\n",
       "      <td>o.k.</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1441584000</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$14.99</td>\n",
       "      <td>p78489708</td>\n",
       "      <td>23609516</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>2015-09</td>\n",
       "      <td>14.99</td>\n",
       "      <td>Five Stars o.k.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.0</td>\n",
       "      <td>11 16, 2006</td>\n",
       "      <td>u40719083</td>\n",
       "      <td>Katchen's performace throughout this collectio...</td>\n",
       "      <td>Superb interpretations by Katchen</td>\n",
       "      <td>1163635200</td>\n",
       "      <td>Classical</td>\n",
       "      <td>$31.04</td>\n",
       "      <td>p63362921</td>\n",
       "      <td>40704096</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>2006-11</td>\n",
       "      <td>31.04</td>\n",
       "      <td>Superb interpretations by Katchen Katchen's pe...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.0</td>\n",
       "      <td>08 12, 2016</td>\n",
       "      <td>u66989964</td>\n",
       "      <td>She is my fave!</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1470960000</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$11.57</td>\n",
       "      <td>p83852395</td>\n",
       "      <td>05580669</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>2016-08</td>\n",
       "      <td>11.57</td>\n",
       "      <td>Five Stars She is my fave!</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.0</td>\n",
       "      <td>10 31, 2016</td>\n",
       "      <td>u34469863</td>\n",
       "      <td>Really Great</td>\n",
       "      <td>Really Great</td>\n",
       "      <td>1477872000</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$7.98</td>\n",
       "      <td>p51677500</td>\n",
       "      <td>28306462</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>7.98</td>\n",
       "      <td>Really Great Really Great</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199985</th>\n",
       "      <td>4.0</td>\n",
       "      <td>12 4, 2014</td>\n",
       "      <td>u65640560</td>\n",
       "      <td>Had to have in my collection..</td>\n",
       "      <td>Four Stars</td>\n",
       "      <td>1417651200</td>\n",
       "      <td>Jazz</td>\n",
       "      <td>$2.15</td>\n",
       "      <td>p76891834</td>\n",
       "      <td>67429536</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>2014-12</td>\n",
       "      <td>2.15</td>\n",
       "      <td>Four Stars Had to have in my collection..</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199989</th>\n",
       "      <td>5.0</td>\n",
       "      <td>12 28, 2010</td>\n",
       "      <td>u92755262</td>\n",
       "      <td>I first heard Taking Back Sunday this summer w...</td>\n",
       "      <td>Great CD</td>\n",
       "      <td>1293494400</td>\n",
       "      <td>Alternative Rock</td>\n",
       "      <td>$12.97</td>\n",
       "      <td>p50896575</td>\n",
       "      <td>46693447</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>2010-12</td>\n",
       "      <td>12.97</td>\n",
       "      <td>Great CD I first heard Taking Back Sunday this...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199994</th>\n",
       "      <td>4.0</td>\n",
       "      <td>09 3, 2014</td>\n",
       "      <td>u97165206</td>\n",
       "      <td>I love this lp this album is really good\\none ...</td>\n",
       "      <td>great song</td>\n",
       "      <td>1409702400</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$49.99</td>\n",
       "      <td>p58216418</td>\n",
       "      <td>07085315</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>2014-09</td>\n",
       "      <td>49.99</td>\n",
       "      <td>great song I love this lp this album is really...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>4.0</td>\n",
       "      <td>05 1, 2004</td>\n",
       "      <td>u68902609</td>\n",
       "      <td>With this, Mariah's third album, Mariah proved...</td>\n",
       "      <td>Well Done Mariah! You Show 'Em!</td>\n",
       "      <td>1083369600</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$7.98</td>\n",
       "      <td>p84118731</td>\n",
       "      <td>35077372</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>2004-05</td>\n",
       "      <td>7.98</td>\n",
       "      <td>Well Done Mariah! You Show 'Em! With this, Mar...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>5.0</td>\n",
       "      <td>03 20, 2016</td>\n",
       "      <td>u04485604</td>\n",
       "      <td>Get it now ! Right now ! I am partial. I am a ...</td>\n",
       "      <td>Our Poet</td>\n",
       "      <td>1458432000</td>\n",
       "      <td>Alternative Rock</td>\n",
       "      <td>$11.07</td>\n",
       "      <td>p19134748</td>\n",
       "      <td>27463540</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>2016-03</td>\n",
       "      <td>11.07</td>\n",
       "      <td>Our Poet Get it now ! Right now ! I am partial...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49285 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        overall   reviewTime reviewerID  \\\n",
       "0           4.0  08 24, 2010  u04428712   \n",
       "5           5.0   09 7, 2015  u07624734   \n",
       "11          5.0  11 16, 2006  u40719083   \n",
       "14          5.0  08 12, 2016  u66989964   \n",
       "21          5.0  10 31, 2016  u34469863   \n",
       "...         ...          ...        ...   \n",
       "199985      4.0   12 4, 2014  u65640560   \n",
       "199989      5.0  12 28, 2010  u92755262   \n",
       "199994      4.0   09 3, 2014  u97165206   \n",
       "199995      4.0   05 1, 2004  u68902609   \n",
       "199998      5.0  03 20, 2016  u04485604   \n",
       "\n",
       "                                               reviewText  \\\n",
       "0       So is Katy Perry's new album \"Teenage Dream\" c...   \n",
       "5                                                    o.k.   \n",
       "11      Katchen's performace throughout this collectio...   \n",
       "14                                        She is my fave!   \n",
       "21                                           Really Great   \n",
       "...                                                   ...   \n",
       "199985                     Had to have in my collection..   \n",
       "199989  I first heard Taking Back Sunday this summer w...   \n",
       "199994  I love this lp this album is really good\\none ...   \n",
       "199995  With this, Mariah's third album, Mariah proved...   \n",
       "199998  Get it now ! Right now ! I am partial. I am a ...   \n",
       "\n",
       "                                                  summary  unixReviewTime  \\\n",
       "0       Amazing that I Actually Bought This...More Ama...      1282608000   \n",
       "5                                              Five Stars      1441584000   \n",
       "11                      Superb interpretations by Katchen      1163635200   \n",
       "14                                             Five Stars      1470960000   \n",
       "21                                           Really Great      1477872000   \n",
       "...                                                   ...             ...   \n",
       "199985                                         Four Stars      1417651200   \n",
       "199989                                           Great CD      1293494400   \n",
       "199994                                         great song      1409702400   \n",
       "199995                    Well Done Mariah! You Show 'Em!      1083369600   \n",
       "199998                                           Our Poet      1458432000   \n",
       "\n",
       "                category   price     itemID reviewHash  ... reviewHour  \\\n",
       "0                    Pop  $35.93  p70761125   85559980  ...         20   \n",
       "5                    Pop  $14.99  p78489708   23609516  ...         20   \n",
       "11             Classical  $31.04  p63362921   40704096  ...         19   \n",
       "14                   Pop  $11.57  p83852395   05580669  ...         20   \n",
       "21                   Pop   $7.98  p51677500   28306462  ...         20   \n",
       "...                  ...     ...        ...        ...  ...        ...   \n",
       "199985              Jazz   $2.15  p76891834   67429536  ...         19   \n",
       "199989  Alternative Rock  $12.97  p50896575   46693447  ...         19   \n",
       "199994               Pop  $49.99  p58216418   07085315  ...         20   \n",
       "199995               Pop   $7.98  p84118731   35077372  ...         20   \n",
       "199998  Alternative Rock  $11.07  p19134748   27463540  ...         20   \n",
       "\n",
       "       reviewMonthYear  cleanedPrice  \\\n",
       "0              2010-08         35.93   \n",
       "5              2015-09         14.99   \n",
       "11             2006-11         31.04   \n",
       "14             2016-08         11.57   \n",
       "21             2016-10          7.98   \n",
       "...                ...           ...   \n",
       "199985         2014-12          2.15   \n",
       "199989         2010-12         12.97   \n",
       "199994         2014-09         49.99   \n",
       "199995         2004-05          7.98   \n",
       "199998         2016-03         11.07   \n",
       "\n",
       "                                           fullReviewText  isPop isClassical  \\\n",
       "0       Amazing that I Actually Bought This...More Ama...      1           0   \n",
       "5                                         Five Stars o.k.      1           0   \n",
       "11      Superb interpretations by Katchen Katchen's pe...      0           1   \n",
       "14                             Five Stars She is my fave!      1           0   \n",
       "21                              Really Great Really Great      1           0   \n",
       "...                                                   ...    ...         ...   \n",
       "199985          Four Stars Had to have in my collection..      0           0   \n",
       "199989  Great CD I first heard Taking Back Sunday this...      0           0   \n",
       "199994  great song I love this lp this album is really...      1           0   \n",
       "199995  Well Done Mariah! You Show 'Em! With this, Mar...      1           0   \n",
       "199998  Our Poet Get it now ! Right now ! I am partial...      0           0   \n",
       "\n",
       "        isDanceElectronic  isAlternativeRock  isJazz  reviewWordCount  \n",
       "0                       0                  0       0              277  \n",
       "5                       0                  0       0                3  \n",
       "11                      0                  0       0              226  \n",
       "14                      0                  0       0                6  \n",
       "21                      0                  0       0                4  \n",
       "...                   ...                ...     ...              ...  \n",
       "199985                  0                  0       1                8  \n",
       "199989                  0                  1       0               43  \n",
       "199994                  0                  0       0               16  \n",
       "199995                  0                  0       0              172  \n",
       "199998                  0                  1       0               47  \n",
       "\n",
       "[49285 rows x 22 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "data_df['reviewMonth'] = data_df['reviewTime'].apply(lambda x: x.split(' ')[0])\n",
    "data_df['reviewYear'] = data_df['reviewTime'].apply(lambda x: x.split(' ')[2])\n",
    "data_df['reviewHour'] = data_df['unixReviewTime'].apply(lambda x: datetime.fromtimestamp(x).hour)\n",
    "data_df['reviewMonthYear'] = data_df['reviewYear'] + '-' + data_df['reviewMonth']\n",
    "\n",
    "data_df['cleanedPrice'] = data_df['price'].apply(lambda x: trim_price(x))\n",
    "data_df = data_df[data_df['cleanedPrice'] != \"\"]\n",
    "data_df['cleanedPrice'] = data_df['cleanedPrice'].astype('float')\n",
    "\n",
    "data_df['fixedReviewText'] = np.where(pd.isnull(data_df['reviewText']), \"\", data_df['reviewText'])\n",
    "data_df['fixedSummary'] = np.where(pd.isnull(data_df['summary']), \"\", data_df['summary'])\n",
    "data_df['fullReviewText'] = data_df['fixedSummary'] + \" \" + data_df['fixedReviewText']\n",
    "\n",
    "data_df = data_df.drop(columns=['fixedReviewText', 'fixedSummary'])\n",
    "\n",
    "genres = data_df['category'].unique()\n",
    "\n",
    "for genre in genres:\n",
    "    genre_col = \"is\" + genre.replace(\" \", \"\").replace(\"&\", \"\")\n",
    "    data_df[genre_col] = data_df['category'].apply(lambda x: 1 if x == genre else 0)\n",
    "\n",
    "data_df['reviewWordCount'] = data_df['fullReviewText'].apply(lambda x: len(x.split()))\n",
    "\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_MSE(actuals, predicteds):\n",
    "    \"\"\"Calculates the Mean Squared Error between `actuals` and `predicteds`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    actuals: np.array\n",
    "        A numpy array of the actual values.\n",
    "    predicteds: np.array\n",
    "        A numpy array of the predicted values.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        A float representing the Mean Squared Error between `actuals` and\n",
    "        `predicteds`.\n",
    "    \n",
    "    \"\"\"\n",
    "    return (((actuals - predicteds)**2).sum()) / (len(actuals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "genres = data_df['category'].unique()\n",
    "X_train_set = []\n",
    "X_val_set = []\n",
    "y_train_set = []\n",
    "y_val_set = []\n",
    "\n",
    "for genre in genres:\n",
    "    genre_df = data_df[data_df['category'] == genre]\n",
    "    targets = genre_df['overall']\n",
    "    feature_data = genre_df.drop(columns=['overall'])\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        feature_data, targets, shuffle=True, test_size=0.2, random_state=17)\n",
    "    X_train_set.append(X_train)\n",
    "    X_val_set.append(X_val)\n",
    "    y_train_set.append(y_train)\n",
    "    y_val_set.append(y_val)\n",
    "\n",
    "X_train = pd.concat(X_train_set)\n",
    "X_val = pd.concat(X_val_set)\n",
    "y_train = pd.concat(y_train_set)\n",
    "y_val = pd.concat(y_val_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative Filtering\n",
    "\n",
    "In this model we only need a users ID, the items ID, and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_item_matrix(df, rating_col, user_col, item_col):\n",
    "    return sp.csr_matrix(df[rating_col], (df[user_col], df[item_col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.concat([X_train, pd.DataFrame(y_train, columns=['overall'])], axis=1)\n",
    "val_data = pd.concat([X_val, pd.DataFrame(y_val, columns=['overall'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "item_matrix = train_data.pivot(index='itemID', columns='reviewerID', values='overall')\n",
    "item_matrix = item_matrix.fillna(0)\n",
    "user_item_train_matrix = sp.csr_matrix(item_matrix.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_average = train_data['overall'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "model_knn = NearestNeighbors(metric='cosine', algorithm='brute', n_neighbors=5)\n",
    "model_knn.fit(user_item_train_matrix)\n",
    "item_neighbors = np.asarray(model_knn.kneighbors(user_item_train_matrix, return_distance=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_matrix = train_data.pivot(index='reviewerID', columns='itemID', values='overall')\n",
    "user_matrix = user_matrix.fillna(0)\n",
    "user_item_train_matrix = sp.csr_matrix(user_matrix.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_knn = NearestNeighbors(metric='cosine', algorithm='brute', n_neighbors=5)\n",
    "model_knn.fit(user_item_train_matrix)\n",
    "user_neighbors = np.asarray(model_knn.kneighbors(user_item_train_matrix, return_distance=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user_avg = train_data.groupby(train_data['reviewerID'], as_index=False)['overall'].mean()\n",
    "train_item_avg = train_data.groupby(train_data['itemID'], as_index=False)['overall'].mean()\n",
    "train_user_avg.columns = ['reviewerID', 'userAverage']\n",
    "train_item_avg.columns = ['itemID', 'itemAverage']\n",
    "train_user_avg = train_user_avg.set_index('reviewerID')\n",
    "train_item_avg = train_item_avg.set_index('itemID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_avgs = []\n",
    "for i in range(len(item_neighbors)):\n",
    "    item_avgs.append(train_item_avg['itemAverage'][item_matrix.index[item_neighbors[i]]].mean())\n",
    "\n",
    "item_avgs = pd.concat([pd.DataFrame(item_matrix.index, columns=['itemID']), pd.DataFrame(item_avgs, columns=['itemRating'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_avgs = []\n",
    "for i in range(len(user_neighbors)):\n",
    "    user_avgs.append(train_user_avg['userAverage'][user_matrix.index[user_neighbors[i]]].mean())\n",
    "\n",
    "user_avgs = pd.concat([pd.DataFrame(user_matrix.index, columns=['reviewerID']), pd.DataFrame(user_avgs, columns=['userRating'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average_data(X, total_avg, user_avgs, item_avgs):\n",
    "    \"\"\"Calculates the error based on the weighted average prediction.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: pd.DataFrame\n",
    "        The DataFrame of features.\n",
    "    y: np.array\n",
    "        A numpy array containing the targets\n",
    "    total_avg: float\n",
    "        The average across all users/items.\n",
    "    user_avgs: pd.DataFrame\n",
    "        A DataFrame containing the average rating for each user.\n",
    "    item_avgs: pd.DataFrame\n",
    "        A DataFrame containing the average rating for each item.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        A float representing the mean squared error of the predictions.\n",
    "    \n",
    "    \"\"\"\n",
    "    df_user = pd.merge(X, user_avgs, how='left', on=['reviewerID'])\n",
    "    df_final = pd.merge(df_user, item_avgs, how='left', on=['itemID'])\n",
    "    df_final = df_final[['userRating', 'itemRating']]\n",
    "    df_final = df_final.fillna(total_avg)\n",
    "    df_final.index = X.index\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_aug = weighted_average_data(X_train, global_average, user_avgs, item_avgs)\n",
    "X_val_aug = weighted_average_data(X_val, global_average, user_avgs, item_avgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mod = pd.concat([X_train, X_train_aug], axis=1)\n",
    "X_val_mod = pd.concat([X_val, X_val_aug], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_rating(rating):\n",
    "    \"\"\"Thresholds `rating` to lie in the range [1, 5].\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rating: float\n",
    "        The rating to be thresholded.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        A float representing the thresholded rating.\n",
    "    \n",
    "    \"\"\"\n",
    "    if rating < 1:\n",
    "        return 1\n",
    "    if rating > 5:\n",
    "        return 5\n",
    "    return rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.8121797657358844\n",
      "Validation MSE: 0.9497614288949675\n"
     ]
    }
   ],
   "source": [
    "X_train_mod['pred'] = (0.5 * X_train_mod['userRating']) + (0.5 * X_train_mod['itemRating'])\n",
    "X_train_mod['pred'].apply(lambda x: threshold_rating(x))\n",
    "print(\"Training MSE: {}\".format(calculate_MSE(y_train, X_train_mod['pred'])))\n",
    "\n",
    "X_val_mod['pred'] = (0.5 * X_val_mod['userRating']) + (0.5 * X_val_mod['itemRating'])\n",
    "X_val_mod['pred'].apply(lambda x: threshold_rating(x))\n",
    "print(\"Validation MSE: {}\".format(calculate_MSE(y_val, X_val_mod['pred'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['cleanedPrice', 'isPop', 'isAlternativeRock', 'isJazz', 'isClassical', 'isDanceElectronic', 'reviewWordCount']\n",
    "X_train_reg1 = X_train[columns_to_keep]\n",
    "X_val_reg1 = X_val[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-99-2b5d7d7878ed>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train_reg1['reviewWordCount'] = X_train_reg1['reviewWordCount'].apply(lambda x: np.log(x))\n",
      "<ipython-input-99-2b5d7d7878ed>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val_reg1['reviewWordCount'] = X_val_reg1['reviewWordCount'].apply(lambda x: np.log(x))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_train_reg1['reviewWordCount'] = X_train_reg1['reviewWordCount'].apply(lambda x: np.log(x))\n",
    "X_val_reg1['reviewWordCount'] = X_val_reg1['reviewWordCount'].apply(lambda x: np.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-100-550e8da2fe0c>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.dropna(inplace=True)\n",
      "<ipython-input-100-550e8da2fe0c>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.dropna(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "def clean_dataset(df):\n",
    "    df.dropna(inplace=True)\n",
    "    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "    return df[indices_to_keep].astype(np.float64)\n",
    "\n",
    "X_train_reg1 = clean_dataset(X_train_reg1)\n",
    "y_train1 = y_train[y_train.index.isin(X_train_reg1.index)]\n",
    "X_train1 = X_train[X_train.index.isin(X_train_reg1.index)]\n",
    "\n",
    "X_val_reg1 = clean_dataset(X_val_reg1)\n",
    "y_val1 = y_val[y_val.index.isin(X_val_reg1.index)]\n",
    "X_val1 = X_val[X_val.index.isin(X_val_reg1.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mod = X_train_mod[X_train_mod.index.isin(X_train_reg1.index)]\n",
    "X_val_mod = X_val_mod[X_val_mod.index.isin(X_val_reg1.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reg1 = min_max_scaler.fit_transform(X_train_reg1)\n",
    "X_val_reg1 = min_max_scaler.transform(X_val_reg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha = 0.0\n",
      "------------\n",
      "Training Error: 0.9588535445992827\n",
      "Validation Error: 0.9262869599431929\n",
      "\n",
      "Alpha = 0.01\n",
      "------------\n",
      "Training Error: 0.9588428957078677\n",
      "Validation Error: 0.9262350113636552\n",
      "\n",
      "Alpha = 0.03\n",
      "------------\n",
      "Training Error: 0.9588433477027621\n",
      "Validation Error: 0.9262349757449848\n",
      "\n",
      "Alpha = 0.1\n",
      "------------\n",
      "Training Error: 0.9588449304068484\n",
      "Validation Error: 0.926234860637202\n",
      "\n",
      "Alpha = 0.3\n",
      "------------\n",
      "Training Error: 0.9588494564503761\n",
      "Validation Error: 0.9262346102237373\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "vthreshold_rating = np.vectorize(threshold_rating)\n",
    "\n",
    "alphas = [0.0, 0.01, 0.03, 0.1, 0.3]\n",
    "for alpha in alphas:\n",
    "    print(\"Alpha = {}\".format(alpha))\n",
    "    print(\"------------\")\n",
    "    reg_model = Ridge(alpha=alpha)\n",
    "    reg_model.fit(X_train_reg1, y_train1)\n",
    "    print(\"Training Error: {}\".format(calculate_MSE(y_train1, vthreshold_rating(reg_model.predict(X_train_reg1)))))\n",
    "    print(\"Validation Error: {}\".format(calculate_MSE(y_val1, vthreshold_rating(reg_model.predict(X_val_reg1)))))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Matthew/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def process_review_text(review_text, exclude_text, ps):\n",
    "    \"\"\"Pre-processes the text given by `review_text`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    review_text: str\n",
    "        The review text to be processed.\n",
    "    exclude_text: collection\n",
    "        A collection of words to be excluded.\n",
    "    ps: PorterStemmer\n",
    "        The PorterStemmer used to perform word stemming.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A string representing the processed version of `review_text`.\n",
    "    \n",
    "    \"\"\"\n",
    "    review = re.sub('[^a-zA-Z0-9]', ' ', review_text).lower().split()\n",
    "    review = [ps.stem(word) for word in review if not word in exclude_text]\n",
    "    return ' '.join(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-77-0d22f415ca83>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train1['processedReview'] = X_train1['fullReviewText'].apply(lambda x: process_review_text(x, exclude_english, ps))\n",
      "<ipython-input-77-0d22f415ca83>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val1['processedReview'] = X_val1['fullReviewText'].apply(lambda x: process_review_text(x, exclude_english, ps))\n"
     ]
    }
   ],
   "source": [
    "exclude_english = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "X_train1['processedReview'] = X_train1['fullReviewText'].apply(lambda x: process_review_text(x, exclude_english, ps))\n",
    "X_val1['processedReview'] = X_val1['fullReviewText'].apply(lambda x: process_review_text(x, exclude_english, ps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=1500)\n",
    "X_train_cv1 = cv.fit_transform(X_train1['processedReview'])\n",
    "X_val_cv1 = cv.transform(X_val1['processedReview'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "X_train_reg1_sp = sp.csr_matrix(X_train_reg1)\n",
    "X_train_cv_reg1 = sp.hstack((X_train_cv1, X_train_reg1_sp), format='csr')\n",
    "\n",
    "X_val_reg1_sp = sp.csr_matrix(X_val_reg1)\n",
    "X_val_cv_reg1 = sp.hstack((X_val_cv1, X_val_reg1_sp), format='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.01, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 6.561420677223477\n",
      "Validation Error: 6.583459713622337\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 6.539023898229513\n",
      "Validation Error: 6.564129017293043\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 6.484453433139949\n",
      "Validation Error: 6.517187858800406\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 6.411167259498902\n",
      "Validation Error: 6.475931639305919\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 2.993880709086672\n",
      "Validation Error: 2.9946954242174733\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 2.960697750071717\n",
      "Validation Error: 2.9646697856430824\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 2.8737119799995106\n",
      "Validation Error: 2.8932473709982465\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 2.753299083559392\n",
      "Validation Error: 2.8318332372229595\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 1.1926080219949884\n",
      "Validation Error: 1.1732094328170066\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 1.143658223983988\n",
      "Validation Error: 1.1280934031057286\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 1.023122410037057\n",
      "Validation Error: 1.0334664004790017\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8503154830638224\n",
      "Validation Error: 0.9629626657487433\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8681853917452161\n",
      "Validation Error: 0.8386496961638952\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7942277991980465\n",
      "Validation Error: 0.7709874267457265\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6379305554886673\n",
      "Validation Error: 0.662870805684716\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4090012039636347\n",
      "Validation Error: 0.6077346766321643\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 1.6571878830581932\n",
      "Validation Error: 1.6450137135466703\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 1.615457184618825\n",
      "Validation Error: 1.606545292338938\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 1.5090559939697927\n",
      "Validation Error: 1.5212456722480958\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 1.3542203108779522\n",
      "Validation Error: 1.4500479196233311\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.9318904044154998\n",
      "Validation Error: 0.9051161132227797\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.870548501849734\n",
      "Validation Error: 0.8481599595635091\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7340262790144473\n",
      "Validation Error: 0.7448564256470764\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5335184951771168\n",
      "Validation Error: 0.6770290319698306\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8547867806485374\n",
      "Validation Error: 0.8253682097966348\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7761078289747573\n",
      "Validation Error: 0.7534594712699987\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6151842031979793\n",
      "Validation Error: 0.6469247122305137\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.37706918707251746\n",
      "Validation Error: 0.5966494530335275\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7859795770007909\n",
      "Validation Error: 0.7585648368050062\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6947882334178169\n",
      "Validation Error: 0.679507649545087\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5099467940042608\n",
      "Validation Error: 0.5945913616580879\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.24017128066049634\n",
      "Validation Error: 0.572693788639393\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.865170081991152\n",
      "Validation Error: 0.8356093334908363\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7888418969750828\n",
      "Validation Error: 0.7656945934929492\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6356962214210389\n",
      "Validation Error: 0.6615849585119684\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.41024003485066074\n",
      "Validation Error: 0.6117925578933857\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8148141999906112\n",
      "Validation Error: 0.7866123693112871\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7266930115651957\n",
      "Validation Error: 0.7077187673362105\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5545709150856094\n",
      "Validation Error: 0.6164642932923652\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.3001521219013388\n",
      "Validation Error: 0.58721956565373\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7613749405253972\n",
      "Validation Error: 0.7353015317001932\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6667906165509955\n",
      "Validation Error: 0.6562067552653699\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.47436923379078433\n",
      "Validation Error: 0.5869045296103904\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.20460793582735723\n",
      "Validation Error: 0.5770293501683815\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.698905166757861\n",
      "Validation Error: 0.6783063478858798\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6018409100338165\n",
      "Validation Error: 0.6124901425976507\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.36797676273559415\n",
      "Validation Error: 0.5699558539982096\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error: 0.10222184461770129\n",
      "Validation Error: 0.5810409504586627\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7755903637661227\n",
      "Validation Error: 0.7489373297092125\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6849541330099715\n",
      "Validation Error: 0.6704786783439802\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5134840315951585\n",
      "Validation Error: 0.607261137409494\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.2442679164955763\n",
      "Validation Error: 0.6162037373385177\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.724601098296789\n",
      "Validation Error: 0.7005524473535436\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6347506512694173\n",
      "Validation Error: 0.6311534705798817\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.43354164287281205\n",
      "Validation Error: 0.5941150810245096\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.15086169825980553\n",
      "Validation Error: 0.6268613641177824\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6825761052398333\n",
      "Validation Error: 0.6646299958959496\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5886644636013009\n",
      "Validation Error: 0.6052973579119296\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.3536147248655745\n",
      "Validation Error: 0.5912957530696389\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.0770519780802823\n",
      "Validation Error: 0.6356583868706597\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.639694768325981\n",
      "Validation Error: 0.6358433876885132\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5224066632184315\n",
      "Validation Error: 0.5834169240012566\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.23800463743738237\n",
      "Validation Error: 0.5959858465513069\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.017830720754862573\n",
      "Validation Error: 0.6471025503557827\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7295930188619871\n",
      "Validation Error: 0.7038920567541768\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6450193762505255\n",
      "Validation Error: 0.6398796482456197\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4650369090229907\n",
      "Validation Error: 0.6111265089747598\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.18034337972854644\n",
      "Validation Error: 0.6849747849830464\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.687521617191076\n",
      "Validation Error: 0.668103969530689\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6017485212964041\n",
      "Validation Error: 0.6150391299764018\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.38315195997667867\n",
      "Validation Error: 0.6090241546170393\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.09676509343698798\n",
      "Validation Error: 0.700267119409869\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6528379769359417\n",
      "Validation Error: 0.6414111033433925\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5544993624008249\n",
      "Validation Error: 0.5980241440133184\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.30438316511199076\n",
      "Validation Error: 0.6154114427231584\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.04062101216994391\n",
      "Validation Error: 0.7094899765001101\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6147715385103462\n",
      "Validation Error: 0.6265473894461743\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4885378678614022\n",
      "Validation Error: 0.5910022461086326\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.17407301391964708\n",
      "Validation Error: 0.6439440765155783\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.003118709907745082\n",
      "Validation Error: 0.7185895294876302\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "learning_rates = [0.01, 0.03, 0.1, 0.3, 0.5]\n",
    "estimators = [50, 100, 200, 500]\n",
    "depths = [1, 2, 5, 10]\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for estimator in estimators:\n",
    "        for depth in depths:\n",
    "            print(\"Learning Rate: {0}, # Estimators: {1}, Depth: {2}\".format(learning_rate, estimator, depth))\n",
    "            print(\"--------------------------------------------------\")\n",
    "            xg_reg = XGBRegressor(\n",
    "                learning_rate=learning_rate, max_depth=depth, n_estimators=estimator)\n",
    "            xg_reg.fit(X_train_cv_reg1, y_train1)\n",
    "            print(\"Training Error: {}\".format(calculate_MSE(y_train1, vthreshold_rating(xg_reg.predict(X_train_cv_reg1)))))\n",
    "            print(\"Validation Error: {}\".format(calculate_MSE(y_val1, vthreshold_rating(xg_reg.predict(X_val_cv_reg1)))))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that `learning_rate=0.3`, `n_estimators=500`, and `max_depth=2` provides a very good model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error based on XGBoost CountVectorizer prediction: 0.522\n",
      "Validation error based on XGBoost CountVectorizer prediction: 0.583\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "xg_reg = XGBRegressor(learning_rate=0.3, n_estimators=500, max_depth=2)\n",
    "xg_reg.fit(X_train_cv_reg1, y_train1)\n",
    "\n",
    "train_MSE = calculate_MSE(y_train1, vthreshold_rating(xg_reg.predict(X_train_cv_reg1)))\n",
    "val_MSE = calculate_MSE(y_val1, vthreshold_rating(xg_reg.predict(X_val_cv_reg1)))\n",
    "\n",
    "print(\"Training error based on XGBoost CountVectorizer prediction: %.3f\" % train_MSE)\n",
    "print(\"Validation error based on XGBoost CountVectorizer prediction: %.3f\" % val_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta Models\n",
    "\n",
    "In this section we look at models that combine both collaborative filtering and language models.\n",
    "\n",
    "We start by using the predictions of the collaborative filtering as features in our language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['cleanedPrice', 'isPop', 'isAlternativeRock', 'isJazz', 'isClassical', 'isDanceElectronic', 'reviewWordCount', 'userRating', 'itemRating']\n",
    "X_train_reg2 = X_train_mod[columns_to_keep]\n",
    "X_val_reg2 = X_val_mod[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-105-d8a296a7aa94>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train_reg2['reviewWordCount'] = X_train_reg2['reviewWordCount'].apply(lambda x: np.log(x))\n",
      "<ipython-input-105-d8a296a7aa94>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val_reg2['reviewWordCount'] = X_val_reg2['reviewWordCount'].apply(lambda x: np.log(x))\n"
     ]
    }
   ],
   "source": [
    "min_max_scaler = MinMaxScaler()\n",
    "X_train_reg2['reviewWordCount'] = X_train_reg2['reviewWordCount'].apply(lambda x: np.log(x))\n",
    "X_val_reg2['reviewWordCount'] = X_val_reg2['reviewWordCount'].apply(lambda x: np.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-100-550e8da2fe0c>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.dropna(inplace=True)\n",
      "<ipython-input-100-550e8da2fe0c>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.dropna(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "X_train_reg2 = clean_dataset(X_train_reg2)\n",
    "y_train2 = y_train[y_train.index.isin(X_train_reg2.index)]\n",
    "X_train2 = X_train[X_train.index.isin(X_train_reg2.index)]\n",
    "\n",
    "X_val_reg2 = clean_dataset(X_val_reg2)\n",
    "y_val2 = y_val[y_val.index.isin(X_val_reg2.index)]\n",
    "X_val2 = X_val[X_val.index.isin(X_val_reg2.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reg2 = min_max_scaler.fit_transform(X_train_reg2)\n",
    "X_val_reg2 = min_max_scaler.transform(X_val_reg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha = 0.0\n",
      "------------\n",
      "Training Error: 0.7891828923294948\n",
      "Validation Error: 0.9486008427597297\n",
      "\n",
      "Alpha = 0.01\n",
      "------------\n",
      "Training Error: 0.7891677848703272\n",
      "Validation Error: 0.9484601751838659\n",
      "\n",
      "Alpha = 0.03\n",
      "------------\n",
      "Training Error: 0.7891678465440518\n",
      "Validation Error: 0.9484577298907713\n",
      "\n",
      "Alpha = 0.1\n",
      "------------\n",
      "Training Error: 0.7891680642355218\n",
      "Validation Error: 0.948449171698209\n",
      "\n",
      "Alpha = 0.3\n",
      "------------\n",
      "Training Error: 0.7891687018526102\n",
      "Validation Error: 0.9484247228291963\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.0, 0.01, 0.03, 0.1, 0.3]\n",
    "for alpha in alphas:\n",
    "    print(\"Alpha = {}\".format(alpha))\n",
    "    print(\"------------\")\n",
    "    reg_model = Ridge(alpha=alpha)\n",
    "    reg_model.fit(X_train_reg2, y_train2)\n",
    "    print(\"Training Error: {}\".format(calculate_MSE(y_train2, vthreshold_rating(reg_model.predict(X_train_reg2)))))\n",
    "    print(\"Validation Error: {}\".format(calculate_MSE(y_val2, vthreshold_rating(reg_model.predict(X_val_reg2)))))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-109-25810bf4f6a4>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train2['processedReview'] = X_train2['fullReviewText'].apply(lambda x: process_review_text(x, exclude_english, ps))\n",
      "<ipython-input-109-25810bf4f6a4>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val2['processedReview'] = X_val2['fullReviewText'].apply(lambda x: process_review_text(x, exclude_english, ps))\n"
     ]
    }
   ],
   "source": [
    "exclude_english = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "X_train2['processedReview'] = X_train2['fullReviewText'].apply(lambda x: process_review_text(x, exclude_english, ps))\n",
    "X_val2['processedReview'] = X_val2['fullReviewText'].apply(lambda x: process_review_text(x, exclude_english, ps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_features=1500)\n",
    "X_train_cv2 = cv.fit_transform(X_train2['processedReview'])\n",
    "X_val_cv2 = cv.transform(X_val2['processedReview'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reg2_sp = sp.csr_matrix(X_train_reg2)\n",
    "X_train_cv_reg2 = sp.hstack((X_train_cv2, X_train_reg2_sp), format='csr')\n",
    "\n",
    "X_val_reg2_sp = sp.csr_matrix(X_val_reg2)\n",
    "X_val_cv_reg2 = sp.hstack((X_val_cv2, X_val_reg2_sp), format='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.01, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 6.503735872991274\n",
      "Validation Error: 6.883393063136512\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 6.472925759109321\n",
      "Validation Error: 6.873810349242472\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 6.415552951975483\n",
      "Validation Error: 6.766778004848318\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 6.336512333143151\n",
      "Validation Error: 6.702224040577343\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 2.9116597565047635\n",
      "Validation Error: 3.2502935567689333\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 2.868043939305317\n",
      "Validation Error: 3.208625304323009\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 2.7780535325484026\n",
      "Validation Error: 3.092532168888972\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 2.6461880452656565\n",
      "Validation Error: 3.0224225434770875\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 1.0918346220330994\n",
      "Validation Error: 1.3135588463747994\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 1.036147689801702\n",
      "Validation Error: 1.2635888455120803\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.9146986694500452\n",
      "Validation Error: 1.1616875425943325\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7337391925862761\n",
      "Validation Error: 1.0908617788789443\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.756386356086193\n",
      "Validation Error: 0.8996191559372384\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6839209384629215\n",
      "Validation Error: 0.8403416793262685\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5360902242478431\n",
      "Validation Error: 0.7367690362634365\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.31446922435595465\n",
      "Validation Error: 0.6811167992372987\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 1.5627177723097216\n",
      "Validation Error: 1.846452518919343\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 1.5119653601832954\n",
      "Validation Error: 1.7960544459510177\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 1.4032360632395287\n",
      "Validation Error: 1.6804284272855077\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 1.2401300435868885\n",
      "Validation Error: 1.6018033435097432\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8239466745938151\n",
      "Validation Error: 0.9942010555413113\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7620201935774863\n",
      "Validation Error: 0.9443683509025774\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6266979082527001\n",
      "Validation Error: 0.8415338280252339\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.42568372015336575\n",
      "Validation Error: 0.7730068336676122\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7426928131144244\n",
      "Validation Error: 0.885071876050487\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6671662762814085\n",
      "Validation Error: 0.8232709367647633\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.515852358064282\n",
      "Validation Error: 0.7198106463169784\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.2890217747658168\n",
      "Validation Error: 0.6667757772826471\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6771667436192434\n",
      "Validation Error: 0.8162237821223852\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5930182057061686\n",
      "Validation Error: 0.7417321041296694\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8356626506024096\n",
      "Validation Error: 0.6542027155916784\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.1741651887529198\n",
      "Validation Error: 0.6306806169805278\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7532719545067948\n",
      "Validation Error: 0.8984023303350915\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6823394868546818\n",
      "Validation Error: 0.8415287026010446\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5346141250969264\n",
      "Validation Error: 0.7337423743840931\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.3130148012635119\n",
      "Validation Error: 0.6872977133258573\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7040695828928955\n",
      "Validation Error: 0.8450873275924572\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6234277468668492\n",
      "Validation Error: 0.778036234504546\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4601100858885094\n",
      "Validation Error: 0.677615698823088\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6884971464806595\n",
      "Validation Error: 0.6534113519806398\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6550334651549221\n",
      "Validation Error: 0.79237246870429\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5693960927723146\n",
      "Validation Error: 0.7172584528181635\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7959670259987318\n",
      "Validation Error: 0.6401843150790001\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.14231621233302494\n",
      "Validation Error: 0.6375265433888077\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6019752200057845\n",
      "Validation Error: 0.7323421618628351\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8891819911223843\n",
      "Validation Error: 0.6624839452884889\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.29619556817303605\n",
      "Validation Error: 0.6161472705041048\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error: 0.06626693654491513\n",
      "Validation Error: 0.629127338706747\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6695470399035651\n",
      "Validation Error: 0.8320175368867865\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5877134032041788\n",
      "Validation Error: 0.7312280170602444\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.821128725428028\n",
      "Validation Error: 0.6644344297462841\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.18017706776835557\n",
      "Validation Error: 0.6767750787866058\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6250522630858881\n",
      "Validation Error: 0.7799276543267967\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5423968546848821\n",
      "Validation Error: 0.6814512809318369\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7572352568167406\n",
      "Validation Error: 0.6419165688148936\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.10392467283260896\n",
      "Validation Error: 0.6762583510226913\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5903803945789322\n",
      "Validation Error: 0.7412608779634012\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5016847541934427\n",
      "Validation Error: 0.6515057605621919\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.28299454631394083\n",
      "Validation Error: 0.6316645878775758\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.05050973316984014\n",
      "Validation Error: 0.6759090507785038\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5541520219459554\n",
      "Validation Error: 0.6982901469331604\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8248065948002536\n",
      "Validation Error: 0.626830781451005\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.1817675341128254\n",
      "Validation Error: 0.6344574740234872\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.009288338708908602\n",
      "Validation Error: 0.6786818518535688\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.635053766967533\n",
      "Validation Error: 0.8025454034164896\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5565634106569863\n",
      "Validation Error: 0.698938232190281\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.37772052797915046\n",
      "Validation Error: 0.6784713122415739\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.13745887321542427\n",
      "Validation Error: 0.7281470255669896\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5974075260376706\n",
      "Validation Error: 0.7515024210684589\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8964870006341155\n",
      "Validation Error: 0.6696219617387341\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.3067495566385541\n",
      "Validation Error: 0.6694607004808133\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.07818992036583028\n",
      "Validation Error: 0.7319921716274881\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5674944647436064\n",
      "Validation Error: 0.7110229400138558\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.47198463424299336\n",
      "Validation Error: 0.6490063890569089\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.2281092082780791\n",
      "Validation Error: 0.6684967836349853\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.02496529929461796\n",
      "Validation Error: 0.7367603182741793\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5343279686884503\n",
      "Validation Error: 0.6809905159455284\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7973367152821813\n",
      "Validation Error: 0.6322757540964926\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.12355123516941925\n",
      "Validation Error: 0.6746721036001143\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.0011216059666213293\n",
      "Validation Error: 0.7389609370739282\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.01, 0.03, 0.1, 0.3, 0.5]\n",
    "estimators = [50, 100, 200, 500]\n",
    "depths = [1, 2, 5, 10]\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for estimator in estimators:\n",
    "        for depth in depths:\n",
    "            print(\"Learning Rate: {0}, # Estimators: {1}, Depth: {2}\".format(learning_rate, estimator, depth))\n",
    "            print(\"--------------------------------------------------\")\n",
    "            xg_reg = XGBRegressor(\n",
    "                learning_rate=learning_rate, max_depth=depth, n_estimators=estimator)\n",
    "            xg_reg.fit(X_train_cv_reg2, y_train2)\n",
    "            print(\"Training Error: {}\".format(calculate_MSE(y_train2, vthreshold_rating(xg_reg.predict(X_train_cv_reg2)))))\n",
    "            print(\"Validation Error: {}\".format(calculate_MSE(y_val2, vthreshold_rating(xg_reg.predict(X_val_cv_reg2)))))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems `learning_rate=0.3`, `n_estimators=200`, `max_depth=2` performs the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error based on XGBoost CountVectorizer prediction: 0.502\n",
      "Validation error based on XGBoost CountVectorizer prediction: 0.652\n"
     ]
    }
   ],
   "source": [
    "xg_reg = XGBRegressor(learning_rate=0.3, n_estimators=200, max_depth=2)\n",
    "xg_reg.fit(X_train_cv_reg2, y_train2)\n",
    "\n",
    "train_MSE = calculate_MSE(y_train2, vthreshold_rating(xg_reg.predict(X_train_cv_reg2)))\n",
    "val_MSE = calculate_MSE(y_val2, vthreshold_rating(xg_reg.predict(X_val_cv_reg2)))\n",
    "\n",
    "print(\"Training error based on XGBoost CountVectorizer prediction: %.3f\" % train_MSE)\n",
    "print(\"Validation error based on XGBoost CountVectorizer prediction: %.3f\" % val_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is actually much worse compared to the pure language model.\n",
    "\n",
    "However, we could also create a meta model by taking a weighted average of predictions from collaborative filtering and the pure language model. We now try this for a few candidate weightings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight: 0.0\n",
      "------------\n",
      "Training error: 0.812\n",
      "Validation error: 0.950\n",
      "\n",
      "Weight: 0.1\n",
      "------------\n",
      "Training error: 0.754\n",
      "Validation error: 0.882\n",
      "\n",
      "Weight: 0.3\n",
      "------------\n",
      "Training error: 0.658\n",
      "Validation error: 0.768\n",
      "\n",
      "Weight: 0.5\n",
      "------------\n",
      "Training error: 0.587\n",
      "Validation error: 0.681\n",
      "\n",
      "Weight: 0.7\n",
      "------------\n",
      "Training error: 0.542\n",
      "Validation error: 0.621\n",
      "\n",
      "Weight: 0.9\n",
      "------------\n",
      "Training error: 0.522\n",
      "Validation error: 0.589\n",
      "\n",
      "Weight: 1.0\n",
      "------------\n",
      "Training error: 0.522\n",
      "Validation error: 0.583\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weights = [0.0, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0]\n",
    "\n",
    "xg_reg = XGBRegressor(learning_rate=0.3, n_estimators=500, max_depth=2)\n",
    "xg_reg.fit(X_train_cv_reg1, y_train1)\n",
    "\n",
    "reg_train_preds = vthreshold_rating(xg_reg.predict(X_train_cv_reg1))\n",
    "reg_val_preds = vthreshold_rating(xg_reg.predict(X_val_cv_reg1))\n",
    "\n",
    "cf_train_preds = vthreshold_rating(X_train_mod['pred'])\n",
    "cf_val_preds = vthreshold_rating(X_val_mod['pred'])\n",
    "\n",
    "for weight in weights:\n",
    "    print(\"Weight: %.1f\" % weight)\n",
    "    print(\"------------\")\n",
    "    train_MSE = calculate_MSE(y_train1, ((weight*reg_train_preds) + ((1.0 - weight)*cf_train_preds)))\n",
    "    val_MSE = calculate_MSE(y_val1, ((weight*reg_val_preds) + ((1.0 - weight)*cf_val_preds)))\n",
    "    print(\"Training error: %.3f\" % train_MSE)\n",
    "    print(\"Validation error: %.3f\" % val_MSE)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csc2515-env-3.8",
   "language": "python",
   "name": "csc2515-env-3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
