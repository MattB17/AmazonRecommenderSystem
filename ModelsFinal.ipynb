{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Models\n",
    "\n",
    "In this notebook we look at some final candidate models and assess their performance on a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "random.seed(17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "\n",
    "We will subsample 50,000 of the 200,000 records for training by sampling 25% of the data from each of the five music categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = None\n",
    "with open(os.path.join('data', 'train.json'), 'r') as train_file:\n",
    "    data = [json.loads(row) for row in train_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame(data).drop(columns=['image'])\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>category</th>\n",
       "      <th>price</th>\n",
       "      <th>itemID</th>\n",
       "      <th>reviewHash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>08 24, 2010</td>\n",
       "      <td>u04428712</td>\n",
       "      <td>So is Katy Perry's new album \"Teenage Dream\" c...</td>\n",
       "      <td>Amazing that I Actually Bought This...More Ama...</td>\n",
       "      <td>1282608000</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$35.93</td>\n",
       "      <td>p70761125</td>\n",
       "      <td>85559980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>10 13, 2015</td>\n",
       "      <td>u92735614</td>\n",
       "      <td>I REALLY enjoy this pairing of Anderson and Po...</td>\n",
       "      <td>Love the Music, Hate the Light Show</td>\n",
       "      <td>1444694400</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$89.86</td>\n",
       "      <td>p82172532</td>\n",
       "      <td>24751194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>09 7, 2015</td>\n",
       "      <td>u07624734</td>\n",
       "      <td>o.k.</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1441584000</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$14.99</td>\n",
       "      <td>p78489708</td>\n",
       "      <td>23609516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.0</td>\n",
       "      <td>11 16, 2006</td>\n",
       "      <td>u40719083</td>\n",
       "      <td>Katchen's performace throughout this collectio...</td>\n",
       "      <td>Superb interpretations by Katchen</td>\n",
       "      <td>1163635200</td>\n",
       "      <td>Classical</td>\n",
       "      <td>$31.04</td>\n",
       "      <td>p63362921</td>\n",
       "      <td>40704096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.0</td>\n",
       "      <td>08 12, 2016</td>\n",
       "      <td>u66989964</td>\n",
       "      <td>She is my fave!</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1470960000</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$11.57</td>\n",
       "      <td>p83852395</td>\n",
       "      <td>05580669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199991</th>\n",
       "      <td>3.0</td>\n",
       "      <td>02 6, 2014</td>\n",
       "      <td>u95571154</td>\n",
       "      <td>Bruno Mars second studio album, Unorthodox Juk...</td>\n",
       "      <td>Orthodox Jukebox</td>\n",
       "      <td>1391644800</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$10.45</td>\n",
       "      <td>p67630640</td>\n",
       "      <td>96294720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199993</th>\n",
       "      <td>5.0</td>\n",
       "      <td>11 15, 2014</td>\n",
       "      <td>u85625112</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1416009600</td>\n",
       "      <td>Alternative Rock</td>\n",
       "      <td>$9.95</td>\n",
       "      <td>p61111203</td>\n",
       "      <td>47089196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199994</th>\n",
       "      <td>4.0</td>\n",
       "      <td>09 3, 2014</td>\n",
       "      <td>u97165206</td>\n",
       "      <td>I love this lp this album is really good\\none ...</td>\n",
       "      <td>great song</td>\n",
       "      <td>1409702400</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$49.99</td>\n",
       "      <td>p58216418</td>\n",
       "      <td>07085315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>4.0</td>\n",
       "      <td>05 1, 2004</td>\n",
       "      <td>u68902609</td>\n",
       "      <td>With this, Mariah's third album, Mariah proved...</td>\n",
       "      <td>Well Done Mariah! You Show 'Em!</td>\n",
       "      <td>1083369600</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$7.98</td>\n",
       "      <td>p84118731</td>\n",
       "      <td>35077372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>5.0</td>\n",
       "      <td>02 27, 2017</td>\n",
       "      <td>u15269603</td>\n",
       "      <td>Fantastic CD.  All the hits are here and even ...</td>\n",
       "      <td>Great collection, excellent sound!</td>\n",
       "      <td>1488153600</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$11.49</td>\n",
       "      <td>p08613950</td>\n",
       "      <td>09788722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50001 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        overall   reviewTime reviewerID  \\\n",
       "0           4.0  08 24, 2010  u04428712   \n",
       "2           4.0  10 13, 2015  u92735614   \n",
       "5           5.0   09 7, 2015  u07624734   \n",
       "11          5.0  11 16, 2006  u40719083   \n",
       "14          5.0  08 12, 2016  u66989964   \n",
       "...         ...          ...        ...   \n",
       "199991      3.0   02 6, 2014  u95571154   \n",
       "199993      5.0  11 15, 2014  u85625112   \n",
       "199994      4.0   09 3, 2014  u97165206   \n",
       "199995      4.0   05 1, 2004  u68902609   \n",
       "199996      5.0  02 27, 2017  u15269603   \n",
       "\n",
       "                                               reviewText  \\\n",
       "0       So is Katy Perry's new album \"Teenage Dream\" c...   \n",
       "2       I REALLY enjoy this pairing of Anderson and Po...   \n",
       "5                                                    o.k.   \n",
       "11      Katchen's performace throughout this collectio...   \n",
       "14                                        She is my fave!   \n",
       "...                                                   ...   \n",
       "199991  Bruno Mars second studio album, Unorthodox Juk...   \n",
       "199993                                          Excellent   \n",
       "199994  I love this lp this album is really good\\none ...   \n",
       "199995  With this, Mariah's third album, Mariah proved...   \n",
       "199996  Fantastic CD.  All the hits are here and even ...   \n",
       "\n",
       "                                                  summary  unixReviewTime  \\\n",
       "0       Amazing that I Actually Bought This...More Ama...      1282608000   \n",
       "2                     Love the Music, Hate the Light Show      1444694400   \n",
       "5                                              Five Stars      1441584000   \n",
       "11                      Superb interpretations by Katchen      1163635200   \n",
       "14                                             Five Stars      1470960000   \n",
       "...                                                   ...             ...   \n",
       "199991                                   Orthodox Jukebox      1391644800   \n",
       "199993                                         Five Stars      1416009600   \n",
       "199994                                         great song      1409702400   \n",
       "199995                    Well Done Mariah! You Show 'Em!      1083369600   \n",
       "199996                 Great collection, excellent sound!      1488153600   \n",
       "\n",
       "                category   price     itemID reviewHash  \n",
       "0                    Pop  $35.93  p70761125   85559980  \n",
       "2                    Pop  $89.86  p82172532   24751194  \n",
       "5                    Pop  $14.99  p78489708   23609516  \n",
       "11             Classical  $31.04  p63362921   40704096  \n",
       "14                   Pop  $11.57  p83852395   05580669  \n",
       "...                  ...     ...        ...        ...  \n",
       "199991               Pop  $10.45  p67630640   96294720  \n",
       "199993  Alternative Rock   $9.95  p61111203   47089196  \n",
       "199994               Pop  $49.99  p58216418   07085315  \n",
       "199995               Pop   $7.98  p84118731   35077372  \n",
       "199996               Pop  $11.49  p08613950   09788722  \n",
       "\n",
       "[50001 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = data_df['category'].unique()\n",
    "dfs = []\n",
    "for category in categories:\n",
    "    dfs.append(data_df[data_df['category'] == category].sample(frac=0.25))\n",
    "data_df = pd.concat(dfs, axis=0)\n",
    "data_df = data_df.sort_index()\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing\n",
    "\n",
    "We apply feature cleaning as prototyped before and then split into a training and validation set, ensuring that the proportion of data points in training vs validation is consistent for each music category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_price(price):\n",
    "    \"\"\"Trims `price` to remove the $ sign.\n",
    "    \n",
    "    If the price variable does not have the format $x.xx\n",
    "    then the empty string is returned.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    price: str\n",
    "        A string representing a price.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A string representing `price` but with the $ sign removed,\n",
    "        or the empty string if `price` does not have the correct\n",
    "        format.\n",
    "    \n",
    "    \"\"\"\n",
    "    if (not pd.isnull(price) and isinstance(price, str) and\n",
    "        len(price) > 0 and price[0] == '$'):\n",
    "        return price[1:]\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-d3ba8e2e1b50>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_df['cleanedPrice'] = data_df['cleanedPrice'].astype('float')\n",
      "<ipython-input-6-d3ba8e2e1b50>:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_df['fixedReviewText'] = np.where(pd.isnull(data_df['reviewText']), \"\", data_df['reviewText'])\n",
      "<ipython-input-6-d3ba8e2e1b50>:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_df['fixedSummary'] = np.where(pd.isnull(data_df['summary']), \"\", data_df['summary'])\n",
      "<ipython-input-6-d3ba8e2e1b50>:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_df['fullReviewText'] = data_df['fixedSummary'] + \" \" + data_df['fixedReviewText']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>category</th>\n",
       "      <th>price</th>\n",
       "      <th>itemID</th>\n",
       "      <th>reviewHash</th>\n",
       "      <th>...</th>\n",
       "      <th>reviewHour</th>\n",
       "      <th>reviewMonthYear</th>\n",
       "      <th>cleanedPrice</th>\n",
       "      <th>fullReviewText</th>\n",
       "      <th>isPop</th>\n",
       "      <th>isClassical</th>\n",
       "      <th>isJazz</th>\n",
       "      <th>isAlternativeRock</th>\n",
       "      <th>isDanceElectronic</th>\n",
       "      <th>reviewWordCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>08 24, 2010</td>\n",
       "      <td>u04428712</td>\n",
       "      <td>So is Katy Perry's new album \"Teenage Dream\" c...</td>\n",
       "      <td>Amazing that I Actually Bought This...More Ama...</td>\n",
       "      <td>1282608000</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$35.93</td>\n",
       "      <td>p70761125</td>\n",
       "      <td>85559980</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>2010-08</td>\n",
       "      <td>35.93</td>\n",
       "      <td>Amazing that I Actually Bought This...More Ama...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>10 13, 2015</td>\n",
       "      <td>u92735614</td>\n",
       "      <td>I REALLY enjoy this pairing of Anderson and Po...</td>\n",
       "      <td>Love the Music, Hate the Light Show</td>\n",
       "      <td>1444694400</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$89.86</td>\n",
       "      <td>p82172532</td>\n",
       "      <td>24751194</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>2015-10</td>\n",
       "      <td>89.86</td>\n",
       "      <td>Love the Music, Hate the Light Show I REALLY e...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>09 7, 2015</td>\n",
       "      <td>u07624734</td>\n",
       "      <td>o.k.</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1441584000</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$14.99</td>\n",
       "      <td>p78489708</td>\n",
       "      <td>23609516</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>2015-09</td>\n",
       "      <td>14.99</td>\n",
       "      <td>Five Stars o.k.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.0</td>\n",
       "      <td>11 16, 2006</td>\n",
       "      <td>u40719083</td>\n",
       "      <td>Katchen's performace throughout this collectio...</td>\n",
       "      <td>Superb interpretations by Katchen</td>\n",
       "      <td>1163635200</td>\n",
       "      <td>Classical</td>\n",
       "      <td>$31.04</td>\n",
       "      <td>p63362921</td>\n",
       "      <td>40704096</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>2006-11</td>\n",
       "      <td>31.04</td>\n",
       "      <td>Superb interpretations by Katchen Katchen's pe...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.0</td>\n",
       "      <td>08 12, 2016</td>\n",
       "      <td>u66989964</td>\n",
       "      <td>She is my fave!</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1470960000</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$11.57</td>\n",
       "      <td>p83852395</td>\n",
       "      <td>05580669</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>2016-08</td>\n",
       "      <td>11.57</td>\n",
       "      <td>Five Stars She is my fave!</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199991</th>\n",
       "      <td>3.0</td>\n",
       "      <td>02 6, 2014</td>\n",
       "      <td>u95571154</td>\n",
       "      <td>Bruno Mars second studio album, Unorthodox Juk...</td>\n",
       "      <td>Orthodox Jukebox</td>\n",
       "      <td>1391644800</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$10.45</td>\n",
       "      <td>p67630640</td>\n",
       "      <td>96294720</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>2014-02</td>\n",
       "      <td>10.45</td>\n",
       "      <td>Orthodox Jukebox Bruno Mars second studio albu...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199993</th>\n",
       "      <td>5.0</td>\n",
       "      <td>11 15, 2014</td>\n",
       "      <td>u85625112</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1416009600</td>\n",
       "      <td>Alternative Rock</td>\n",
       "      <td>$9.95</td>\n",
       "      <td>p61111203</td>\n",
       "      <td>47089196</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>2014-11</td>\n",
       "      <td>9.95</td>\n",
       "      <td>Five Stars Excellent</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199994</th>\n",
       "      <td>4.0</td>\n",
       "      <td>09 3, 2014</td>\n",
       "      <td>u97165206</td>\n",
       "      <td>I love this lp this album is really good\\none ...</td>\n",
       "      <td>great song</td>\n",
       "      <td>1409702400</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$49.99</td>\n",
       "      <td>p58216418</td>\n",
       "      <td>07085315</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>2014-09</td>\n",
       "      <td>49.99</td>\n",
       "      <td>great song I love this lp this album is really...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>4.0</td>\n",
       "      <td>05 1, 2004</td>\n",
       "      <td>u68902609</td>\n",
       "      <td>With this, Mariah's third album, Mariah proved...</td>\n",
       "      <td>Well Done Mariah! You Show 'Em!</td>\n",
       "      <td>1083369600</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$7.98</td>\n",
       "      <td>p84118731</td>\n",
       "      <td>35077372</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>2004-05</td>\n",
       "      <td>7.98</td>\n",
       "      <td>Well Done Mariah! You Show 'Em! With this, Mar...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>5.0</td>\n",
       "      <td>02 27, 2017</td>\n",
       "      <td>u15269603</td>\n",
       "      <td>Fantastic CD.  All the hits are here and even ...</td>\n",
       "      <td>Great collection, excellent sound!</td>\n",
       "      <td>1488153600</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$11.49</td>\n",
       "      <td>p08613950</td>\n",
       "      <td>09788722</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>2017-02</td>\n",
       "      <td>11.49</td>\n",
       "      <td>Great collection, excellent sound! Fantastic C...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49268 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        overall   reviewTime reviewerID  \\\n",
       "0           4.0  08 24, 2010  u04428712   \n",
       "2           4.0  10 13, 2015  u92735614   \n",
       "5           5.0   09 7, 2015  u07624734   \n",
       "11          5.0  11 16, 2006  u40719083   \n",
       "14          5.0  08 12, 2016  u66989964   \n",
       "...         ...          ...        ...   \n",
       "199991      3.0   02 6, 2014  u95571154   \n",
       "199993      5.0  11 15, 2014  u85625112   \n",
       "199994      4.0   09 3, 2014  u97165206   \n",
       "199995      4.0   05 1, 2004  u68902609   \n",
       "199996      5.0  02 27, 2017  u15269603   \n",
       "\n",
       "                                               reviewText  \\\n",
       "0       So is Katy Perry's new album \"Teenage Dream\" c...   \n",
       "2       I REALLY enjoy this pairing of Anderson and Po...   \n",
       "5                                                    o.k.   \n",
       "11      Katchen's performace throughout this collectio...   \n",
       "14                                        She is my fave!   \n",
       "...                                                   ...   \n",
       "199991  Bruno Mars second studio album, Unorthodox Juk...   \n",
       "199993                                          Excellent   \n",
       "199994  I love this lp this album is really good\\none ...   \n",
       "199995  With this, Mariah's third album, Mariah proved...   \n",
       "199996  Fantastic CD.  All the hits are here and even ...   \n",
       "\n",
       "                                                  summary  unixReviewTime  \\\n",
       "0       Amazing that I Actually Bought This...More Ama...      1282608000   \n",
       "2                     Love the Music, Hate the Light Show      1444694400   \n",
       "5                                              Five Stars      1441584000   \n",
       "11                      Superb interpretations by Katchen      1163635200   \n",
       "14                                             Five Stars      1470960000   \n",
       "...                                                   ...             ...   \n",
       "199991                                   Orthodox Jukebox      1391644800   \n",
       "199993                                         Five Stars      1416009600   \n",
       "199994                                         great song      1409702400   \n",
       "199995                    Well Done Mariah! You Show 'Em!      1083369600   \n",
       "199996                 Great collection, excellent sound!      1488153600   \n",
       "\n",
       "                category   price     itemID reviewHash  ... reviewHour  \\\n",
       "0                    Pop  $35.93  p70761125   85559980  ...         20   \n",
       "2                    Pop  $89.86  p82172532   24751194  ...         20   \n",
       "5                    Pop  $14.99  p78489708   23609516  ...         20   \n",
       "11             Classical  $31.04  p63362921   40704096  ...         19   \n",
       "14                   Pop  $11.57  p83852395   05580669  ...         20   \n",
       "...                  ...     ...        ...        ...  ...        ...   \n",
       "199991               Pop  $10.45  p67630640   96294720  ...         19   \n",
       "199993  Alternative Rock   $9.95  p61111203   47089196  ...         19   \n",
       "199994               Pop  $49.99  p58216418   07085315  ...         20   \n",
       "199995               Pop   $7.98  p84118731   35077372  ...         20   \n",
       "199996               Pop  $11.49  p08613950   09788722  ...         19   \n",
       "\n",
       "       reviewMonthYear  cleanedPrice  \\\n",
       "0              2010-08         35.93   \n",
       "2              2015-10         89.86   \n",
       "5              2015-09         14.99   \n",
       "11             2006-11         31.04   \n",
       "14             2016-08         11.57   \n",
       "...                ...           ...   \n",
       "199991         2014-02         10.45   \n",
       "199993         2014-11          9.95   \n",
       "199994         2014-09         49.99   \n",
       "199995         2004-05          7.98   \n",
       "199996         2017-02         11.49   \n",
       "\n",
       "                                           fullReviewText  isPop isClassical  \\\n",
       "0       Amazing that I Actually Bought This...More Ama...      1           0   \n",
       "2       Love the Music, Hate the Light Show I REALLY e...      1           0   \n",
       "5                                         Five Stars o.k.      1           0   \n",
       "11      Superb interpretations by Katchen Katchen's pe...      0           1   \n",
       "14                             Five Stars She is my fave!      1           0   \n",
       "...                                                   ...    ...         ...   \n",
       "199991  Orthodox Jukebox Bruno Mars second studio albu...      1           0   \n",
       "199993                               Five Stars Excellent      0           0   \n",
       "199994  great song I love this lp this album is really...      1           0   \n",
       "199995  Well Done Mariah! You Show 'Em! With this, Mar...      1           0   \n",
       "199996  Great collection, excellent sound! Fantastic C...      1           0   \n",
       "\n",
       "        isJazz  isAlternativeRock  isDanceElectronic  reviewWordCount  \n",
       "0            0                  0                  0              277  \n",
       "2            0                  0                  0              133  \n",
       "5            0                  0                  0                3  \n",
       "11           0                  0                  0              226  \n",
       "14           0                  0                  0                6  \n",
       "...        ...                ...                ...              ...  \n",
       "199991       0                  0                  0              145  \n",
       "199993       0                  1                  0                3  \n",
       "199994       0                  0                  0               16  \n",
       "199995       0                  0                  0              172  \n",
       "199996       0                  0                  0               47  \n",
       "\n",
       "[49268 rows x 22 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "data_df['reviewMonth'] = data_df['reviewTime'].apply(lambda x: x.split(' ')[0])\n",
    "data_df['reviewYear'] = data_df['reviewTime'].apply(lambda x: x.split(' ')[2])\n",
    "data_df['reviewHour'] = data_df['unixReviewTime'].apply(lambda x: datetime.fromtimestamp(x).hour)\n",
    "data_df['reviewMonthYear'] = data_df['reviewYear'] + '-' + data_df['reviewMonth']\n",
    "\n",
    "data_df['cleanedPrice'] = data_df['price'].apply(lambda x: trim_price(x))\n",
    "data_df = data_df[data_df['cleanedPrice'] != \"\"]\n",
    "data_df['cleanedPrice'] = data_df['cleanedPrice'].astype('float')\n",
    "\n",
    "data_df['fixedReviewText'] = np.where(pd.isnull(data_df['reviewText']), \"\", data_df['reviewText'])\n",
    "data_df['fixedSummary'] = np.where(pd.isnull(data_df['summary']), \"\", data_df['summary'])\n",
    "data_df['fullReviewText'] = data_df['fixedSummary'] + \" \" + data_df['fixedReviewText']\n",
    "\n",
    "data_df = data_df.drop(columns=['fixedReviewText', 'fixedSummary'])\n",
    "\n",
    "genres = data_df['category'].unique()\n",
    "\n",
    "for genre in genres:\n",
    "    genre_col = \"is\" + genre.replace(\" \", \"\").replace(\"&\", \"\")\n",
    "    data_df[genre_col] = data_df['category'].apply(lambda x: 1 if x == genre else 0)\n",
    "\n",
    "data_df['reviewWordCount'] = data_df['fullReviewText'].apply(lambda x: len(x.split()))\n",
    "\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_MSE(actuals, predicteds):\n",
    "    \"\"\"Calculates the Mean Squared Error between `actuals` and `predicteds`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    actuals: np.array\n",
    "        A numpy array of the actual values.\n",
    "    predicteds: np.array\n",
    "        A numpy array of the predicted values.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        A float representing the Mean Squared Error between `actuals` and\n",
    "        `predicteds`.\n",
    "    \n",
    "    \"\"\"\n",
    "    return (((actuals - predicteds)**2).sum()) / (len(actuals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "genres = data_df['category'].unique()\n",
    "X_train_set = []\n",
    "X_val_set = []\n",
    "y_train_set = []\n",
    "y_val_set = []\n",
    "\n",
    "for genre in genres:\n",
    "    genre_df = data_df[data_df['category'] == genre]\n",
    "    targets = genre_df['overall']\n",
    "    feature_data = genre_df.drop(columns=['overall'])\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        feature_data, targets, shuffle=True, test_size=0.2, random_state=17)\n",
    "    X_train_set.append(X_train)\n",
    "    X_val_set.append(X_val)\n",
    "    y_train_set.append(y_train)\n",
    "    y_val_set.append(y_val)\n",
    "\n",
    "X_train = pd.concat(X_train_set)\n",
    "X_val = pd.concat(X_val_set)\n",
    "y_train = pd.concat(y_train_set)\n",
    "y_val = pd.concat(y_val_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative Filtering\n",
    "\n",
    "In this model we only need a users ID, the items ID, and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.concat([X_train, pd.DataFrame(y_train, columns=['overall'])], axis=1)\n",
    "val_data = pd.concat([X_val, pd.DataFrame(y_val, columns=['overall'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "item_matrix = train_data.pivot(index='itemID', columns='reviewerID', values='overall')\n",
    "item_matrix = item_matrix.fillna(0)\n",
    "user_item_train_matrix = sp.csr_matrix(item_matrix.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_item_avg = train_data.groupby(train_data['itemID'], as_index=False)['overall'].mean()\n",
    "train_item_avg.columns = ['itemID', 'itemAverage']\n",
    "train_item_avg = train_item_avg.set_index('itemID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def build_knn_model(train_matrix, k):\n",
    "    \"\"\"Builds a kNN model on `train_matrix` with `k` neighbours.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    train_matrix: sp.csr_matrix\n",
    "        The sparse matrix used to build the kNN model.\n",
    "    k: int\n",
    "        The number of neighbours to use in the kNN model.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    NearestNeighbors\n",
    "        A NearestNeighbors model fit to `train_matrix`.\n",
    "    \n",
    "    \"\"\"\n",
    "    model_knn = NearestNeighbors(metric='cosine', algorithm='brute', n_neighbors=k)\n",
    "    model_knn.fit(train_matrix)\n",
    "    return model_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_preds_from_knn(knn_model, train_matrix, items, item_avgs):\n",
    "    \"\"\"Gets the kNN predictions for the items in `items`.\n",
    "    \n",
    "    This assumes that every item in items was fit on the\n",
    "    knn_model. This is just a precomputation step to get\n",
    "    the predictions for items in the training set.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    knn_model: NearestNeighbors\n",
    "        A NearestNeighbors model that has been fit.\n",
    "    train_matrix: sp.csr_matrix\n",
    "        The sparse matrix representing the training data.\n",
    "    items: np.array\n",
    "        An array of item indices for items in `knn_model`.\n",
    "    item_avgs: pd.DataFrame\n",
    "        A pandas dataframe containing the average rating for\n",
    "        each item in `items`.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame containing the predicted rating for each item\n",
    "        in `items`.\n",
    "    \n",
    "    \"\"\"\n",
    "    item_neighbors = np.asarray(knn_model.kneighbors(train_matrix, return_distance=False))\n",
    "    knn_avgs = np.zeros(len(item_neighbors))   # this is more efficient than appending multiple times (no resizing)\n",
    "    for i in range(len(item_neighbors)):\n",
    "        knn_avgs[i] = item_avgs['itemAverage'][items[item_neighbors[i]]].mean()    # average of average ratings for neighbors\n",
    "    return pd.concat([pd.DataFrame(items, columns=['itemID']),\n",
    "                      pd.DataFrame(knn_avgs, columns=['itemRating'])],\n",
    "                    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ratings(X, item_preds, default_val, merge_col):\n",
    "    \"\"\"Predicts the item ratings for the items in `X`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: pd.DataFrame\n",
    "        The DataFrame of features.\n",
    "    item_preds: pd.DataFrame\n",
    "        The DataFrame of predicted ratings for the items.\n",
    "    default_val: float\n",
    "        A default rating used for unseen items.\n",
    "    merge_col: str\n",
    "        The column to merge on.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame containing the predicted item ratings for\n",
    "        the records in `X`.\n",
    "    \n",
    "    \"\"\"\n",
    "    id_col = \"{}ID\".format(merge_col)\n",
    "    rating_col = \"{}Rating\".format(merge_col)\n",
    "    df_item = pd.merge(X, item_preds, how='left', on=[id_col])\n",
    "    df_item[rating_col] = df_item[rating_col].fillna(default_val)\n",
    "    df_item.index = X.index\n",
    "    return df_item[rating_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_knn_train_validation_preds(train_df, val_df, train_matrix, k, items, item_avgs):\n",
    "    \"\"\"Gets predictions on `train_df` and `val_df` from a kNN model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    train_df: pd.DataFrame\n",
    "        A DataFrame of the training data.\n",
    "    val_df: pd.DataFrame\n",
    "        A DataFrame of the validation data.\n",
    "    train_matrix: sp.csr_matrix\n",
    "        The sparse matrix used to train the kNN model.\n",
    "    k: int\n",
    "        The number of neighbours in the kNN model.\n",
    "    items: np.array\n",
    "        An array of strings representing the ids of the\n",
    "        items used in training.\n",
    "    item_avgs: pd.DataFrame\n",
    "        A DataFrame containing the average rating for the\n",
    "        items in `items`.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.array, np.array\n",
    "        Arrays of predictions on the training and validation sets, respectively.\n",
    "    \n",
    "    \"\"\"\n",
    "    knn_model = build_knn_model(train_matrix, k)\n",
    "    knn_preds = get_item_preds_from_knn(knn_model, train_matrix, items, item_avgs)\n",
    "    \n",
    "    # prediction for a new item\n",
    "    new_item_vec = np.zeros(train_matrix.shape[1])\n",
    "    new_item_neighbours = knn_model.kneighbors(new_item_vec.reshape(1, -1), return_distance=False)\n",
    "    new_item_pred = item_avgs['itemAverage'][items[new_item_neighbours[0]]].mean()\n",
    "    \n",
    "    train_pred = predict_ratings(train_df, knn_preds, new_item_pred, \"item\")\n",
    "    val_pred = predict_ratings(val_df, knn_preds, new_item_pred, \"item\")\n",
    "    return train_pred, val_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN with k = 1\n",
      "---------------\n",
      "Training MSE: 0.6054944420256291\n",
      "Validation MSE: 1.1887542546616416\n",
      "\n",
      "kNN with k = 2\n",
      "---------------\n",
      "Training MSE: 0.7876298298731864\n",
      "Validation MSE: 1.1489554773903679\n",
      "\n",
      "kNN with k = 5\n",
      "---------------\n",
      "Training MSE: 0.8917560361747044\n",
      "Validation MSE: 1.1251204748525687\n",
      "\n",
      "kNN with k = 10\n",
      "---------------\n",
      "Training MSE: 0.9132694240765542\n",
      "Validation MSE: 1.0281384304384038\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k_vals = [1, 2, 5, 10]\n",
    "\n",
    "for k in k_vals:\n",
    "    print(\"kNN with k = {}\".format(k))\n",
    "    print(\"---------------\")\n",
    "    train_preds, val_preds = get_item_knn_train_validation_preds(\n",
    "        train_data, val_data, user_item_train_matrix, k, item_matrix.index, train_item_avg)\n",
    "    print(\"Training MSE: {}\".format(calculate_MSE(train_preds, train_data['overall'])))\n",
    "    print(\"Validation MSE: {}\".format(calculate_MSE(val_preds, val_data['overall'])))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_matrix = train_data.pivot(index='reviewerID', columns='itemID', values='overall')\n",
    "user_matrix = user_matrix.fillna(0)\n",
    "user_item_train_matrix = sp.csr_matrix(user_matrix.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user_avg = train_data.groupby(train_data['reviewerID'], as_index=False)['overall'].mean()\n",
    "train_user_avg.columns = ['reviewerID', 'reviewerAverage']\n",
    "train_user_avg = train_user_avg.set_index('reviewerID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_preds_from_knn(knn_model, train_matrix, users, user_avgs):\n",
    "    \"\"\"Gets the kNN predictions for the user in `users`.\n",
    "    \n",
    "    This assumes that `knn_model` was fit on every user in \n",
    "    `users`. This is just a precomputation step to get\n",
    "    the predictions for users in the training set.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    knn_model: NearestNeighbors\n",
    "        A NearestNeighbors model that has been fit.\n",
    "    train_matrix: sp.csr_matrix\n",
    "        The sparse matrix representing the training data.\n",
    "    users: np.array\n",
    "        An array of user ids for users in `knn_model`.\n",
    "    user_avgs: pd.DataFrame\n",
    "        A pandas dataframe containing the average rating for\n",
    "        each user in `users`.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame containing the predicted rating for each user\n",
    "        in `users`.\n",
    "    \n",
    "    \"\"\"\n",
    "    user_neighbors = np.asarray(knn_model.kneighbors(train_matrix, return_distance=False))\n",
    "    knn_avgs = np.zeros(len(user_neighbors))   # this is more efficient than appending multiple times (no resizing)\n",
    "    for i in range(len(user_neighbors)):\n",
    "        knn_avgs[i] = user_avgs['reviewerAverage'][users[user_neighbors[i]]].mean()    # average of average ratings for neighbors\n",
    "    return pd.concat([pd.DataFrame(users, columns=['reviewerID']),\n",
    "                      pd.DataFrame(knn_avgs, columns=['reviewerRating'])],\n",
    "                    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_knn_train_validation_preds(train_df, val_df, train_matrix, k, users, user_avgs):\n",
    "    \"\"\"Gets predictions on `train_df` and `val_df` from a kNN model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    train_df: pd.DataFrame\n",
    "        A DataFrame of the training data.\n",
    "    val_df: pd.DataFrame\n",
    "        A DataFrame of the validation data.\n",
    "    train_matrix: sp.csr_matrix\n",
    "        The sparse matrix used to train the kNN model.\n",
    "    k: int\n",
    "        The number of neighbours in the kNN model.\n",
    "    users: np.array\n",
    "        An array of strings representing the ids of the\n",
    "        users used in training.\n",
    "    user_avgs: pd.DataFrame\n",
    "        A DataFrame containing the average rating for the\n",
    "        users in `users`.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.array, np.array\n",
    "        Arrays of predictions on the training and validation sets, respectively.\n",
    "    \n",
    "    \"\"\"\n",
    "    knn_model = build_knn_model(train_matrix, k)\n",
    "    knn_preds = get_user_preds_from_knn(knn_model, train_matrix, users, user_avgs)\n",
    "    \n",
    "    # prediction for a new user\n",
    "    new_user_vec = np.zeros(train_matrix.shape[1])\n",
    "    new_user_neighbours = knn_model.kneighbors(new_user_vec.reshape(1, -1), return_distance=False)\n",
    "    new_user_pred = user_avgs['reviewerAverage'][users[new_user_neighbours[0]]].mean()\n",
    "    \n",
    "    train_pred = predict_ratings(train_df, knn_preds, new_user_pred, \"reviewer\")\n",
    "    val_pred = predict_ratings(val_df, knn_preds, new_user_pred, \"reviewer\")\n",
    "    return train_pred, val_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN with k = 1\n",
      "---------------\n",
      "Training MSE: 0.6976994353937871\n",
      "Validation MSE: 1.253129016513731\n",
      "\n",
      "kNN with k = 2\n",
      "---------------\n",
      "Training MSE: 0.920613376246797\n",
      "Validation MSE: 1.6157140669012258\n",
      "\n",
      "kNN with k = 5\n",
      "---------------\n",
      "Training MSE: 0.836200292189724\n",
      "Validation MSE: 1.1936500936072167\n",
      "\n",
      "kNN with k = 10\n",
      "---------------\n",
      "Training MSE: 0.8726691799530596\n",
      "Validation MSE: 1.0170220310726248\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k_vals = [1, 2, 5, 10]\n",
    "\n",
    "for k in k_vals:\n",
    "    print(\"kNN with k = {}\".format(k))\n",
    "    print(\"---------------\")\n",
    "    train_preds, val_preds = get_user_knn_train_validation_preds(\n",
    "        train_data, val_data, user_item_train_matrix, k, user_matrix.index, train_user_avg)\n",
    "    print(\"Training MSE: {}\".format(calculate_MSE(train_preds, train_data['overall'])))\n",
    "    print(\"Validation MSE: {}\".format(calculate_MSE(val_preds, val_data['overall'])))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Models\n",
    "\n",
    "The first languge model is built with TfidfVectorizer (Term Frequency - Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['cleanedPrice', 'isPop', 'isAlternativeRock', 'isJazz', 'isClassical', 'isDanceElectronic', 'reviewWordCount']\n",
    "X_train_reg1 = X_train[columns_to_keep]\n",
    "X_val_reg1 = X_val[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-25-2b5d7d7878ed>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train_reg1['reviewWordCount'] = X_train_reg1['reviewWordCount'].apply(lambda x: np.log(x))\n",
      "<ipython-input-25-2b5d7d7878ed>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val_reg1['reviewWordCount'] = X_val_reg1['reviewWordCount'].apply(lambda x: np.log(x))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_train_reg1['reviewWordCount'] = X_train_reg1['reviewWordCount'].apply(lambda x: np.log(x))\n",
    "X_val_reg1['reviewWordCount'] = X_val_reg1['reviewWordCount'].apply(lambda x: np.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(df):\n",
    "    \"\"\"Cleans the dataset `df`. Removing null values.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame\n",
    "        The dataframe to be cleaned.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The cleaned dataframe obtained from `df`.\n",
    "    \n",
    "    \"\"\"\n",
    "    df.dropna(inplace=True)\n",
    "    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "    return df[indices_to_keep].astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-550e8da2fe0c>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.dropna(inplace=True)\n",
      "<ipython-input-26-550e8da2fe0c>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.dropna(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "X_train_reg1 = clean_dataset(X_train_reg1)\n",
    "y_train1 = y_train[y_train.index.isin(X_train_reg1.index)]\n",
    "X_train1 = X_train[X_train.index.isin(X_train_reg1.index)]\n",
    "\n",
    "X_val_reg1 = clean_dataset(X_val_reg1)\n",
    "y_val1 = y_val[y_val.index.isin(X_val_reg1.index)]\n",
    "X_val1 = X_val[X_val.index.isin(X_val_reg1.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mod = X_train_mod[X_train_mod.index.isin(X_train_reg1.index)]\n",
    "X_val_mod = X_val_mod[X_val_mod.index.isin(X_val_reg1.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reg1 = min_max_scaler.fit_transform(X_train_reg1)\n",
    "X_val_reg1 = min_max_scaler.transform(X_val_reg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha = 0.0\n",
      "------------\n",
      "Training Error: 0.9588535445992827\n",
      "Validation Error: 0.9262869599431929\n",
      "\n",
      "Alpha = 0.01\n",
      "------------\n",
      "Training Error: 0.9588428957078677\n",
      "Validation Error: 0.9262350113636552\n",
      "\n",
      "Alpha = 0.03\n",
      "------------\n",
      "Training Error: 0.9588433477027621\n",
      "Validation Error: 0.9262349757449848\n",
      "\n",
      "Alpha = 0.1\n",
      "------------\n",
      "Training Error: 0.9588449304068484\n",
      "Validation Error: 0.926234860637202\n",
      "\n",
      "Alpha = 0.3\n",
      "------------\n",
      "Training Error: 0.9588494564503761\n",
      "Validation Error: 0.9262346102237373\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "vthreshold_rating = np.vectorize(threshold_rating)\n",
    "\n",
    "alphas = [0.0, 0.01, 0.03, 0.1, 0.3]\n",
    "for alpha in alphas:\n",
    "    print(\"Alpha = {}\".format(alpha))\n",
    "    print(\"------------\")\n",
    "    reg_model = Ridge(alpha=alpha)\n",
    "    reg_model.fit(X_train_reg1, y_train1)\n",
    "    print(\"Training Error: {}\".format(calculate_MSE(y_train1, vthreshold_rating(reg_model.predict(X_train_reg1)))))\n",
    "    print(\"Validation Error: {}\".format(calculate_MSE(y_val1, vthreshold_rating(reg_model.predict(X_val_reg1)))))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Matthew/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def process_review_text(review_text, exclude_text, ps):\n",
    "    \"\"\"Pre-processes the text given by `review_text`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    review_text: str\n",
    "        The review text to be processed.\n",
    "    exclude_text: collection\n",
    "        A collection of words to be excluded.\n",
    "    ps: PorterStemmer\n",
    "        The PorterStemmer used to perform word stemming.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A string representing the processed version of `review_text`.\n",
    "    \n",
    "    \"\"\"\n",
    "    review = re.sub('[^a-zA-Z0-9]', ' ', review_text).lower().split()\n",
    "    review = [ps.stem(word) for word in review if not word in exclude_text]\n",
    "    return ' '.join(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-30-0d22f415ca83>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train1['processedReview'] = X_train1['fullReviewText'].apply(lambda x: process_review_text(x, exclude_english, ps))\n"
     ]
    }
   ],
   "source": [
    "exclude_english = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "X_train1['processedReview'] = X_train1['fullReviewText'].apply(lambda x: process_review_text(x, exclude_english, ps))\n",
    "X_val1['processedReview'] = X_val1['fullReviewText'].apply(lambda x: process_review_text(x, exclude_english, ps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TFIDF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cv = TfidfVectorizer()\n",
    "X_train_cv1 = cv.fit_transform(X_train1['processedReview'])\n",
    "X_val_cv1 = cv.transform(X_val1['processedReview'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "X_train_reg1_sp = sp.csr_matrix(X_train_reg1)\n",
    "X_train_cv_reg1 = sp.hstack((X_train_cv1, X_train_reg1_sp), format='csr')\n",
    "\n",
    "X_val_reg1_sp = sp.csr_matrix(X_val_reg1)\n",
    "X_val_cv_reg1 = sp.hstack((X_val_cv1, X_val_reg1_sp), format='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.03, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 1.657059470470493\n",
      "Validation Error: 1.643835435274604\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 1.6152387609816117\n",
      "Validation Error: 1.6027478863791698\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 1.5064334150057164\n",
      "Validation Error: 1.5156320233080705\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.9278305199606406\n",
      "Validation Error: 0.9140717888445251\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8659974631467421\n",
      "Validation Error: 0.8564736279922335\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7210557554945155\n",
      "Validation Error: 0.7517563198863484\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8420321019890331\n",
      "Validation Error: 0.8303079627493093\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.760438332990258\n",
      "Validation Error: 0.7566828311093866\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5896194009217547\n",
      "Validation Error: 0.6535086962140353\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7555609022602061\n",
      "Validation Error: 0.7485147430606864\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6590850071979402\n",
      "Validation Error: 0.6711793088009371\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.47360315538919273\n",
      "Validation Error: 0.5932841336237142\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 1000, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6855883782073438\n",
      "Validation Error: 0.6865773121105989\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 1000, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5876296474006382\n",
      "Validation Error: 0.6209610572979316\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 1000, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.39732599783579103\n",
      "Validation Error: 0.5663672308565999\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 2000, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6212677003234078\n",
      "Validation Error: 0.635851126577105\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 2000, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5205553944189065\n",
      "Validation Error: 0.5847359615797614\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 2000, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.32560582973057645\n",
      "Validation Error: 0.5534224735912971\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8545968150570561\n",
      "Validation Error: 0.8414255326803074\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7794450778503823\n",
      "Validation Error: 0.7738756431891397\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6129962177190347\n",
      "Validation Error: 0.6694610966797805\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7925282038733316\n",
      "Validation Error: 0.7834807180729493\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7024723613600568\n",
      "Validation Error: 0.70557621080342\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5223297054547508\n",
      "Validation Error: 0.6196355936520616\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7228788488981269\n",
      "Validation Error: 0.7194598043082383\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.627291375936086\n",
      "Validation Error: 0.6484349222902557\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.43942147911424845\n",
      "Validation Error: 0.5836214563767533\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6345008650217865\n",
      "Validation Error: 0.6464427410424495\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5356364937959868\n",
      "Validation Error: 0.5919058072540884\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.34107565980631027\n",
      "Validation Error: 0.5583308097709344\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 1000, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5751382642336652\n",
      "Validation Error: 0.6042656362545261\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 1000, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4674547805926879\n",
      "Validation Error: 0.5678206985403677\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 1000, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.26389885352292236\n",
      "Validation Error: 0.5533574690248638\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 2000, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.516572726861557\n",
      "Validation Error: 0.579436937688131\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 2000, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.396455433751136\n",
      "Validation Error: 0.555741968542181\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 2000, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.1830210928647965\n",
      "Validation Error: 0.5552062594121979\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.744051463481692\n",
      "Validation Error: 0.73679533762195\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6520878991813419\n",
      "Validation Error: 0.6677982783905524\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.47698980767163796\n",
      "Validation Error: 0.6027373381199861\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6745373545809814\n",
      "Validation Error: 0.6797227794827705\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.581382142653119\n",
      "Validation Error: 0.6194256185196815\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.3978382518415956\n",
      "Validation Error: 0.5827049991384502\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6122876045792917\n",
      "Validation Error: 0.631317910464996\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5156446019001003\n",
      "Validation Error: 0.588368027020797\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.321234731383588\n",
      "Validation Error: 0.574044496597672\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5360901839445277\n",
      "Validation Error: 0.5872589998388212\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4239971630356931\n",
      "Validation Error: 0.5674633156890021\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.2097980605071751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: 0.5755454593030278\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 1000, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.47510323441389246\n",
      "Validation Error: 0.5700952445865527\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 1000, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.34608801412853196\n",
      "Validation Error: 0.5653763316636292\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 1000, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.12860696671980432\n",
      "Validation Error: 0.5820806629028914\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 2000, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4057182092140655\n",
      "Validation Error: 0.5615677713800927\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 2000, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.2572209693941599\n",
      "Validation Error: 0.5717024770539526\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 2000, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.06158134148522578\n",
      "Validation Error: 0.5941727681381203\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6884696559911426\n",
      "Validation Error: 0.6901496898328511\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6040972142598152\n",
      "Validation Error: 0.640601724855694\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4281311531232244\n",
      "Validation Error: 0.6062766114411388\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6258774251878654\n",
      "Validation Error: 0.6456346335434061\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5391333421637524\n",
      "Validation Error: 0.601871342035567\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.3498182913642454\n",
      "Validation Error: 0.5995151465293299\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5699934202836568\n",
      "Validation Error: 0.608549780432093\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.47151235059922875\n",
      "Validation Error: 0.5851280574783898\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.2666964580782263\n",
      "Validation Error: 0.6020804870484332\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4921170253057247\n",
      "Validation Error: 0.5832150541871\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.3699860060813426\n",
      "Validation Error: 0.5763399485382776\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.1531064452882579\n",
      "Validation Error: 0.6127551826258106\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 1000, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.42479532043043283\n",
      "Validation Error: 0.5756453073438189\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 1000, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.28104789880097103\n",
      "Validation Error: 0.5815911921924128\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 1000, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.0765476377520462\n",
      "Validation Error: 0.631059336390693\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 2000, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.34663535703830856\n",
      "Validation Error: 0.5779904777414281\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 2000, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.18620836936464788\n",
      "Validation Error: 0.5937201839185794\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 2000, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.026037115073510343\n",
      "Validation Error: 0.6410182223229509\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "learning_rates = [0.03, 0.1, 0.3, 0.5]\n",
    "estimators = [50, 100, 200, 500, 1000, 2000]\n",
    "depths = [1, 2, 5]\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for estimator in estimators:\n",
    "        for depth in depths:\n",
    "            print(\"Learning Rate: {0}, # Estimators: {1}, Depth: {2}\".format(learning_rate, estimator, depth))\n",
    "            print(\"--------------------------------------------------\")\n",
    "            xg_reg = XGBRegressor(\n",
    "                learning_rate=learning_rate, max_depth=depth, n_estimators=estimator)\n",
    "            xg_reg.fit(X_train_cv_reg1, y_train1)\n",
    "            print(\"Training Error: {}\".format(calculate_MSE(y_train1, vthreshold_rating(xg_reg.predict(X_train_cv_reg1)))))\n",
    "            print(\"Validation Error: {}\".format(calculate_MSE(y_val1, vthreshold_rating(xg_reg.predict(X_val_cv_reg1)))))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that `learning_rate=0.1`, `n_estimators=2000`, and `max_depth=1` provides a very good model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error based on XGBoost CountVectorizer prediction: 0.517\n",
      "Validation error based on XGBoost CountVectorizer prediction: 0.579\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "xg_reg = XGBRegressor(learning_rate=0.1, n_estimators=2000, max_depth=1)\n",
    "xg_reg.fit(X_train_cv_reg1, y_train1)\n",
    "\n",
    "train_MSE = calculate_MSE(y_train1, vthreshold_rating(xg_reg.predict(X_train_cv_reg1)))\n",
    "val_MSE = calculate_MSE(y_val1, vthreshold_rating(xg_reg.predict(X_val_cv_reg1)))\n",
    "\n",
    "print(\"Training error based on XGBoost CountVectorizer prediction: %.3f\" % train_MSE)\n",
    "print(\"Validation error based on XGBoost CountVectorizer prediction: %.3f\" % val_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2v = Word2Vec(list(X_train1['processedReview']), size=350, window=10, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_review_vector(review_text):\n",
    "    \"\"\"Creates a vector for the review given by `review_text`.\n",
    "    \n",
    "    The word vectors for each word in the review are averaged\n",
    "    to build a vector for the review.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    review_text: str\n",
    "        The review for which the vector is generated.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    vector\n",
    "        A vector for the review.\n",
    "    \n",
    "    \"\"\"\n",
    "    review = [word for word in review_text if word in w2v.wv.vocab]\n",
    "    if len(review) > 0:\n",
    "        return np.mean(w2v[review], axis=0)\n",
    "    return np.zeros(350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-82-07f028c5b26d>:20: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  return np.mean(w2v[review], axis=0)\n",
      "<ipython-input-83-ba0c28ab83e8>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train1['review_vector'] = X_train1['processedReview'].apply(lambda x: create_review_vector(x))\n"
     ]
    }
   ],
   "source": [
    "X_train1['review_vector'] = X_train1['processedReview'].apply(lambda x: create_review_vector(x))\n",
    "X_val1['review_vector'] = X_val1['processedReview'].apply(lambda x: create_review_vector(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_review_vec_df(review_vecs, indices):\n",
    "    \"\"\"Creates a dataframe from `review_vecs`.\n",
    "    \n",
    "    Each numpy array in review_vecs is converted to a \n",
    "    row in the resulting dataframe.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    review_vecs: list\n",
    "        A list of numpy arrays where each array corresponds\n",
    "        to the review vector for a review.\n",
    "    indicies: np.array\n",
    "        A numpy array of indices for the DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The DataFrame obtained from converting `review_vecs`\n",
    "        to a dataframe.\n",
    "    \n",
    "    \"\"\"\n",
    "    review_vec_df = pd.DataFrame(np.vstack(review_vecs))\n",
    "    review_vec_df.columns = [\"word\" + str(col) for col in review_vec_df.columns]\n",
    "    review_vec_df.index = indices\n",
    "    return review_vec_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_wv = create_review_vec_df(X_train1['review_vector'], X_train1.index)\n",
    "X_val_wv = create_review_vec_df(X_val1['review_vector'], X_val1.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reg1_df = pd.DataFrame(np.vstack(X_train_reg1))\n",
    "X_train_reg1_df.index = X_train1.index\n",
    "\n",
    "X_val_reg1_df = pd.DataFrame(np.vstack(X_val_reg1))\n",
    "X_val_reg1_df.index = X_val1.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_wv_reg = pd.concat([X_train_wv, X_train_reg1_df], axis=1)\n",
    "X_val_wv_reg = pd.concat([X_val_wv, X_val_reg1_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha = 0.0\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=6.37186e-18): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error: 0.8940535781682711\n",
      "Validation Error: 0.9018064217232329\n",
      "\n",
      "Alpha = 0.01\n",
      "------------\n",
      "Training Error: 0.9079188841981554\n",
      "Validation Error: 0.9021185371572493\n",
      "\n",
      "Alpha = 0.03\n",
      "------------\n",
      "Training Error: 0.907959198977622\n",
      "Validation Error: 0.9021166181450053\n",
      "\n",
      "Alpha = 0.1\n",
      "------------\n",
      "Training Error: 0.9080974389160185\n",
      "Validation Error: 0.9021763444454539\n",
      "\n",
      "Alpha = 0.3\n",
      "------------\n",
      "Training Error: 0.9083702951020889\n",
      "Validation Error: 0.9023891975104213\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "alphas = [0.0, 0.01, 0.03, 0.1, 0.3]\n",
    "for alpha in alphas:\n",
    "    print(\"Alpha = {}\".format(alpha))\n",
    "    print(\"------------\")\n",
    "    reg_model = Ridge(alpha=alpha)\n",
    "    reg_model.fit(X_train_wv_reg, y_train1)\n",
    "    print(\"Training Error: {}\".format(calculate_MSE(y_train1, vthreshold_rating(reg_model.predict(X_train_wv_reg)))))\n",
    "    print(\"Validation Error: {}\".format(calculate_MSE(y_val1, vthreshold_rating(reg_model.predict(X_val_wv_reg)))))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.03, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 1.6765857758092944\n",
      "Validation Error: 1.6647381372164318\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 1.6580919193530188\n",
      "Validation Error: 1.647719877424904\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 1.608621216796498\n",
      "Validation Error: 1.6276771719437315\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.9686630666825585\n",
      "Validation Error: 0.96021394548407\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.9469037643105891\n",
      "Validation Error: 0.9420444818347719\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8693182573628832\n",
      "Validation Error: 0.9210646118398489\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.9210158070137735\n",
      "Validation Error: 0.9145612959951956\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8950339252158956\n",
      "Validation Error: 0.8963131488685573\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7806337257735777\n",
      "Validation Error: 0.8801334926646515\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.9047880967011823\n",
      "Validation Error: 0.9018306637183156\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8717430765777114\n",
      "Validation Error: 0.8874012535422684\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6753387441185651\n",
      "Validation Error: 0.8795959004553644\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 1000, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8932517468796682\n",
      "Validation Error: 0.8957477878079494\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 1000, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8498530658144292\n",
      "Validation Error: 0.8835547148844185\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 1000, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5401318383248765\n",
      "Validation Error: 0.884903781153897\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 2000, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8816510027360334\n",
      "Validation Error: 0.8912973513891556\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 2000, Depth: 2\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-e677b0c5c51e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m             xg_reg = XGBRegressor(\n\u001b[1;32m     11\u001b[0m                 learning_rate=learning_rate, max_depth=depth, n_estimators=estimator)\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mxg_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_wv_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Error: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalculate_MSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvthreshold_rating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxg_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_wv_reg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation Error: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalculate_MSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvthreshold_rating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxg_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_wv_reg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    540\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'eval_metric'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         self._Booster = train(params, train_dmatrix,\n\u001b[0m\u001b[1;32m    543\u001b[0m                               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_num_boosting_rounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m     return _train_internal(params, dtrain,\n\u001b[0m\u001b[1;32m    209\u001b[0m                            \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_boost_round\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0m\u001b[1;32m   1160\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m                                                     dtrain.handle))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rates = [0.03, 0.1, 0.3, 0.5]\n",
    "estimators = [50, 100, 200, 500, 1000, 2000]\n",
    "depths = [1, 2, 5]\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for estimator in estimators:\n",
    "        for depth in depths:\n",
    "            print(\"Learning Rate: {0}, # Estimators: {1}, Depth: {2}\".format(learning_rate, estimator, depth))\n",
    "            print(\"--------------------------------------------------\")\n",
    "            xg_reg = XGBRegressor(\n",
    "                learning_rate=learning_rate, max_depth=depth, n_estimators=estimator)\n",
    "            xg_reg.fit(X_train_wv_reg, y_train1)\n",
    "            print(\"Training Error: {}\".format(calculate_MSE(y_train1, vthreshold_rating(xg_reg.predict(X_train_wv_reg)))))\n",
    "            print(\"Validation Error: {}\".format(calculate_MSE(y_val1, vthreshold_rating(xg_reg.predict(X_val_wv_reg)))))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta Models\n",
    "\n",
    "In this section we look at models that combine both collaborative filtering and language models.\n",
    "\n",
    "We start by using the predictions of the collaborative filtering as features in our language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['cleanedPrice', 'isPop', 'isAlternativeRock', 'isJazz', 'isClassical', 'isDanceElectronic', 'reviewWordCount', 'userRating', 'itemRating']\n",
    "X_train_reg2 = X_train_mod[columns_to_keep]\n",
    "X_val_reg2 = X_val_mod[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-105-d8a296a7aa94>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train_reg2['reviewWordCount'] = X_train_reg2['reviewWordCount'].apply(lambda x: np.log(x))\n",
      "<ipython-input-105-d8a296a7aa94>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val_reg2['reviewWordCount'] = X_val_reg2['reviewWordCount'].apply(lambda x: np.log(x))\n"
     ]
    }
   ],
   "source": [
    "min_max_scaler = MinMaxScaler()\n",
    "X_train_reg2['reviewWordCount'] = X_train_reg2['reviewWordCount'].apply(lambda x: np.log(x))\n",
    "X_val_reg2['reviewWordCount'] = X_val_reg2['reviewWordCount'].apply(lambda x: np.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-100-550e8da2fe0c>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.dropna(inplace=True)\n",
      "<ipython-input-100-550e8da2fe0c>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.dropna(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "X_train_reg2 = clean_dataset(X_train_reg2)\n",
    "y_train2 = y_train[y_train.index.isin(X_train_reg2.index)]\n",
    "X_train2 = X_train[X_train.index.isin(X_train_reg2.index)]\n",
    "\n",
    "X_val_reg2 = clean_dataset(X_val_reg2)\n",
    "y_val2 = y_val[y_val.index.isin(X_val_reg2.index)]\n",
    "X_val2 = X_val[X_val.index.isin(X_val_reg2.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reg2 = min_max_scaler.fit_transform(X_train_reg2)\n",
    "X_val_reg2 = min_max_scaler.transform(X_val_reg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha = 0.0\n",
      "------------\n",
      "Training Error: 0.7891828923294948\n",
      "Validation Error: 0.9486008427597297\n",
      "\n",
      "Alpha = 0.01\n",
      "------------\n",
      "Training Error: 0.7891677848703272\n",
      "Validation Error: 0.9484601751838659\n",
      "\n",
      "Alpha = 0.03\n",
      "------------\n",
      "Training Error: 0.7891678465440518\n",
      "Validation Error: 0.9484577298907713\n",
      "\n",
      "Alpha = 0.1\n",
      "------------\n",
      "Training Error: 0.7891680642355218\n",
      "Validation Error: 0.948449171698209\n",
      "\n",
      "Alpha = 0.3\n",
      "------------\n",
      "Training Error: 0.7891687018526102\n",
      "Validation Error: 0.9484247228291963\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.0, 0.01, 0.03, 0.1, 0.3]\n",
    "for alpha in alphas:\n",
    "    print(\"Alpha = {}\".format(alpha))\n",
    "    print(\"------------\")\n",
    "    reg_model = Ridge(alpha=alpha)\n",
    "    reg_model.fit(X_train_reg2, y_train2)\n",
    "    print(\"Training Error: {}\".format(calculate_MSE(y_train2, vthreshold_rating(reg_model.predict(X_train_reg2)))))\n",
    "    print(\"Validation Error: {}\".format(calculate_MSE(y_val2, vthreshold_rating(reg_model.predict(X_val_reg2)))))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-109-25810bf4f6a4>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train2['processedReview'] = X_train2['fullReviewText'].apply(lambda x: process_review_text(x, exclude_english, ps))\n",
      "<ipython-input-109-25810bf4f6a4>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val2['processedReview'] = X_val2['fullReviewText'].apply(lambda x: process_review_text(x, exclude_english, ps))\n"
     ]
    }
   ],
   "source": [
    "exclude_english = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "X_train2['processedReview'] = X_train2['fullReviewText'].apply(lambda x: process_review_text(x, exclude_english, ps))\n",
    "X_val2['processedReview'] = X_val2['fullReviewText'].apply(lambda x: process_review_text(x, exclude_english, ps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_features=1500)\n",
    "X_train_cv2 = cv.fit_transform(X_train2['processedReview'])\n",
    "X_val_cv2 = cv.transform(X_val2['processedReview'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reg2_sp = sp.csr_matrix(X_train_reg2)\n",
    "X_train_cv_reg2 = sp.hstack((X_train_cv2, X_train_reg2_sp), format='csr')\n",
    "\n",
    "X_val_reg2_sp = sp.csr_matrix(X_val_reg2)\n",
    "X_val_cv_reg2 = sp.hstack((X_val_cv2, X_val_reg2_sp), format='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.01, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 6.503735872991274\n",
      "Validation Error: 6.883393063136512\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 6.472925759109321\n",
      "Validation Error: 6.873810349242472\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 6.415552951975483\n",
      "Validation Error: 6.766778004848318\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 6.336512333143151\n",
      "Validation Error: 6.702224040577343\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 2.9116597565047635\n",
      "Validation Error: 3.2502935567689333\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 2.868043939305317\n",
      "Validation Error: 3.208625304323009\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 2.7780535325484026\n",
      "Validation Error: 3.092532168888972\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 2.6461880452656565\n",
      "Validation Error: 3.0224225434770875\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 1.0918346220330994\n",
      "Validation Error: 1.3135588463747994\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 1.036147689801702\n",
      "Validation Error: 1.2635888455120803\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.9146986694500452\n",
      "Validation Error: 1.1616875425943325\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7337391925862761\n",
      "Validation Error: 1.0908617788789443\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.756386356086193\n",
      "Validation Error: 0.8996191559372384\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6839209384629215\n",
      "Validation Error: 0.8403416793262685\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5360902242478431\n",
      "Validation Error: 0.7367690362634365\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.31446922435595465\n",
      "Validation Error: 0.6811167992372987\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 1.5627177723097216\n",
      "Validation Error: 1.846452518919343\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 1.5119653601832954\n",
      "Validation Error: 1.7960544459510177\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 1.4032360632395287\n",
      "Validation Error: 1.6804284272855077\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 1.2401300435868885\n",
      "Validation Error: 1.6018033435097432\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8239466745938151\n",
      "Validation Error: 0.9942010555413113\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7620201935774863\n",
      "Validation Error: 0.9443683509025774\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6266979082527001\n",
      "Validation Error: 0.8415338280252339\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.42568372015336575\n",
      "Validation Error: 0.7730068336676122\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7426928131144244\n",
      "Validation Error: 0.885071876050487\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6671662762814085\n",
      "Validation Error: 0.8232709367647633\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.515852358064282\n",
      "Validation Error: 0.7198106463169784\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.2890217747658168\n",
      "Validation Error: 0.6667757772826471\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6771667436192434\n",
      "Validation Error: 0.8162237821223852\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5930182057061686\n",
      "Validation Error: 0.7417321041296694\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8356626506024096\n",
      "Validation Error: 0.6542027155916784\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.1741651887529198\n",
      "Validation Error: 0.6306806169805278\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7532719545067948\n",
      "Validation Error: 0.8984023303350915\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6823394868546818\n",
      "Validation Error: 0.8415287026010446\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5346141250969264\n",
      "Validation Error: 0.7337423743840931\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.3130148012635119\n",
      "Validation Error: 0.6872977133258573\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7040695828928955\n",
      "Validation Error: 0.8450873275924572\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6234277468668492\n",
      "Validation Error: 0.778036234504546\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4601100858885094\n",
      "Validation Error: 0.677615698823088\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6884971464806595\n",
      "Validation Error: 0.6534113519806398\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6550334651549221\n",
      "Validation Error: 0.79237246870429\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5693960927723146\n",
      "Validation Error: 0.7172584528181635\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7959670259987318\n",
      "Validation Error: 0.6401843150790001\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.14231621233302494\n",
      "Validation Error: 0.6375265433888077\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6019752200057845\n",
      "Validation Error: 0.7323421618628351\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8891819911223843\n",
      "Validation Error: 0.6624839452884889\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.29619556817303605\n",
      "Validation Error: 0.6161472705041048\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error: 0.06626693654491513\n",
      "Validation Error: 0.629127338706747\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6695470399035651\n",
      "Validation Error: 0.8320175368867865\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5877134032041788\n",
      "Validation Error: 0.7312280170602444\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.821128725428028\n",
      "Validation Error: 0.6644344297462841\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.18017706776835557\n",
      "Validation Error: 0.6767750787866058\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6250522630858881\n",
      "Validation Error: 0.7799276543267967\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5423968546848821\n",
      "Validation Error: 0.6814512809318369\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7572352568167406\n",
      "Validation Error: 0.6419165688148936\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.10392467283260896\n",
      "Validation Error: 0.6762583510226913\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5903803945789322\n",
      "Validation Error: 0.7412608779634012\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5016847541934427\n",
      "Validation Error: 0.6515057605621919\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.28299454631394083\n",
      "Validation Error: 0.6316645878775758\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.05050973316984014\n",
      "Validation Error: 0.6759090507785038\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5541520219459554\n",
      "Validation Error: 0.6982901469331604\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8248065948002536\n",
      "Validation Error: 0.626830781451005\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.1817675341128254\n",
      "Validation Error: 0.6344574740234872\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.009288338708908602\n",
      "Validation Error: 0.6786818518535688\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.635053766967533\n",
      "Validation Error: 0.8025454034164896\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5565634106569863\n",
      "Validation Error: 0.698938232190281\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.37772052797915046\n",
      "Validation Error: 0.6784713122415739\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.13745887321542427\n",
      "Validation Error: 0.7281470255669896\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5974075260376706\n",
      "Validation Error: 0.7515024210684589\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8964870006341155\n",
      "Validation Error: 0.6696219617387341\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.3067495566385541\n",
      "Validation Error: 0.6694607004808133\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.07818992036583028\n",
      "Validation Error: 0.7319921716274881\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5674944647436064\n",
      "Validation Error: 0.7110229400138558\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.47198463424299336\n",
      "Validation Error: 0.6490063890569089\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.2281092082780791\n",
      "Validation Error: 0.6684967836349853\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.02496529929461796\n",
      "Validation Error: 0.7367603182741793\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5343279686884503\n",
      "Validation Error: 0.6809905159455284\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7973367152821813\n",
      "Validation Error: 0.6322757540964926\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.12355123516941925\n",
      "Validation Error: 0.6746721036001143\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.0011216059666213293\n",
      "Validation Error: 0.7389609370739282\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.01, 0.03, 0.1, 0.3, 0.5]\n",
    "estimators = [50, 100, 200, 500]\n",
    "depths = [1, 2, 5, 10]\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for estimator in estimators:\n",
    "        for depth in depths:\n",
    "            print(\"Learning Rate: {0}, # Estimators: {1}, Depth: {2}\".format(learning_rate, estimator, depth))\n",
    "            print(\"--------------------------------------------------\")\n",
    "            xg_reg = XGBRegressor(\n",
    "                learning_rate=learning_rate, max_depth=depth, n_estimators=estimator)\n",
    "            xg_reg.fit(X_train_cv_reg2, y_train2)\n",
    "            print(\"Training Error: {}\".format(calculate_MSE(y_train2, vthreshold_rating(xg_reg.predict(X_train_cv_reg2)))))\n",
    "            print(\"Validation Error: {}\".format(calculate_MSE(y_val2, vthreshold_rating(xg_reg.predict(X_val_cv_reg2)))))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems `learning_rate=0.3`, `n_estimators=200`, `max_depth=2` performs the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error based on XGBoost CountVectorizer prediction: 0.502\n",
      "Validation error based on XGBoost CountVectorizer prediction: 0.652\n"
     ]
    }
   ],
   "source": [
    "xg_reg = XGBRegressor(learning_rate=0.3, n_estimators=200, max_depth=2)\n",
    "xg_reg.fit(X_train_cv_reg2, y_train2)\n",
    "\n",
    "train_MSE = calculate_MSE(y_train2, vthreshold_rating(xg_reg.predict(X_train_cv_reg2)))\n",
    "val_MSE = calculate_MSE(y_val2, vthreshold_rating(xg_reg.predict(X_val_cv_reg2)))\n",
    "\n",
    "print(\"Training error based on XGBoost CountVectorizer prediction: %.3f\" % train_MSE)\n",
    "print(\"Validation error based on XGBoost CountVectorizer prediction: %.3f\" % val_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is actually much worse compared to the pure language model.\n",
    "\n",
    "However, we could also create a meta model by taking a weighted average of predictions from collaborative filtering and the pure language model. We now try this for a few candidate weightings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight: 0.0\n",
      "------------\n",
      "Training error: 0.812\n",
      "Validation error: 0.950\n",
      "\n",
      "Weight: 0.1\n",
      "------------\n",
      "Training error: 0.754\n",
      "Validation error: 0.882\n",
      "\n",
      "Weight: 0.3\n",
      "------------\n",
      "Training error: 0.658\n",
      "Validation error: 0.768\n",
      "\n",
      "Weight: 0.5\n",
      "------------\n",
      "Training error: 0.587\n",
      "Validation error: 0.681\n",
      "\n",
      "Weight: 0.7\n",
      "------------\n",
      "Training error: 0.542\n",
      "Validation error: 0.621\n",
      "\n",
      "Weight: 0.9\n",
      "------------\n",
      "Training error: 0.522\n",
      "Validation error: 0.589\n",
      "\n",
      "Weight: 1.0\n",
      "------------\n",
      "Training error: 0.522\n",
      "Validation error: 0.583\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weights = [0.0, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0]\n",
    "\n",
    "xg_reg = XGBRegressor(learning_rate=0.3, n_estimators=500, max_depth=2)\n",
    "xg_reg.fit(X_train_cv_reg1, y_train1)\n",
    "\n",
    "reg_train_preds = vthreshold_rating(xg_reg.predict(X_train_cv_reg1))\n",
    "reg_val_preds = vthreshold_rating(xg_reg.predict(X_val_cv_reg1))\n",
    "\n",
    "cf_train_preds = vthreshold_rating(X_train_mod['pred'])\n",
    "cf_val_preds = vthreshold_rating(X_val_mod['pred'])\n",
    "\n",
    "for weight in weights:\n",
    "    print(\"Weight: %.1f\" % weight)\n",
    "    print(\"------------\")\n",
    "    train_MSE = calculate_MSE(y_train1, ((weight*reg_train_preds) + ((1.0 - weight)*cf_train_preds)))\n",
    "    val_MSE = calculate_MSE(y_val1, ((weight*reg_val_preds) + ((1.0 - weight)*cf_val_preds)))\n",
    "    print(\"Training error: %.3f\" % train_MSE)\n",
    "    print(\"Validation error: %.3f\" % val_MSE)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language and Popularity\n",
    "\n",
    "Combining a language model with features representing how popular an item is (how many times it was rated) and how often the user rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_rating_counts = X_train.groupby(X_train['reviewerID'])['price'].count()\n",
    "item_rating_counts = X_train.groupby(X_train['itemID'])['price'].count()\n",
    "user_rating_counts = pd.DataFrame(\n",
    "    user_rating_counts.values, columns=['userRatingCount'], index=user_rating_counts.index).reset_index()\n",
    "item_rating_counts = pd.DataFrame(\n",
    "    item_rating_counts.values, columns=['itemRatingCount'], index=item_rating_counts.index).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_to_zero(val):\n",
    "    \"\"\"Converts `val` to 0 if it is null. Otherwise, `val` is unchanged.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    val: int\n",
    "        The value to be converted.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        0 if `val` is null. Otherwise `val`.\n",
    "    \n",
    "    \"\"\"\n",
    "    if pd.isnull(val):\n",
    "        return 0\n",
    "    return val\n",
    "\n",
    "def append_user_item_counts(data_df, user_counts, item_counts):\n",
    "    \"\"\"Appends `user_counts` and `item_counts` to `data_df`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_df: pd.DataFrame\n",
    "        The DataFrame being appended to.\n",
    "    user_counts: pd.DataFrame\n",
    "        A DataFrame containing user counts.\n",
    "    item_counts: pd.DataFrame\n",
    "        A DataFrame containing item counts.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The DataFrame obtained from `data_df` after appending\n",
    "        `user_counts` and `item_counts`.\n",
    "    \n",
    "    \"\"\"\n",
    "    user_df = pd.merge(data_df, user_counts, how='left', left_on='reviewerID', right_on='reviewerID')\n",
    "    user_item_df = pd.merge(user_df, item_counts, how='left', left_on='itemID', right_on='itemID')\n",
    "    user_item_df['userRatingCount'] = user_item_df['userRatingCount'].apply(null_to_zero)\n",
    "    user_item_df['itemRatingCount'] = user_item_df['itemRatingCount'].apply(null_to_zero)\n",
    "    user_item_df.index = data_df.index\n",
    "    return user_item_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = append_user_item_counts(X_train, user_rating_counts, item_rating_counts)\n",
    "val_df = append_user_item_counts(X_val, user_rating_counts, item_rating_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['cleanedPrice', 'isPop', 'isAlternativeRock', 'isJazz', 'isClassical', 'isDanceElectronic', 'reviewWordCount', 'userRatingCount', 'itemRatingCount']\n",
    "X_train_reg3 = train_df[columns_to_keep]\n",
    "X_val_reg3 = val_df[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-94-063b9bc4b222>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train_reg3['reviewWordCount'] = X_train_reg3['reviewWordCount'].apply(lambda x: np.log(x))\n",
      "<ipython-input-94-063b9bc4b222>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val_reg3['reviewWordCount'] = X_val_reg3['reviewWordCount'].apply(lambda x: np.log(x))\n"
     ]
    }
   ],
   "source": [
    "min_max_scaler = MinMaxScaler()\n",
    "X_train_reg3['reviewWordCount'] = X_train_reg3['reviewWordCount'].apply(lambda x: np.log(x))\n",
    "X_val_reg3['reviewWordCount'] = X_val_reg3['reviewWordCount'].apply(lambda x: np.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-47-ab573a02d313>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.dropna(inplace=True)\n",
      "<ipython-input-47-ab573a02d313>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.dropna(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "X_train_reg3 = clean_dataset(X_train_reg3)\n",
    "y_train3 = y_train[y_train.index.isin(X_train_reg3.index)]\n",
    "X_train3 = X_train[X_train.index.isin(X_train_reg3.index)]\n",
    "\n",
    "X_val_reg3 = clean_dataset(X_val_reg3)\n",
    "y_val3 = y_val[y_val.index.isin(X_val_reg3.index)]\n",
    "X_val3 = X_val[X_val.index.isin(X_val_reg3.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reg3 = min_max_scaler.fit_transform(X_train_reg3)\n",
    "X_val_reg3 = min_max_scaler.transform(X_val_reg3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha = 0.0\n",
      "------------\n",
      "Training Error: 0.9568060307877613\n",
      "Validation Error: 0.9646544613043789\n",
      "\n",
      "Alpha = 0.01\n",
      "------------\n",
      "Training Error: 0.956800728863852\n",
      "Validation Error: 0.9647191518211016\n",
      "\n",
      "Alpha = 0.03\n",
      "------------\n",
      "Training Error: 0.9568009871918867\n",
      "Validation Error: 0.9647195661109845\n",
      "\n",
      "Alpha = 0.1\n",
      "------------\n",
      "Training Error: 0.9568018951045272\n",
      "Validation Error: 0.9647210150463716\n",
      "\n",
      "Alpha = 0.3\n",
      "------------\n",
      "Training Error: 0.9568045191684653\n",
      "Validation Error: 0.9647251444048965\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "vthreshold_rating = np.vectorize(threshold_rating)\n",
    "\n",
    "alphas = [0.0, 0.01, 0.03, 0.1, 0.3]\n",
    "for alpha in alphas:\n",
    "    print(\"Alpha = {}\".format(alpha))\n",
    "    print(\"------------\")\n",
    "    reg_model = Ridge(alpha=alpha)\n",
    "    reg_model.fit(X_train_reg3, y_train3)\n",
    "    print(\"Training Error: {}\".format(calculate_MSE(y_train3, vthreshold_rating(reg_model.predict(X_train_reg3)))))\n",
    "    print(\"Validation Error: {}\".format(calculate_MSE(y_val3, vthreshold_rating(reg_model.predict(X_val_reg3)))))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-98-6f3f49189b95>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train3['processedReview'] = X_train3['fullReviewText'].apply(lambda x: process_review_text(x, exclude_english, ps))\n"
     ]
    }
   ],
   "source": [
    "exclude_english = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "X_train3['processedReview'] = X_train3['fullReviewText'].apply(lambda x: process_review_text(x, exclude_english, ps))\n",
    "X_val3['processedReview'] = X_val3['fullReviewText'].apply(lambda x: process_review_text(x, exclude_english, ps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = TfidfVectorizer()\n",
    "X_train_cv3 = cv.fit_transform(X_train3['processedReview'])\n",
    "X_val_cv3 = cv.transform(X_val3['processedReview'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reg3_sp = sp.csr_matrix(X_train_reg3)\n",
    "X_train_cv_reg3 = sp.hstack((X_train_cv3, X_train_reg3_sp), format='csr')\n",
    "\n",
    "X_val_reg3_sp = sp.csr_matrix(X_val_reg3)\n",
    "X_val_cv_reg3 = sp.hstack((X_val_cv3, X_val_reg3_sp), format='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.03, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 1.6475736165735617\n",
      "Validation Error: 1.6682075246391694\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 1.6071295532038965\n",
      "Validation Error: 1.6310201443086647\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 1.5028318707489112\n",
      "Validation Error: 1.5495761671615798\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.9188830540222396\n",
      "Validation Error: 0.9353547910345448\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8600495191978598\n",
      "Validation Error: 0.881932049456795\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7201781189903523\n",
      "Validation Error: 0.7801585045289339\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8358542125593243\n",
      "Validation Error: 0.8524116989451223\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7575521211963017\n",
      "Validation Error: 0.7816492401441552\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5911242521683677\n",
      "Validation Error: 0.6799836218202735\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7522296784766774\n",
      "Validation Error: 0.7710499979951905\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6590958734068679\n",
      "Validation Error: 0.6963058635540857\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.47524779223081454\n",
      "Validation Error: 0.6169625168916922\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 1000, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6840239875276554\n",
      "Validation Error: 0.708668378563387\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 1000, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5883616702456608\n",
      "Validation Error: 0.6456573269501363\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 1000, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.3945263767180505\n",
      "Validation Error: 0.5883451835989083\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 2000, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6207078584437424\n",
      "Validation Error: 0.6580050611387002\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 2000, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.520265334484549\n",
      "Validation Error: 0.6081044312378009\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 2000, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.31576463665444865\n",
      "Validation Error: 0.5691880078101187\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8477734920732338\n",
      "Validation Error: 0.8635943746474168\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7749300669574346\n",
      "Validation Error: 0.7981384547602882\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6167290560220933\n",
      "Validation Error: 0.6956828488938885\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7882345587966298\n",
      "Validation Error: 0.8054653617464994\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7006842326345166\n",
      "Validation Error: 0.7302868619337366\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5249558345103099\n",
      "Validation Error: 0.6427159067885031\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7203132538824941\n",
      "Validation Error: 0.7408813852056034\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6265104438792016\n",
      "Validation Error: 0.6727893088001192\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4416964683999406\n",
      "Validation Error: 0.6056129960291073\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.633835093681103\n",
      "Validation Error: 0.6679129018559286\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.535126030404487\n",
      "Validation Error: 0.6147146349358291\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.33484181551582953\n",
      "Validation Error: 0.5781107129023814\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 1000, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5734279248759746\n",
      "Validation Error: 0.6253798643746618\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 1000, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4646703381592451\n",
      "Validation Error: 0.5849421595422992\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 1000, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.2582097930287942\n",
      "Validation Error: 0.5684419865565185\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 2000, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5126593867047677\n",
      "Validation Error: 0.5965906853543875\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 2000, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.3902803677299543\n",
      "Validation Error: 0.5710136122022752\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 2000, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.17732587253593382\n",
      "Validation Error: 0.5690646667309286\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7411732383827632\n",
      "Validation Error: 0.759707375282048\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6505469321804074\n",
      "Validation Error: 0.6895700971241407\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4727977921151798\n",
      "Validation Error: 0.6307768432871251\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6735473495300585\n",
      "Validation Error: 0.6989564952069758\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5822821205901875\n",
      "Validation Error: 0.6445336971168617\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.39029499049662936\n",
      "Validation Error: 0.6072173975217089\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6117261411566616\n",
      "Validation Error: 0.6496854485676338\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5124212346375444\n",
      "Validation Error: 0.6101837914618298\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.3095451376107483\n",
      "Validation Error: 0.5915776409381389\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.532571029776479\n",
      "Validation Error: 0.6050473770308531\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.41584513873021584\n",
      "Validation Error: 0.5837684399296836\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error: 0.20162587407172136\n",
      "Validation Error: 0.5881889589189081\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 1000, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.46988545890059963\n",
      "Validation Error: 0.5847003633397732\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 1000, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.3362558793070724\n",
      "Validation Error: 0.5794432857811497\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 1000, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.12375197406176025\n",
      "Validation Error: 0.5985382001059424\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 2000, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.39958957988091076\n",
      "Validation Error: 0.5768256291512879\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 2000, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.25034101761429917\n",
      "Validation Error: 0.5806002474770103\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 2000, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.06019494312452797\n",
      "Validation Error: 0.6085860952332176\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6886184967370189\n",
      "Validation Error: 0.709207693413248\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.605120127446229\n",
      "Validation Error: 0.6623091375899822\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4232876498633171\n",
      "Validation Error: 0.6313728919279659\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6265128749324141\n",
      "Validation Error: 0.6659953986186704\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5391947706513114\n",
      "Validation Error: 0.629572021160769\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.3411706318602605\n",
      "Validation Error: 0.6190735936530634\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5685132945124897\n",
      "Validation Error: 0.625856054874359\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4686430066136844\n",
      "Validation Error: 0.6084846310427406\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.25741324962437\n",
      "Validation Error: 0.6254098415238337\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.48800358806423483\n",
      "Validation Error: 0.5947882639535944\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.3621446359275361\n",
      "Validation Error: 0.6009731684638782\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.142511742186234\n",
      "Validation Error: 0.6411311209017816\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 1000, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4196509881598354\n",
      "Validation Error: 0.5857653200183447\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 1000, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.2727586485507299\n",
      "Validation Error: 0.6041429285982238\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 1000, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.07213499372611183\n",
      "Validation Error: 0.6529643132811481\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 2000, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.3405460738755207\n",
      "Validation Error: 0.589919631885785\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 2000, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.18287273684008926\n",
      "Validation Error: 0.6204527249413128\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 2000, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.024671270151059365\n",
      "Validation Error: 0.6671292855882747\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.03, 0.1, 0.3, 0.5]\n",
    "estimators = [50, 100, 200, 500, 1000, 2000]\n",
    "depths = [1, 2, 5]\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for estimator in estimators:\n",
    "        for depth in depths:\n",
    "            print(\"Learning Rate: {0}, # Estimators: {1}, Depth: {2}\".format(learning_rate, estimator, depth))\n",
    "            print(\"--------------------------------------------------\")\n",
    "            xg_reg = XGBRegressor(\n",
    "                learning_rate=learning_rate, max_depth=depth, n_estimators=estimator)\n",
    "            xg_reg.fit(X_train_cv_reg3, y_train3)\n",
    "            print(\"Training Error: {}\".format(calculate_MSE(y_train3, vthreshold_rating(xg_reg.predict(X_train_cv_reg3)))))\n",
    "            print(\"Validation Error: {}\".format(calculate_MSE(y_val3, vthreshold_rating(xg_reg.predict(X_val_cv_reg3)))))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not any better than the pure language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csc2515-env-3.8",
   "language": "python",
   "name": "csc2515-env-3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
