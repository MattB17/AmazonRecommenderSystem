{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Models\n",
    "\n",
    "In this notebook we look at some final candidate models and assess their performance on a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "random.seed(17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "\n",
    "We will subsample 50,000 of the 200,000 records for training by sampling 25% of the data from each of the five music categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = None\n",
    "with open(os.path.join('data', 'train.json'), 'r') as train_file:\n",
    "    data = [json.loads(row) for row in train_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame(data).drop(columns=['image'])\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>category</th>\n",
       "      <th>price</th>\n",
       "      <th>itemID</th>\n",
       "      <th>reviewHash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>08 24, 2010</td>\n",
       "      <td>u04428712</td>\n",
       "      <td>So is Katy Perry's new album \"Teenage Dream\" c...</td>\n",
       "      <td>Amazing that I Actually Bought This...More Ama...</td>\n",
       "      <td>1282608000</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$35.93</td>\n",
       "      <td>p70761125</td>\n",
       "      <td>85559980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.0</td>\n",
       "      <td>12 4, 2013</td>\n",
       "      <td>u77782870</td>\n",
       "      <td>Kelly sounds great on this CD  She is back.  I...</td>\n",
       "      <td>GET IT</td>\n",
       "      <td>1386115200</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$6.98</td>\n",
       "      <td>p11658191</td>\n",
       "      <td>15086745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.0</td>\n",
       "      <td>04 3, 2002</td>\n",
       "      <td>u25030850</td>\n",
       "      <td>Me personally I am not a big fan of Pearl Jam ...</td>\n",
       "      <td>Hmmmmm...........</td>\n",
       "      <td>1017792000</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$6.81</td>\n",
       "      <td>p99659606</td>\n",
       "      <td>41124728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5.0</td>\n",
       "      <td>06 10, 2015</td>\n",
       "      <td>u12668412</td>\n",
       "      <td>very good cd</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1433894400</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$2.43</td>\n",
       "      <td>p76494211</td>\n",
       "      <td>71863260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.0</td>\n",
       "      <td>08 12, 2016</td>\n",
       "      <td>u66989964</td>\n",
       "      <td>She is my fave!</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1470960000</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$11.57</td>\n",
       "      <td>p83852395</td>\n",
       "      <td>05580669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199974</th>\n",
       "      <td>4.0</td>\n",
       "      <td>01 9, 2007</td>\n",
       "      <td>u17199019</td>\n",
       "      <td>Listening to this album, there are some great ...</td>\n",
       "      <td>A different approach from JB for sure!</td>\n",
       "      <td>1168300800</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$15.43</td>\n",
       "      <td>p47061408</td>\n",
       "      <td>34190361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199976</th>\n",
       "      <td>4.0</td>\n",
       "      <td>08 21, 2012</td>\n",
       "      <td>u15729510</td>\n",
       "      <td>His cd has a great sound..there is his talent ...</td>\n",
       "      <td>the great sound of Sanborn</td>\n",
       "      <td>1345507200</td>\n",
       "      <td>Jazz</td>\n",
       "      <td>$22.81</td>\n",
       "      <td>p99893340</td>\n",
       "      <td>66451838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199984</th>\n",
       "      <td>4.0</td>\n",
       "      <td>11 28, 2015</td>\n",
       "      <td>u10400124</td>\n",
       "      <td>Nearly all his music in one place a very good ...</td>\n",
       "      <td>Excellent Deal</td>\n",
       "      <td>1448668800</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$14.23</td>\n",
       "      <td>p82805261</td>\n",
       "      <td>95444453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199987</th>\n",
       "      <td>4.0</td>\n",
       "      <td>03 23, 2014</td>\n",
       "      <td>u32710934</td>\n",
       "      <td>Love priest, halford one of the best live show...</td>\n",
       "      <td>Remaster ?</td>\n",
       "      <td>1395532800</td>\n",
       "      <td>Alternative Rock</td>\n",
       "      <td>$11.89</td>\n",
       "      <td>p75893713</td>\n",
       "      <td>97935046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199994</th>\n",
       "      <td>4.0</td>\n",
       "      <td>09 3, 2014</td>\n",
       "      <td>u97165206</td>\n",
       "      <td>I love this lp this album is really good\\none ...</td>\n",
       "      <td>great song</td>\n",
       "      <td>1409702400</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$49.99</td>\n",
       "      <td>p58216418</td>\n",
       "      <td>07085315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50001 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        overall   reviewTime reviewerID  \\\n",
       "0           4.0  08 24, 2010  u04428712   \n",
       "8           5.0   12 4, 2013  u77782870   \n",
       "10          3.0   04 3, 2002  u25030850   \n",
       "13          5.0  06 10, 2015  u12668412   \n",
       "14          5.0  08 12, 2016  u66989964   \n",
       "...         ...          ...        ...   \n",
       "199974      4.0   01 9, 2007  u17199019   \n",
       "199976      4.0  08 21, 2012  u15729510   \n",
       "199984      4.0  11 28, 2015  u10400124   \n",
       "199987      4.0  03 23, 2014  u32710934   \n",
       "199994      4.0   09 3, 2014  u97165206   \n",
       "\n",
       "                                               reviewText  \\\n",
       "0       So is Katy Perry's new album \"Teenage Dream\" c...   \n",
       "8       Kelly sounds great on this CD  She is back.  I...   \n",
       "10      Me personally I am not a big fan of Pearl Jam ...   \n",
       "13                                           very good cd   \n",
       "14                                        She is my fave!   \n",
       "...                                                   ...   \n",
       "199974  Listening to this album, there are some great ...   \n",
       "199976  His cd has a great sound..there is his talent ...   \n",
       "199984  Nearly all his music in one place a very good ...   \n",
       "199987  Love priest, halford one of the best live show...   \n",
       "199994  I love this lp this album is really good\\none ...   \n",
       "\n",
       "                                                  summary  unixReviewTime  \\\n",
       "0       Amazing that I Actually Bought This...More Ama...      1282608000   \n",
       "8                                                  GET IT      1386115200   \n",
       "10                                      Hmmmmm...........      1017792000   \n",
       "13                                             Five Stars      1433894400   \n",
       "14                                             Five Stars      1470960000   \n",
       "...                                                   ...             ...   \n",
       "199974             A different approach from JB for sure!      1168300800   \n",
       "199976                         the great sound of Sanborn      1345507200   \n",
       "199984                                     Excellent Deal      1448668800   \n",
       "199987                                         Remaster ?      1395532800   \n",
       "199994                                         great song      1409702400   \n",
       "\n",
       "                category   price     itemID reviewHash  \n",
       "0                    Pop  $35.93  p70761125   85559980  \n",
       "8                    Pop   $6.98  p11658191   15086745  \n",
       "10                   Pop   $6.81  p99659606   41124728  \n",
       "13                   Pop   $2.43  p76494211   71863260  \n",
       "14                   Pop  $11.57  p83852395   05580669  \n",
       "...                  ...     ...        ...        ...  \n",
       "199974               Pop  $15.43  p47061408   34190361  \n",
       "199976              Jazz  $22.81  p99893340   66451838  \n",
       "199984               Pop  $14.23  p82805261   95444453  \n",
       "199987  Alternative Rock  $11.89  p75893713   97935046  \n",
       "199994               Pop  $49.99  p58216418   07085315  \n",
       "\n",
       "[50001 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = data_df['category'].unique()\n",
    "dfs = []\n",
    "for category in categories:\n",
    "    dfs.append(data_df[data_df['category'] == category].sample(frac=0.25))\n",
    "data_df = pd.concat(dfs, axis=0)\n",
    "data_df = data_df.sort_index()\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing\n",
    "\n",
    "We apply feature cleaning as prototyped before and then split into a training and validation set, ensuring that the proportion of data points in training vs validation is consistent for each music category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_price(price):\n",
    "    \"\"\"Trims `price` to remove the $ sign.\n",
    "    \n",
    "    If the price variable does not have the format $x.xx\n",
    "    then the empty string is returned.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    price: str\n",
    "        A string representing a price.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A string representing `price` but with the $ sign removed,\n",
    "        or the empty string if `price` does not have the correct\n",
    "        format.\n",
    "    \n",
    "    \"\"\"\n",
    "    if (not pd.isnull(price) and isinstance(price, str) and\n",
    "        len(price) > 0 and price[0] == '$'):\n",
    "        return price[1:]\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-d3ba8e2e1b50>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_df['cleanedPrice'] = data_df['cleanedPrice'].astype('float')\n",
      "<ipython-input-6-d3ba8e2e1b50>:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_df['fixedReviewText'] = np.where(pd.isnull(data_df['reviewText']), \"\", data_df['reviewText'])\n",
      "<ipython-input-6-d3ba8e2e1b50>:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_df['fixedSummary'] = np.where(pd.isnull(data_df['summary']), \"\", data_df['summary'])\n",
      "<ipython-input-6-d3ba8e2e1b50>:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_df['fullReviewText'] = data_df['fixedSummary'] + \" \" + data_df['fixedReviewText']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>category</th>\n",
       "      <th>price</th>\n",
       "      <th>itemID</th>\n",
       "      <th>reviewHash</th>\n",
       "      <th>...</th>\n",
       "      <th>reviewHour</th>\n",
       "      <th>reviewMonthYear</th>\n",
       "      <th>cleanedPrice</th>\n",
       "      <th>fullReviewText</th>\n",
       "      <th>isPop</th>\n",
       "      <th>isDanceElectronic</th>\n",
       "      <th>isJazz</th>\n",
       "      <th>isClassical</th>\n",
       "      <th>isAlternativeRock</th>\n",
       "      <th>reviewWordCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>08 24, 2010</td>\n",
       "      <td>u04428712</td>\n",
       "      <td>So is Katy Perry's new album \"Teenage Dream\" c...</td>\n",
       "      <td>Amazing that I Actually Bought This...More Ama...</td>\n",
       "      <td>1282608000</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$35.93</td>\n",
       "      <td>p70761125</td>\n",
       "      <td>85559980</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>2010-08</td>\n",
       "      <td>35.93</td>\n",
       "      <td>Amazing that I Actually Bought This...More Ama...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.0</td>\n",
       "      <td>12 4, 2013</td>\n",
       "      <td>u77782870</td>\n",
       "      <td>Kelly sounds great on this CD  She is back.  I...</td>\n",
       "      <td>GET IT</td>\n",
       "      <td>1386115200</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$6.98</td>\n",
       "      <td>p11658191</td>\n",
       "      <td>15086745</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>2013-12</td>\n",
       "      <td>6.98</td>\n",
       "      <td>GET IT Kelly sounds great on this CD  She is b...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.0</td>\n",
       "      <td>04 3, 2002</td>\n",
       "      <td>u25030850</td>\n",
       "      <td>Me personally I am not a big fan of Pearl Jam ...</td>\n",
       "      <td>Hmmmmm...........</td>\n",
       "      <td>1017792000</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$6.81</td>\n",
       "      <td>p99659606</td>\n",
       "      <td>41124728</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>2002-04</td>\n",
       "      <td>6.81</td>\n",
       "      <td>Hmmmmm........... Me personally I am not a big...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5.0</td>\n",
       "      <td>06 10, 2015</td>\n",
       "      <td>u12668412</td>\n",
       "      <td>very good cd</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1433894400</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$2.43</td>\n",
       "      <td>p76494211</td>\n",
       "      <td>71863260</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>2015-06</td>\n",
       "      <td>2.43</td>\n",
       "      <td>Five Stars very good cd</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.0</td>\n",
       "      <td>08 12, 2016</td>\n",
       "      <td>u66989964</td>\n",
       "      <td>She is my fave!</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1470960000</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$11.57</td>\n",
       "      <td>p83852395</td>\n",
       "      <td>05580669</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>2016-08</td>\n",
       "      <td>11.57</td>\n",
       "      <td>Five Stars She is my fave!</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199974</th>\n",
       "      <td>4.0</td>\n",
       "      <td>01 9, 2007</td>\n",
       "      <td>u17199019</td>\n",
       "      <td>Listening to this album, there are some great ...</td>\n",
       "      <td>A different approach from JB for sure!</td>\n",
       "      <td>1168300800</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$15.43</td>\n",
       "      <td>p47061408</td>\n",
       "      <td>34190361</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>2007-01</td>\n",
       "      <td>15.43</td>\n",
       "      <td>A different approach from JB for sure! Listeni...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199976</th>\n",
       "      <td>4.0</td>\n",
       "      <td>08 21, 2012</td>\n",
       "      <td>u15729510</td>\n",
       "      <td>His cd has a great sound..there is his talent ...</td>\n",
       "      <td>the great sound of Sanborn</td>\n",
       "      <td>1345507200</td>\n",
       "      <td>Jazz</td>\n",
       "      <td>$22.81</td>\n",
       "      <td>p99893340</td>\n",
       "      <td>66451838</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>2012-08</td>\n",
       "      <td>22.81</td>\n",
       "      <td>the great sound of Sanborn His cd has a great ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199984</th>\n",
       "      <td>4.0</td>\n",
       "      <td>11 28, 2015</td>\n",
       "      <td>u10400124</td>\n",
       "      <td>Nearly all his music in one place a very good ...</td>\n",
       "      <td>Excellent Deal</td>\n",
       "      <td>1448668800</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$14.23</td>\n",
       "      <td>p82805261</td>\n",
       "      <td>95444453</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>2015-11</td>\n",
       "      <td>14.23</td>\n",
       "      <td>Excellent Deal Nearly all his music in one pla...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199987</th>\n",
       "      <td>4.0</td>\n",
       "      <td>03 23, 2014</td>\n",
       "      <td>u32710934</td>\n",
       "      <td>Love priest, halford one of the best live show...</td>\n",
       "      <td>Remaster ?</td>\n",
       "      <td>1395532800</td>\n",
       "      <td>Alternative Rock</td>\n",
       "      <td>$11.89</td>\n",
       "      <td>p75893713</td>\n",
       "      <td>97935046</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>2014-03</td>\n",
       "      <td>11.89</td>\n",
       "      <td>Remaster ? Love priest, halford one of the bes...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199994</th>\n",
       "      <td>4.0</td>\n",
       "      <td>09 3, 2014</td>\n",
       "      <td>u97165206</td>\n",
       "      <td>I love this lp this album is really good\\none ...</td>\n",
       "      <td>great song</td>\n",
       "      <td>1409702400</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$49.99</td>\n",
       "      <td>p58216418</td>\n",
       "      <td>07085315</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>2014-09</td>\n",
       "      <td>49.99</td>\n",
       "      <td>great song I love this lp this album is really...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49309 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        overall   reviewTime reviewerID  \\\n",
       "0           4.0  08 24, 2010  u04428712   \n",
       "8           5.0   12 4, 2013  u77782870   \n",
       "10          3.0   04 3, 2002  u25030850   \n",
       "13          5.0  06 10, 2015  u12668412   \n",
       "14          5.0  08 12, 2016  u66989964   \n",
       "...         ...          ...        ...   \n",
       "199974      4.0   01 9, 2007  u17199019   \n",
       "199976      4.0  08 21, 2012  u15729510   \n",
       "199984      4.0  11 28, 2015  u10400124   \n",
       "199987      4.0  03 23, 2014  u32710934   \n",
       "199994      4.0   09 3, 2014  u97165206   \n",
       "\n",
       "                                               reviewText  \\\n",
       "0       So is Katy Perry's new album \"Teenage Dream\" c...   \n",
       "8       Kelly sounds great on this CD  She is back.  I...   \n",
       "10      Me personally I am not a big fan of Pearl Jam ...   \n",
       "13                                           very good cd   \n",
       "14                                        She is my fave!   \n",
       "...                                                   ...   \n",
       "199974  Listening to this album, there are some great ...   \n",
       "199976  His cd has a great sound..there is his talent ...   \n",
       "199984  Nearly all his music in one place a very good ...   \n",
       "199987  Love priest, halford one of the best live show...   \n",
       "199994  I love this lp this album is really good\\none ...   \n",
       "\n",
       "                                                  summary  unixReviewTime  \\\n",
       "0       Amazing that I Actually Bought This...More Ama...      1282608000   \n",
       "8                                                  GET IT      1386115200   \n",
       "10                                      Hmmmmm...........      1017792000   \n",
       "13                                             Five Stars      1433894400   \n",
       "14                                             Five Stars      1470960000   \n",
       "...                                                   ...             ...   \n",
       "199974             A different approach from JB for sure!      1168300800   \n",
       "199976                         the great sound of Sanborn      1345507200   \n",
       "199984                                     Excellent Deal      1448668800   \n",
       "199987                                         Remaster ?      1395532800   \n",
       "199994                                         great song      1409702400   \n",
       "\n",
       "                category   price     itemID reviewHash  ... reviewHour  \\\n",
       "0                    Pop  $35.93  p70761125   85559980  ...         20   \n",
       "8                    Pop   $6.98  p11658191   15086745  ...         19   \n",
       "10                   Pop   $6.81  p99659606   41124728  ...         19   \n",
       "13                   Pop   $2.43  p76494211   71863260  ...         20   \n",
       "14                   Pop  $11.57  p83852395   05580669  ...         20   \n",
       "...                  ...     ...        ...        ...  ...        ...   \n",
       "199974               Pop  $15.43  p47061408   34190361  ...         19   \n",
       "199976              Jazz  $22.81  p99893340   66451838  ...         20   \n",
       "199984               Pop  $14.23  p82805261   95444453  ...         19   \n",
       "199987  Alternative Rock  $11.89  p75893713   97935046  ...         20   \n",
       "199994               Pop  $49.99  p58216418   07085315  ...         20   \n",
       "\n",
       "       reviewMonthYear  cleanedPrice  \\\n",
       "0              2010-08         35.93   \n",
       "8              2013-12          6.98   \n",
       "10             2002-04          6.81   \n",
       "13             2015-06          2.43   \n",
       "14             2016-08         11.57   \n",
       "...                ...           ...   \n",
       "199974         2007-01         15.43   \n",
       "199976         2012-08         22.81   \n",
       "199984         2015-11         14.23   \n",
       "199987         2014-03         11.89   \n",
       "199994         2014-09         49.99   \n",
       "\n",
       "                                           fullReviewText  isPop  \\\n",
       "0       Amazing that I Actually Bought This...More Ama...      1   \n",
       "8       GET IT Kelly sounds great on this CD  She is b...      1   \n",
       "10      Hmmmmm........... Me personally I am not a big...      1   \n",
       "13                                Five Stars very good cd      1   \n",
       "14                             Five Stars She is my fave!      1   \n",
       "...                                                   ...    ...   \n",
       "199974  A different approach from JB for sure! Listeni...      1   \n",
       "199976  the great sound of Sanborn His cd has a great ...      0   \n",
       "199984  Excellent Deal Nearly all his music in one pla...      1   \n",
       "199987  Remaster ? Love priest, halford one of the bes...      0   \n",
       "199994  great song I love this lp this album is really...      1   \n",
       "\n",
       "       isDanceElectronic  isJazz  isClassical  isAlternativeRock  \\\n",
       "0                      0       0            0                  0   \n",
       "8                      0       0            0                  0   \n",
       "10                     0       0            0                  0   \n",
       "13                     0       0            0                  0   \n",
       "14                     0       0            0                  0   \n",
       "...                  ...     ...          ...                ...   \n",
       "199974                 0       0            0                  0   \n",
       "199976                 0       1            0                  0   \n",
       "199984                 0       0            0                  0   \n",
       "199987                 0       0            0                  1   \n",
       "199994                 0       0            0                  0   \n",
       "\n",
       "        reviewWordCount  \n",
       "0                   277  \n",
       "8                    34  \n",
       "10                   77  \n",
       "13                    5  \n",
       "14                    6  \n",
       "...                 ...  \n",
       "199974              108  \n",
       "199976               61  \n",
       "199984               31  \n",
       "199987               22  \n",
       "199994               16  \n",
       "\n",
       "[49309 rows x 22 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "data_df['reviewMonth'] = data_df['reviewTime'].apply(lambda x: x.split(' ')[0])\n",
    "data_df['reviewYear'] = data_df['reviewTime'].apply(lambda x: x.split(' ')[2])\n",
    "data_df['reviewHour'] = data_df['unixReviewTime'].apply(lambda x: datetime.fromtimestamp(x).hour)\n",
    "data_df['reviewMonthYear'] = data_df['reviewYear'] + '-' + data_df['reviewMonth']\n",
    "\n",
    "data_df['cleanedPrice'] = data_df['price'].apply(lambda x: trim_price(x))\n",
    "data_df = data_df[data_df['cleanedPrice'] != \"\"]\n",
    "data_df['cleanedPrice'] = data_df['cleanedPrice'].astype('float')\n",
    "\n",
    "data_df['fixedReviewText'] = np.where(pd.isnull(data_df['reviewText']), \"\", data_df['reviewText'])\n",
    "data_df['fixedSummary'] = np.where(pd.isnull(data_df['summary']), \"\", data_df['summary'])\n",
    "data_df['fullReviewText'] = data_df['fixedSummary'] + \" \" + data_df['fixedReviewText']\n",
    "\n",
    "data_df = data_df.drop(columns=['fixedReviewText', 'fixedSummary'])\n",
    "\n",
    "genres = data_df['category'].unique()\n",
    "\n",
    "for genre in genres:\n",
    "    genre_col = \"is\" + genre.replace(\" \", \"\").replace(\"&\", \"\")\n",
    "    data_df[genre_col] = data_df['category'].apply(lambda x: 1 if x == genre else 0)\n",
    "\n",
    "data_df['reviewWordCount'] = data_df['fullReviewText'].apply(lambda x: len(x.split()))\n",
    "\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_MSE(actuals, predicteds):\n",
    "    \"\"\"Calculates the Mean Squared Error between `actuals` and `predicteds`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    actuals: np.array\n",
    "        A numpy array of the actual values.\n",
    "    predicteds: np.array\n",
    "        A numpy array of the predicted values.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        A float representing the Mean Squared Error between `actuals` and\n",
    "        `predicteds`.\n",
    "    \n",
    "    \"\"\"\n",
    "    return (((actuals - predicteds)**2).sum()) / (len(actuals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "genres = data_df['category'].unique()\n",
    "X_train_set = []\n",
    "X_val_set = []\n",
    "y_train_set = []\n",
    "y_val_set = []\n",
    "\n",
    "for genre in genres:\n",
    "    genre_df = data_df[data_df['category'] == genre]\n",
    "    targets = genre_df['overall']\n",
    "    feature_data = genre_df.drop(columns=['overall'])\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        feature_data, targets, shuffle=True, test_size=0.2, random_state=17)\n",
    "    X_train_set.append(X_train)\n",
    "    X_val_set.append(X_val)\n",
    "    y_train_set.append(y_train)\n",
    "    y_val_set.append(y_val)\n",
    "\n",
    "X_train = pd.concat(X_train_set)\n",
    "X_val = pd.concat(X_val_set)\n",
    "y_train = pd.concat(y_train_set)\n",
    "y_val = pd.concat(y_val_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative Filtering\n",
    "\n",
    "In this model we only need a users ID, the items ID, and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_item_matrix(df, rating_col, user_col, item_col):\n",
    "    return sp.csr_matrix(df[rating_col], (df[user_col], df[item_col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.concat([X_train, pd.DataFrame(y_train, columns=['overall'])], axis=1)\n",
    "val_data = pd.concat([X_val, pd.DataFrame(y_val, columns=['overall'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "item_matrix = train_data.pivot(index='itemID', columns='reviewerID', values='overall')\n",
    "item_matrix = item_matrix.fillna(0)\n",
    "user_item_train_matrix = sp.csr_matrix(item_matrix.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_average = train_data['overall'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "model_knn = NearestNeighbors(metric='cosine', algorithm='brute', n_neighbors=5)\n",
    "model_knn.fit(user_item_train_matrix)\n",
    "item_neighbors = np.asarray(model_knn.kneighbors(user_item_train_matrix, return_distance=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_matrix = train_data.pivot(index='reviewerID', columns='itemID', values='overall')\n",
    "user_matrix = user_matrix.fillna(0)\n",
    "user_item_train_matrix = sp.csr_matrix(user_matrix.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_knn = NearestNeighbors(metric='cosine', algorithm='brute', n_neighbors=5)\n",
    "model_knn.fit(user_item_train_matrix)\n",
    "user_neighbors = np.asarray(model_knn.kneighbors(user_item_train_matrix, return_distance=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user_avg = train_data.groupby(train_data['reviewerID'], as_index=False)['overall'].mean()\n",
    "train_item_avg = train_data.groupby(train_data['itemID'], as_index=False)['overall'].mean()\n",
    "train_user_avg.columns = ['reviewerID', 'userAverage']\n",
    "train_item_avg.columns = ['itemID', 'itemAverage']\n",
    "train_user_avg = train_user_avg.set_index('reviewerID')\n",
    "train_item_avg = train_item_avg.set_index('itemID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_avgs = []\n",
    "for i in range(len(item_neighbors)):\n",
    "    item_avgs.append(train_item_avg['itemAverage'][item_matrix.index[item_neighbors[i]]].mean())\n",
    "\n",
    "item_avgs = pd.concat([pd.DataFrame(item_matrix.index, columns=['itemID']), pd.DataFrame(item_avgs, columns=['itemRating'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_avgs = []\n",
    "for i in range(len(user_neighbors)):\n",
    "    user_avgs.append(train_user_avg['userAverage'][user_matrix.index[user_neighbors[i]]].mean())\n",
    "\n",
    "user_avgs = pd.concat([pd.DataFrame(user_matrix.index, columns=['reviewerID']), pd.DataFrame(user_avgs, columns=['userRating'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average_data(X, total_avg, user_avgs, item_avgs):\n",
    "    \"\"\"Calculates the error based on the weighted average prediction.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: pd.DataFrame\n",
    "        The DataFrame of features.\n",
    "    y: np.array\n",
    "        A numpy array containing the targets\n",
    "    total_avg: float\n",
    "        The average across all users/items.\n",
    "    user_avgs: pd.DataFrame\n",
    "        A DataFrame containing the average rating for each user.\n",
    "    item_avgs: pd.DataFrame\n",
    "        A DataFrame containing the average rating for each item.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        A float representing the mean squared error of the predictions.\n",
    "    \n",
    "    \"\"\"\n",
    "    df_user = pd.merge(X, user_avgs, how='left', on=['reviewerID'])\n",
    "    df_final = pd.merge(df_user, item_avgs, how='left', on=['itemID'])\n",
    "    df_final = df_final[['userRating', 'itemRating']]\n",
    "    df_final = df_final.fillna(total_avg)\n",
    "    df_final.index = X.index\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_aug = weighted_average_data(X_train, global_average, user_avgs, item_avgs)\n",
    "X_val_aug = weighted_average_data(X_val, global_average, user_avgs, item_avgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mod = pd.concat([X_train, X_train_aug], axis=1)\n",
    "X_val_mod = pd.concat([X_val, X_val_aug], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_rating(rating):\n",
    "    \"\"\"Thresholds `rating` to lie in the range [1, 5].\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rating: float\n",
    "        The rating to be thresholded.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        A float representing the thresholded rating.\n",
    "    \n",
    "    \"\"\"\n",
    "    if rating < 1:\n",
    "        return 1\n",
    "    if rating > 5:\n",
    "        return 5\n",
    "    return rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight: 0.00\n",
      "-------------\n",
      "Training MSE: 0.9063375958329644\n",
      "Validation MSE: 1.0265327709287875\n",
      "\n",
      "Weight: 0.10\n",
      "-------------\n",
      "Training MSE: 0.8811929772416747\n",
      "Validation MSE: 1.0043193601179092\n",
      "\n",
      "Weight: 0.30\n",
      "-------------\n",
      "Training MSE: 0.8433587912087248\n",
      "Validation MSE: 0.9725975039754755\n",
      "\n",
      "Weight: 0.50\n",
      "-------------\n",
      "Training MSE: 0.8221313400419478\n",
      "Validation MSE: 0.9578156018054741\n",
      "\n",
      "Weight: 0.70\n",
      "-------------\n",
      "Training MSE: 0.8175106237413438\n",
      "Validation MSE: 0.9599736536079039\n",
      "\n",
      "Weight: 0.90\n",
      "-------------\n",
      "Training MSE: 0.8294966423069127\n",
      "Validation MSE: 0.9790716593827655\n",
      "\n",
      "Weight: 1.00\n",
      "-------------\n",
      "Training MSE: 0.841717177164512\n",
      "Validation MSE: 0.9949731450098583\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weights = [0.0, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0]\n",
    "\n",
    "vthreshold = np.vectorize(threshold_rating)\n",
    "\n",
    "for weight in weights:\n",
    "    print(\"Weight: %.2f\" % weight)\n",
    "    print(\"-------------\")\n",
    "    train_pred = vthreshold((weight * X_train_mod['userRating']) + ((1.0 - weight) * X_train_mod['itemRating']))\n",
    "    val_pred = vthreshold((weight * X_val_mod['userRating']) + ((1.0 - weight) * X_val_mod['itemRating']))\n",
    "    print(\"Training MSE: {}\".format(calculate_MSE(y_train, train_pred)))\n",
    "    print(\"Validation MSE: {}\".format(calculate_MSE(y_val, val_pred)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Models\n",
    "\n",
    "The first languge model is built with TfidfVectorizer (Term Frequency - Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['cleanedPrice', 'isPop', 'isAlternativeRock', 'isJazz', 'isClassical', 'isDanceElectronic', 'reviewWordCount']\n",
    "X_train_reg1 = X_train[columns_to_keep]\n",
    "X_val_reg1 = X_val[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-25-2b5d7d7878ed>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train_reg1['reviewWordCount'] = X_train_reg1['reviewWordCount'].apply(lambda x: np.log(x))\n",
      "<ipython-input-25-2b5d7d7878ed>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val_reg1['reviewWordCount'] = X_val_reg1['reviewWordCount'].apply(lambda x: np.log(x))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_train_reg1['reviewWordCount'] = X_train_reg1['reviewWordCount'].apply(lambda x: np.log(x))\n",
    "X_val_reg1['reviewWordCount'] = X_val_reg1['reviewWordCount'].apply(lambda x: np.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(df):\n",
    "    df.dropna(inplace=True)\n",
    "    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "    return df[indices_to_keep].astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-550e8da2fe0c>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.dropna(inplace=True)\n",
      "<ipython-input-26-550e8da2fe0c>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.dropna(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "X_train_reg1 = clean_dataset(X_train_reg1)\n",
    "y_train1 = y_train[y_train.index.isin(X_train_reg1.index)]\n",
    "X_train1 = X_train[X_train.index.isin(X_train_reg1.index)]\n",
    "\n",
    "X_val_reg1 = clean_dataset(X_val_reg1)\n",
    "y_val1 = y_val[y_val.index.isin(X_val_reg1.index)]\n",
    "X_val1 = X_val[X_val.index.isin(X_val_reg1.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mod = X_train_mod[X_train_mod.index.isin(X_train_reg1.index)]\n",
    "X_val_mod = X_val_mod[X_val_mod.index.isin(X_val_reg1.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reg1 = min_max_scaler.fit_transform(X_train_reg1)\n",
    "X_val_reg1 = min_max_scaler.transform(X_val_reg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha = 0.0\n",
      "------------\n",
      "Training Error: 0.9588535445992827\n",
      "Validation Error: 0.9262869599431929\n",
      "\n",
      "Alpha = 0.01\n",
      "------------\n",
      "Training Error: 0.9588428957078677\n",
      "Validation Error: 0.9262350113636552\n",
      "\n",
      "Alpha = 0.03\n",
      "------------\n",
      "Training Error: 0.9588433477027621\n",
      "Validation Error: 0.9262349757449848\n",
      "\n",
      "Alpha = 0.1\n",
      "------------\n",
      "Training Error: 0.9588449304068484\n",
      "Validation Error: 0.926234860637202\n",
      "\n",
      "Alpha = 0.3\n",
      "------------\n",
      "Training Error: 0.9588494564503761\n",
      "Validation Error: 0.9262346102237373\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "vthreshold_rating = np.vectorize(threshold_rating)\n",
    "\n",
    "alphas = [0.0, 0.01, 0.03, 0.1, 0.3]\n",
    "for alpha in alphas:\n",
    "    print(\"Alpha = {}\".format(alpha))\n",
    "    print(\"------------\")\n",
    "    reg_model = Ridge(alpha=alpha)\n",
    "    reg_model.fit(X_train_reg1, y_train1)\n",
    "    print(\"Training Error: {}\".format(calculate_MSE(y_train1, vthreshold_rating(reg_model.predict(X_train_reg1)))))\n",
    "    print(\"Validation Error: {}\".format(calculate_MSE(y_val1, vthreshold_rating(reg_model.predict(X_val_reg1)))))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Matthew/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def process_review_text(review_text, exclude_text, ps):\n",
    "    \"\"\"Pre-processes the text given by `review_text`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    review_text: str\n",
    "        The review text to be processed.\n",
    "    exclude_text: collection\n",
    "        A collection of words to be excluded.\n",
    "    ps: PorterStemmer\n",
    "        The PorterStemmer used to perform word stemming.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A string representing the processed version of `review_text`.\n",
    "    \n",
    "    \"\"\"\n",
    "    review = re.sub('[^a-zA-Z0-9]', ' ', review_text).lower().split()\n",
    "    review = [ps.stem(word) for word in review if not word in exclude_text]\n",
    "    return ' '.join(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-30-0d22f415ca83>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train1['processedReview'] = X_train1['fullReviewText'].apply(lambda x: process_review_text(x, exclude_english, ps))\n"
     ]
    }
   ],
   "source": [
    "exclude_english = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "X_train1['processedReview'] = X_train1['fullReviewText'].apply(lambda x: process_review_text(x, exclude_english, ps))\n",
    "X_val1['processedReview'] = X_val1['fullReviewText'].apply(lambda x: process_review_text(x, exclude_english, ps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TFIDF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cv = TfidfVectorizer()\n",
    "X_train_cv1 = cv.fit_transform(X_train1['processedReview'])\n",
    "X_val_cv1 = cv.transform(X_val1['processedReview'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "X_train_reg1_sp = sp.csr_matrix(X_train_reg1)\n",
    "X_train_cv_reg1 = sp.hstack((X_train_cv1, X_train_reg1_sp), format='csr')\n",
    "\n",
    "X_val_reg1_sp = sp.csr_matrix(X_val_reg1)\n",
    "X_val_cv_reg1 = sp.hstack((X_val_cv1, X_val_reg1_sp), format='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.03, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 1.657059470470493\n",
      "Validation Error: 1.643835435274604\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 1.6152387609816117\n",
      "Validation Error: 1.6027478863791698\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 1.5064334150057164\n",
      "Validation Error: 1.5156320233080705\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.9278305199606406\n",
      "Validation Error: 0.9140717888445251\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8659974631467421\n",
      "Validation Error: 0.8564736279922335\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7210557554945155\n",
      "Validation Error: 0.7517563198863484\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8420321019890331\n",
      "Validation Error: 0.8303079627493093\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.760438332990258\n",
      "Validation Error: 0.7566828311093866\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5896194009217547\n",
      "Validation Error: 0.6535086962140353\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7555609022602061\n",
      "Validation Error: 0.7485147430606864\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6590850071979402\n",
      "Validation Error: 0.6711793088009371\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.47360315538919273\n",
      "Validation Error: 0.5932841336237142\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 1000, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6855883782073438\n",
      "Validation Error: 0.6865773121105989\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 1000, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5876296474006382\n",
      "Validation Error: 0.6209610572979316\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 1000, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.39732599783579103\n",
      "Validation Error: 0.5663672308565999\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 2000, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6212677003234078\n",
      "Validation Error: 0.635851126577105\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 2000, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5205553944189065\n",
      "Validation Error: 0.5847359615797614\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 2000, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.32560582973057645\n",
      "Validation Error: 0.5534224735912971\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8545968150570561\n",
      "Validation Error: 0.8414255326803074\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7794450778503823\n",
      "Validation Error: 0.7738756431891397\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6129962177190347\n",
      "Validation Error: 0.6694610966797805\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7925282038733316\n",
      "Validation Error: 0.7834807180729493\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7024723613600568\n",
      "Validation Error: 0.70557621080342\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5223297054547508\n",
      "Validation Error: 0.6196355936520616\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7228788488981269\n",
      "Validation Error: 0.7194598043082383\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.627291375936086\n",
      "Validation Error: 0.6484349222902557\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.43942147911424845\n",
      "Validation Error: 0.5836214563767533\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6345008650217865\n",
      "Validation Error: 0.6464427410424495\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5356364937959868\n",
      "Validation Error: 0.5919058072540884\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.34107565980631027\n",
      "Validation Error: 0.5583308097709344\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 1000, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5751382642336652\n",
      "Validation Error: 0.6042656362545261\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 1000, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4674547805926879\n",
      "Validation Error: 0.5678206985403677\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 1000, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.26389885352292236\n",
      "Validation Error: 0.5533574690248638\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 2000, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.516572726861557\n",
      "Validation Error: 0.579436937688131\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 2000, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.396455433751136\n",
      "Validation Error: 0.555741968542181\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 2000, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.1830210928647965\n",
      "Validation Error: 0.5552062594121979\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.744051463481692\n",
      "Validation Error: 0.73679533762195\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6520878991813419\n",
      "Validation Error: 0.6677982783905524\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.47698980767163796\n",
      "Validation Error: 0.6027373381199861\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6745373545809814\n",
      "Validation Error: 0.6797227794827705\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.581382142653119\n",
      "Validation Error: 0.6194256185196815\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.3978382518415956\n",
      "Validation Error: 0.5827049991384502\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6122876045792917\n",
      "Validation Error: 0.631317910464996\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5156446019001003\n",
      "Validation Error: 0.588368027020797\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.321234731383588\n",
      "Validation Error: 0.574044496597672\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5360901839445277\n",
      "Validation Error: 0.5872589998388212\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4239971630356931\n",
      "Validation Error: 0.5674633156890021\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.2097980605071751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: 0.5755454593030278\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 1000, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.47510323441389246\n",
      "Validation Error: 0.5700952445865527\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 1000, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.34608801412853196\n",
      "Validation Error: 0.5653763316636292\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 1000, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.12860696671980432\n",
      "Validation Error: 0.5820806629028914\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 2000, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4057182092140655\n",
      "Validation Error: 0.5615677713800927\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 2000, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.2572209693941599\n",
      "Validation Error: 0.5717024770539526\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 2000, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.06158134148522578\n",
      "Validation Error: 0.5941727681381203\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6884696559911426\n",
      "Validation Error: 0.6901496898328511\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6040972142598152\n",
      "Validation Error: 0.640601724855694\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4281311531232244\n",
      "Validation Error: 0.6062766114411388\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6258774251878654\n",
      "Validation Error: 0.6456346335434061\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5391333421637524\n",
      "Validation Error: 0.601871342035567\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.3498182913642454\n",
      "Validation Error: 0.5995151465293299\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5699934202836568\n",
      "Validation Error: 0.608549780432093\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.47151235059922875\n",
      "Validation Error: 0.5851280574783898\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.2666964580782263\n",
      "Validation Error: 0.6020804870484332\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4921170253057247\n",
      "Validation Error: 0.5832150541871\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.3699860060813426\n",
      "Validation Error: 0.5763399485382776\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.1531064452882579\n",
      "Validation Error: 0.6127551826258106\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 1000, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.42479532043043283\n",
      "Validation Error: 0.5756453073438189\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 1000, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.28104789880097103\n",
      "Validation Error: 0.5815911921924128\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 1000, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.0765476377520462\n",
      "Validation Error: 0.631059336390693\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 2000, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.34663535703830856\n",
      "Validation Error: 0.5779904777414281\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 2000, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.18620836936464788\n",
      "Validation Error: 0.5937201839185794\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 2000, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.026037115073510343\n",
      "Validation Error: 0.6410182223229509\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "learning_rates = [0.03, 0.1, 0.3, 0.5]\n",
    "estimators = [50, 100, 200, 500, 1000, 2000]\n",
    "depths = [1, 2, 5]\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for estimator in estimators:\n",
    "        for depth in depths:\n",
    "            print(\"Learning Rate: {0}, # Estimators: {1}, Depth: {2}\".format(learning_rate, estimator, depth))\n",
    "            print(\"--------------------------------------------------\")\n",
    "            xg_reg = XGBRegressor(\n",
    "                learning_rate=learning_rate, max_depth=depth, n_estimators=estimator)\n",
    "            xg_reg.fit(X_train_cv_reg1, y_train1)\n",
    "            print(\"Training Error: {}\".format(calculate_MSE(y_train1, vthreshold_rating(xg_reg.predict(X_train_cv_reg1)))))\n",
    "            print(\"Validation Error: {}\".format(calculate_MSE(y_val1, vthreshold_rating(xg_reg.predict(X_val_cv_reg1)))))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that `learning_rate=0.1`, `n_estimators=2000`, and `max_depth=1` provides a very good model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error based on XGBoost CountVectorizer prediction: 0.517\n",
      "Validation error based on XGBoost CountVectorizer prediction: 0.579\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "xg_reg = XGBRegressor(learning_rate=0.1, n_estimators=2000, max_depth=1)\n",
    "xg_reg.fit(X_train_cv_reg1, y_train1)\n",
    "\n",
    "train_MSE = calculate_MSE(y_train1, vthreshold_rating(xg_reg.predict(X_train_cv_reg1)))\n",
    "val_MSE = calculate_MSE(y_val1, vthreshold_rating(xg_reg.predict(X_val_cv_reg1)))\n",
    "\n",
    "print(\"Training error based on XGBoost CountVectorizer prediction: %.3f\" % train_MSE)\n",
    "print(\"Validation error based on XGBoost CountVectorizer prediction: %.3f\" % val_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2v = Word2Vec(list(X_train1['processedReview']), size=350, window=10, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_review_vector(review_text):\n",
    "    \"\"\"Creates a vector for the review given by `review_text`.\n",
    "    \n",
    "    The word vectors for each word in the review are averaged\n",
    "    to build a vector for the review.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    review_text: str\n",
    "        The review for which the vector is generated.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    vector\n",
    "        A vector for the review.\n",
    "    \n",
    "    \"\"\"\n",
    "    review = [word for word in review_text if word in w2v.wv.vocab]\n",
    "    if len(review) > 0:\n",
    "        return np.mean(w2v[review], axis=0)\n",
    "    return np.zeros(350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-82-07f028c5b26d>:20: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  return np.mean(w2v[review], axis=0)\n",
      "<ipython-input-83-ba0c28ab83e8>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train1['review_vector'] = X_train1['processedReview'].apply(lambda x: create_review_vector(x))\n"
     ]
    }
   ],
   "source": [
    "X_train1['review_vector'] = X_train1['processedReview'].apply(lambda x: create_review_vector(x))\n",
    "X_val1['review_vector'] = X_val1['processedReview'].apply(lambda x: create_review_vector(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_review_vec_df(review_vecs, indices):\n",
    "    \"\"\"Creates a dataframe from `review_vecs`.\n",
    "    \n",
    "    Each numpy array in review_vecs is converted to a \n",
    "    column in the resulting dataframe.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    review_vecs: list\n",
    "        A list of numpy arrays where each array corresponds\n",
    "        to the review vector for a review.\n",
    "    indicies: np.array\n",
    "        A numpy array of indices for the DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The DataFrame obtained from converting `review_vecs`\n",
    "        to a dataframe.\n",
    "    \n",
    "    \"\"\"\n",
    "    review_vec_df = pd.DataFrame(np.vstack(review_vecs))\n",
    "    review_vec_df.columns = [\"word\" + str(col) for col in review_vec_df.columns]\n",
    "    review_vec_df.index = indices\n",
    "    return review_vec_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_wv = create_review_vec_df(X_train1['review_vector'], X_train1.index)\n",
    "X_val_wv = create_review_vec_df(X_val1['review_vector'], X_val1.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reg1_df = pd.DataFrame(np.vstack(X_train_reg1))\n",
    "X_train_reg1_df.index = X_train1.index\n",
    "\n",
    "X_val_reg1_df = pd.DataFrame(np.vstack(X_val_reg1))\n",
    "X_val_reg1_df.index = X_val1.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_wv_reg = pd.concat([X_train_wv, X_train_reg1_df], axis=1)\n",
    "X_val_wv_reg = pd.concat([X_val_wv, X_val_reg1_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha = 0.0\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=6.37186e-18): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error: 0.8940535781682711\n",
      "Validation Error: 0.9018064217232329\n",
      "\n",
      "Alpha = 0.01\n",
      "------------\n",
      "Training Error: 0.9079188841981554\n",
      "Validation Error: 0.9021185371572493\n",
      "\n",
      "Alpha = 0.03\n",
      "------------\n",
      "Training Error: 0.907959198977622\n",
      "Validation Error: 0.9021166181450053\n",
      "\n",
      "Alpha = 0.1\n",
      "------------\n",
      "Training Error: 0.9080974389160185\n",
      "Validation Error: 0.9021763444454539\n",
      "\n",
      "Alpha = 0.3\n",
      "------------\n",
      "Training Error: 0.9083702951020889\n",
      "Validation Error: 0.9023891975104213\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "alphas = [0.0, 0.01, 0.03, 0.1, 0.3]\n",
    "for alpha in alphas:\n",
    "    print(\"Alpha = {}\".format(alpha))\n",
    "    print(\"------------\")\n",
    "    reg_model = Ridge(alpha=alpha)\n",
    "    reg_model.fit(X_train_wv_reg, y_train1)\n",
    "    print(\"Training Error: {}\".format(calculate_MSE(y_train1, vthreshold_rating(reg_model.predict(X_train_wv_reg)))))\n",
    "    print(\"Validation Error: {}\".format(calculate_MSE(y_val1, vthreshold_rating(reg_model.predict(X_val_wv_reg)))))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.03, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 1.6765857758092944\n",
      "Validation Error: 1.6647381372164318\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 1.6580919193530188\n",
      "Validation Error: 1.647719877424904\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 1.608621216796498\n",
      "Validation Error: 1.6276771719437315\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.9686630666825585\n",
      "Validation Error: 0.96021394548407\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.9469037643105891\n",
      "Validation Error: 0.9420444818347719\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8693182573628832\n",
      "Validation Error: 0.9210646118398489\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.9210158070137735\n",
      "Validation Error: 0.9145612959951956\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8950339252158956\n",
      "Validation Error: 0.8963131488685573\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7806337257735777\n",
      "Validation Error: 0.8801334926646515\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.9047880967011823\n",
      "Validation Error: 0.9018306637183156\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8717430765777114\n",
      "Validation Error: 0.8874012535422684\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6753387441185651\n",
      "Validation Error: 0.8795959004553644\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 1000, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8932517468796682\n",
      "Validation Error: 0.8957477878079494\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 1000, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8498530658144292\n",
      "Validation Error: 0.8835547148844185\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 1000, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5401318383248765\n",
      "Validation Error: 0.884903781153897\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 2000, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8816510027360334\n",
      "Validation Error: 0.8912973513891556\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 2000, Depth: 2\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-e677b0c5c51e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m             xg_reg = XGBRegressor(\n\u001b[1;32m     11\u001b[0m                 learning_rate=learning_rate, max_depth=depth, n_estimators=estimator)\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mxg_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_wv_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Error: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalculate_MSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvthreshold_rating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxg_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_wv_reg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation Error: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalculate_MSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvthreshold_rating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxg_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_wv_reg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    540\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'eval_metric'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         self._Booster = train(params, train_dmatrix,\n\u001b[0m\u001b[1;32m    543\u001b[0m                               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_num_boosting_rounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m     return _train_internal(params, dtrain,\n\u001b[0m\u001b[1;32m    209\u001b[0m                            \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_boost_round\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0m\u001b[1;32m   1160\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m                                                     dtrain.handle))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rates = [0.03, 0.1, 0.3, 0.5]\n",
    "estimators = [50, 100, 200, 500, 1000, 2000]\n",
    "depths = [1, 2, 5]\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for estimator in estimators:\n",
    "        for depth in depths:\n",
    "            print(\"Learning Rate: {0}, # Estimators: {1}, Depth: {2}\".format(learning_rate, estimator, depth))\n",
    "            print(\"--------------------------------------------------\")\n",
    "            xg_reg = XGBRegressor(\n",
    "                learning_rate=learning_rate, max_depth=depth, n_estimators=estimator)\n",
    "            xg_reg.fit(X_train_wv_reg, y_train1)\n",
    "            print(\"Training Error: {}\".format(calculate_MSE(y_train1, vthreshold_rating(xg_reg.predict(X_train_wv_reg)))))\n",
    "            print(\"Validation Error: {}\".format(calculate_MSE(y_val1, vthreshold_rating(xg_reg.predict(X_val_wv_reg)))))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta Models\n",
    "\n",
    "In this section we look at models that combine both collaborative filtering and language models.\n",
    "\n",
    "We start by using the predictions of the collaborative filtering as features in our language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['cleanedPrice', 'isPop', 'isAlternativeRock', 'isJazz', 'isClassical', 'isDanceElectronic', 'reviewWordCount', 'userRating', 'itemRating']\n",
    "X_train_reg2 = X_train_mod[columns_to_keep]\n",
    "X_val_reg2 = X_val_mod[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-105-d8a296a7aa94>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train_reg2['reviewWordCount'] = X_train_reg2['reviewWordCount'].apply(lambda x: np.log(x))\n",
      "<ipython-input-105-d8a296a7aa94>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val_reg2['reviewWordCount'] = X_val_reg2['reviewWordCount'].apply(lambda x: np.log(x))\n"
     ]
    }
   ],
   "source": [
    "min_max_scaler = MinMaxScaler()\n",
    "X_train_reg2['reviewWordCount'] = X_train_reg2['reviewWordCount'].apply(lambda x: np.log(x))\n",
    "X_val_reg2['reviewWordCount'] = X_val_reg2['reviewWordCount'].apply(lambda x: np.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-100-550e8da2fe0c>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.dropna(inplace=True)\n",
      "<ipython-input-100-550e8da2fe0c>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.dropna(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "X_train_reg2 = clean_dataset(X_train_reg2)\n",
    "y_train2 = y_train[y_train.index.isin(X_train_reg2.index)]\n",
    "X_train2 = X_train[X_train.index.isin(X_train_reg2.index)]\n",
    "\n",
    "X_val_reg2 = clean_dataset(X_val_reg2)\n",
    "y_val2 = y_val[y_val.index.isin(X_val_reg2.index)]\n",
    "X_val2 = X_val[X_val.index.isin(X_val_reg2.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reg2 = min_max_scaler.fit_transform(X_train_reg2)\n",
    "X_val_reg2 = min_max_scaler.transform(X_val_reg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha = 0.0\n",
      "------------\n",
      "Training Error: 0.7891828923294948\n",
      "Validation Error: 0.9486008427597297\n",
      "\n",
      "Alpha = 0.01\n",
      "------------\n",
      "Training Error: 0.7891677848703272\n",
      "Validation Error: 0.9484601751838659\n",
      "\n",
      "Alpha = 0.03\n",
      "------------\n",
      "Training Error: 0.7891678465440518\n",
      "Validation Error: 0.9484577298907713\n",
      "\n",
      "Alpha = 0.1\n",
      "------------\n",
      "Training Error: 0.7891680642355218\n",
      "Validation Error: 0.948449171698209\n",
      "\n",
      "Alpha = 0.3\n",
      "------------\n",
      "Training Error: 0.7891687018526102\n",
      "Validation Error: 0.9484247228291963\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.0, 0.01, 0.03, 0.1, 0.3]\n",
    "for alpha in alphas:\n",
    "    print(\"Alpha = {}\".format(alpha))\n",
    "    print(\"------------\")\n",
    "    reg_model = Ridge(alpha=alpha)\n",
    "    reg_model.fit(X_train_reg2, y_train2)\n",
    "    print(\"Training Error: {}\".format(calculate_MSE(y_train2, vthreshold_rating(reg_model.predict(X_train_reg2)))))\n",
    "    print(\"Validation Error: {}\".format(calculate_MSE(y_val2, vthreshold_rating(reg_model.predict(X_val_reg2)))))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-109-25810bf4f6a4>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train2['processedReview'] = X_train2['fullReviewText'].apply(lambda x: process_review_text(x, exclude_english, ps))\n",
      "<ipython-input-109-25810bf4f6a4>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val2['processedReview'] = X_val2['fullReviewText'].apply(lambda x: process_review_text(x, exclude_english, ps))\n"
     ]
    }
   ],
   "source": [
    "exclude_english = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "X_train2['processedReview'] = X_train2['fullReviewText'].apply(lambda x: process_review_text(x, exclude_english, ps))\n",
    "X_val2['processedReview'] = X_val2['fullReviewText'].apply(lambda x: process_review_text(x, exclude_english, ps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_features=1500)\n",
    "X_train_cv2 = cv.fit_transform(X_train2['processedReview'])\n",
    "X_val_cv2 = cv.transform(X_val2['processedReview'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reg2_sp = sp.csr_matrix(X_train_reg2)\n",
    "X_train_cv_reg2 = sp.hstack((X_train_cv2, X_train_reg2_sp), format='csr')\n",
    "\n",
    "X_val_reg2_sp = sp.csr_matrix(X_val_reg2)\n",
    "X_val_cv_reg2 = sp.hstack((X_val_cv2, X_val_reg2_sp), format='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.01, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 6.503735872991274\n",
      "Validation Error: 6.883393063136512\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 6.472925759109321\n",
      "Validation Error: 6.873810349242472\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 6.415552951975483\n",
      "Validation Error: 6.766778004848318\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 6.336512333143151\n",
      "Validation Error: 6.702224040577343\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 2.9116597565047635\n",
      "Validation Error: 3.2502935567689333\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 2.868043939305317\n",
      "Validation Error: 3.208625304323009\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 2.7780535325484026\n",
      "Validation Error: 3.092532168888972\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 2.6461880452656565\n",
      "Validation Error: 3.0224225434770875\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 1.0918346220330994\n",
      "Validation Error: 1.3135588463747994\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 1.036147689801702\n",
      "Validation Error: 1.2635888455120803\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.9146986694500452\n",
      "Validation Error: 1.1616875425943325\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7337391925862761\n",
      "Validation Error: 1.0908617788789443\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.756386356086193\n",
      "Validation Error: 0.8996191559372384\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6839209384629215\n",
      "Validation Error: 0.8403416793262685\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5360902242478431\n",
      "Validation Error: 0.7367690362634365\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.31446922435595465\n",
      "Validation Error: 0.6811167992372987\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 1.5627177723097216\n",
      "Validation Error: 1.846452518919343\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 1.5119653601832954\n",
      "Validation Error: 1.7960544459510177\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 1.4032360632395287\n",
      "Validation Error: 1.6804284272855077\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 1.2401300435868885\n",
      "Validation Error: 1.6018033435097432\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8239466745938151\n",
      "Validation Error: 0.9942010555413113\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7620201935774863\n",
      "Validation Error: 0.9443683509025774\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6266979082527001\n",
      "Validation Error: 0.8415338280252339\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.42568372015336575\n",
      "Validation Error: 0.7730068336676122\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7426928131144244\n",
      "Validation Error: 0.885071876050487\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6671662762814085\n",
      "Validation Error: 0.8232709367647633\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.515852358064282\n",
      "Validation Error: 0.7198106463169784\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.2890217747658168\n",
      "Validation Error: 0.6667757772826471\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6771667436192434\n",
      "Validation Error: 0.8162237821223852\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5930182057061686\n",
      "Validation Error: 0.7417321041296694\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8356626506024096\n",
      "Validation Error: 0.6542027155916784\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.1741651887529198\n",
      "Validation Error: 0.6306806169805278\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7532719545067948\n",
      "Validation Error: 0.8984023303350915\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6823394868546818\n",
      "Validation Error: 0.8415287026010446\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5346141250969264\n",
      "Validation Error: 0.7337423743840931\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.3130148012635119\n",
      "Validation Error: 0.6872977133258573\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7040695828928955\n",
      "Validation Error: 0.8450873275924572\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6234277468668492\n",
      "Validation Error: 0.778036234504546\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4601100858885094\n",
      "Validation Error: 0.677615698823088\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6884971464806595\n",
      "Validation Error: 0.6534113519806398\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6550334651549221\n",
      "Validation Error: 0.79237246870429\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5693960927723146\n",
      "Validation Error: 0.7172584528181635\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7959670259987318\n",
      "Validation Error: 0.6401843150790001\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.14231621233302494\n",
      "Validation Error: 0.6375265433888077\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6019752200057845\n",
      "Validation Error: 0.7323421618628351\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8891819911223843\n",
      "Validation Error: 0.6624839452884889\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.29619556817303605\n",
      "Validation Error: 0.6161472705041048\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error: 0.06626693654491513\n",
      "Validation Error: 0.629127338706747\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6695470399035651\n",
      "Validation Error: 0.8320175368867865\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5877134032041788\n",
      "Validation Error: 0.7312280170602444\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.821128725428028\n",
      "Validation Error: 0.6644344297462841\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.18017706776835557\n",
      "Validation Error: 0.6767750787866058\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6250522630858881\n",
      "Validation Error: 0.7799276543267967\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5423968546848821\n",
      "Validation Error: 0.6814512809318369\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7572352568167406\n",
      "Validation Error: 0.6419165688148936\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.10392467283260896\n",
      "Validation Error: 0.6762583510226913\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5903803945789322\n",
      "Validation Error: 0.7412608779634012\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5016847541934427\n",
      "Validation Error: 0.6515057605621919\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.28299454631394083\n",
      "Validation Error: 0.6316645878775758\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.05050973316984014\n",
      "Validation Error: 0.6759090507785038\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5541520219459554\n",
      "Validation Error: 0.6982901469331604\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8248065948002536\n",
      "Validation Error: 0.626830781451005\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.1817675341128254\n",
      "Validation Error: 0.6344574740234872\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.009288338708908602\n",
      "Validation Error: 0.6786818518535688\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.635053766967533\n",
      "Validation Error: 0.8025454034164896\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5565634106569863\n",
      "Validation Error: 0.698938232190281\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.37772052797915046\n",
      "Validation Error: 0.6784713122415739\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.13745887321542427\n",
      "Validation Error: 0.7281470255669896\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5974075260376706\n",
      "Validation Error: 0.7515024210684589\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8964870006341155\n",
      "Validation Error: 0.6696219617387341\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.3067495566385541\n",
      "Validation Error: 0.6694607004808133\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.07818992036583028\n",
      "Validation Error: 0.7319921716274881\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5674944647436064\n",
      "Validation Error: 0.7110229400138558\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.47198463424299336\n",
      "Validation Error: 0.6490063890569089\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.2281092082780791\n",
      "Validation Error: 0.6684967836349853\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.02496529929461796\n",
      "Validation Error: 0.7367603182741793\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5343279686884503\n",
      "Validation Error: 0.6809905159455284\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7973367152821813\n",
      "Validation Error: 0.6322757540964926\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.12355123516941925\n",
      "Validation Error: 0.6746721036001143\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.0011216059666213293\n",
      "Validation Error: 0.7389609370739282\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.01, 0.03, 0.1, 0.3, 0.5]\n",
    "estimators = [50, 100, 200, 500]\n",
    "depths = [1, 2, 5, 10]\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for estimator in estimators:\n",
    "        for depth in depths:\n",
    "            print(\"Learning Rate: {0}, # Estimators: {1}, Depth: {2}\".format(learning_rate, estimator, depth))\n",
    "            print(\"--------------------------------------------------\")\n",
    "            xg_reg = XGBRegressor(\n",
    "                learning_rate=learning_rate, max_depth=depth, n_estimators=estimator)\n",
    "            xg_reg.fit(X_train_cv_reg2, y_train2)\n",
    "            print(\"Training Error: {}\".format(calculate_MSE(y_train2, vthreshold_rating(xg_reg.predict(X_train_cv_reg2)))))\n",
    "            print(\"Validation Error: {}\".format(calculate_MSE(y_val2, vthreshold_rating(xg_reg.predict(X_val_cv_reg2)))))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems `learning_rate=0.3`, `n_estimators=200`, `max_depth=2` performs the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error based on XGBoost CountVectorizer prediction: 0.502\n",
      "Validation error based on XGBoost CountVectorizer prediction: 0.652\n"
     ]
    }
   ],
   "source": [
    "xg_reg = XGBRegressor(learning_rate=0.3, n_estimators=200, max_depth=2)\n",
    "xg_reg.fit(X_train_cv_reg2, y_train2)\n",
    "\n",
    "train_MSE = calculate_MSE(y_train2, vthreshold_rating(xg_reg.predict(X_train_cv_reg2)))\n",
    "val_MSE = calculate_MSE(y_val2, vthreshold_rating(xg_reg.predict(X_val_cv_reg2)))\n",
    "\n",
    "print(\"Training error based on XGBoost CountVectorizer prediction: %.3f\" % train_MSE)\n",
    "print(\"Validation error based on XGBoost CountVectorizer prediction: %.3f\" % val_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is actually much worse compared to the pure language model.\n",
    "\n",
    "However, we could also create a meta model by taking a weighted average of predictions from collaborative filtering and the pure language model. We now try this for a few candidate weightings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight: 0.0\n",
      "------------\n",
      "Training error: 0.812\n",
      "Validation error: 0.950\n",
      "\n",
      "Weight: 0.1\n",
      "------------\n",
      "Training error: 0.754\n",
      "Validation error: 0.882\n",
      "\n",
      "Weight: 0.3\n",
      "------------\n",
      "Training error: 0.658\n",
      "Validation error: 0.768\n",
      "\n",
      "Weight: 0.5\n",
      "------------\n",
      "Training error: 0.587\n",
      "Validation error: 0.681\n",
      "\n",
      "Weight: 0.7\n",
      "------------\n",
      "Training error: 0.542\n",
      "Validation error: 0.621\n",
      "\n",
      "Weight: 0.9\n",
      "------------\n",
      "Training error: 0.522\n",
      "Validation error: 0.589\n",
      "\n",
      "Weight: 1.0\n",
      "------------\n",
      "Training error: 0.522\n",
      "Validation error: 0.583\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weights = [0.0, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0]\n",
    "\n",
    "xg_reg = XGBRegressor(learning_rate=0.3, n_estimators=500, max_depth=2)\n",
    "xg_reg.fit(X_train_cv_reg1, y_train1)\n",
    "\n",
    "reg_train_preds = vthreshold_rating(xg_reg.predict(X_train_cv_reg1))\n",
    "reg_val_preds = vthreshold_rating(xg_reg.predict(X_val_cv_reg1))\n",
    "\n",
    "cf_train_preds = vthreshold_rating(X_train_mod['pred'])\n",
    "cf_val_preds = vthreshold_rating(X_val_mod['pred'])\n",
    "\n",
    "for weight in weights:\n",
    "    print(\"Weight: %.1f\" % weight)\n",
    "    print(\"------------\")\n",
    "    train_MSE = calculate_MSE(y_train1, ((weight*reg_train_preds) + ((1.0 - weight)*cf_train_preds)))\n",
    "    val_MSE = calculate_MSE(y_val1, ((weight*reg_val_preds) + ((1.0 - weight)*cf_val_preds)))\n",
    "    print(\"Training error: %.3f\" % train_MSE)\n",
    "    print(\"Validation error: %.3f\" % val_MSE)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language and Popularity\n",
    "\n",
    "Combining a language model with features representing how popular an item is (how many times it was rated) and how often the user rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_rating_counts = X_train.groupby(X_train['reviewerID'])['price'].count()\n",
    "item_rating_counts = X_train.groupby(X_train['itemID'])['price'].count()\n",
    "user_rating_counts = pd.DataFrame(\n",
    "    user_rating_counts.values, columns=['userRatingCount'], index=user_rating_counts.index).reset_index()\n",
    "item_rating_counts = pd.DataFrame(\n",
    "    item_rating_counts.values, columns=['itemRatingCount'], index=item_rating_counts.index).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_to_zero(val):\n",
    "    \"\"\"Converts `val` to 0 if it is null. Otherwise, `val` is unchanged.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    val: int\n",
    "        The value to be converted.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        0 if `val` is null. Otherwise `val`.\n",
    "    \n",
    "    \"\"\"\n",
    "    if pd.isnull(val):\n",
    "        return 0\n",
    "    return val\n",
    "\n",
    "def append_user_item_counts(data_df, user_counts, item_counts):\n",
    "    \"\"\"Appends `user_counts` and `item_counts` to `data_df`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_df: pd.DataFrame\n",
    "        The DataFrame being appended to.\n",
    "    user_counts: pd.DataFrame\n",
    "        A DataFrame containing user counts.\n",
    "    item_counts: pd.DataFrame\n",
    "        A DataFrame containing item counts.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The DataFrame obtained from `data_df` after appending\n",
    "        `user_counts` and `item_counts`.\n",
    "    \n",
    "    \"\"\"\n",
    "    user_df = pd.merge(data_df, user_counts, how='left', left_on='reviewerID', right_on='reviewerID')\n",
    "    user_item_df = pd.merge(user_df, item_counts, how='left', left_on='itemID', right_on='itemID')\n",
    "    user_item_df['userRatingCount'] = user_item_df['userRatingCount'].apply(null_to_zero)\n",
    "    user_item_df['itemRatingCount'] = user_item_df['itemRatingCount'].apply(null_to_zero)\n",
    "    user_item_df.index = data_df.index\n",
    "    return user_item_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = append_user_item_counts(X_train, user_rating_counts, item_rating_counts)\n",
    "val_df = append_user_item_counts(X_val, user_rating_counts, item_rating_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['cleanedPrice', 'isPop', 'isAlternativeRock', 'isJazz', 'isClassical', 'isDanceElectronic', 'reviewWordCount', 'userRatingCount', 'itemRatingCount']\n",
    "X_train_reg3 = train_df[columns_to_keep]\n",
    "X_val_reg3 = val_df[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-94-063b9bc4b222>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train_reg3['reviewWordCount'] = X_train_reg3['reviewWordCount'].apply(lambda x: np.log(x))\n",
      "<ipython-input-94-063b9bc4b222>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val_reg3['reviewWordCount'] = X_val_reg3['reviewWordCount'].apply(lambda x: np.log(x))\n"
     ]
    }
   ],
   "source": [
    "min_max_scaler = MinMaxScaler()\n",
    "X_train_reg3['reviewWordCount'] = X_train_reg3['reviewWordCount'].apply(lambda x: np.log(x))\n",
    "X_val_reg3['reviewWordCount'] = X_val_reg3['reviewWordCount'].apply(lambda x: np.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-47-ab573a02d313>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.dropna(inplace=True)\n",
      "<ipython-input-47-ab573a02d313>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.dropna(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "X_train_reg3 = clean_dataset(X_train_reg3)\n",
    "y_train3 = y_train[y_train.index.isin(X_train_reg3.index)]\n",
    "X_train3 = X_train[X_train.index.isin(X_train_reg3.index)]\n",
    "\n",
    "X_val_reg3 = clean_dataset(X_val_reg3)\n",
    "y_val3 = y_val[y_val.index.isin(X_val_reg3.index)]\n",
    "X_val3 = X_val[X_val.index.isin(X_val_reg3.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reg3 = min_max_scaler.fit_transform(X_train_reg3)\n",
    "X_val_reg3 = min_max_scaler.transform(X_val_reg3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha = 0.0\n",
      "------------\n",
      "Training Error: 0.9568060307877613\n",
      "Validation Error: 0.9646544613043789\n",
      "\n",
      "Alpha = 0.01\n",
      "------------\n",
      "Training Error: 0.956800728863852\n",
      "Validation Error: 0.9647191518211016\n",
      "\n",
      "Alpha = 0.03\n",
      "------------\n",
      "Training Error: 0.9568009871918867\n",
      "Validation Error: 0.9647195661109845\n",
      "\n",
      "Alpha = 0.1\n",
      "------------\n",
      "Training Error: 0.9568018951045272\n",
      "Validation Error: 0.9647210150463716\n",
      "\n",
      "Alpha = 0.3\n",
      "------------\n",
      "Training Error: 0.9568045191684653\n",
      "Validation Error: 0.9647251444048965\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "vthreshold_rating = np.vectorize(threshold_rating)\n",
    "\n",
    "alphas = [0.0, 0.01, 0.03, 0.1, 0.3]\n",
    "for alpha in alphas:\n",
    "    print(\"Alpha = {}\".format(alpha))\n",
    "    print(\"------------\")\n",
    "    reg_model = Ridge(alpha=alpha)\n",
    "    reg_model.fit(X_train_reg3, y_train3)\n",
    "    print(\"Training Error: {}\".format(calculate_MSE(y_train3, vthreshold_rating(reg_model.predict(X_train_reg3)))))\n",
    "    print(\"Validation Error: {}\".format(calculate_MSE(y_val3, vthreshold_rating(reg_model.predict(X_val_reg3)))))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-98-6f3f49189b95>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train3['processedReview'] = X_train3['fullReviewText'].apply(lambda x: process_review_text(x, exclude_english, ps))\n"
     ]
    }
   ],
   "source": [
    "exclude_english = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "X_train3['processedReview'] = X_train3['fullReviewText'].apply(lambda x: process_review_text(x, exclude_english, ps))\n",
    "X_val3['processedReview'] = X_val3['fullReviewText'].apply(lambda x: process_review_text(x, exclude_english, ps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = TfidfVectorizer()\n",
    "X_train_cv3 = cv.fit_transform(X_train3['processedReview'])\n",
    "X_val_cv3 = cv.transform(X_val3['processedReview'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reg3_sp = sp.csr_matrix(X_train_reg3)\n",
    "X_train_cv_reg3 = sp.hstack((X_train_cv3, X_train_reg3_sp), format='csr')\n",
    "\n",
    "X_val_reg3_sp = sp.csr_matrix(X_val_reg3)\n",
    "X_val_cv_reg3 = sp.hstack((X_val_cv3, X_val_reg3_sp), format='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.03, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 1.6475736165735617\n",
      "Validation Error: 1.6682075246391694\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 1.6071295532038965\n",
      "Validation Error: 1.6310201443086647\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 1.5028318707489112\n",
      "Validation Error: 1.5495761671615798\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.9188830540222396\n",
      "Validation Error: 0.9353547910345448\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8600495191978598\n",
      "Validation Error: 0.881932049456795\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7201781189903523\n",
      "Validation Error: 0.7801585045289339\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8358542125593243\n",
      "Validation Error: 0.8524116989451223\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7575521211963017\n",
      "Validation Error: 0.7816492401441552\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5911242521683677\n",
      "Validation Error: 0.6799836218202735\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7522296784766774\n",
      "Validation Error: 0.7710499979951905\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6590958734068679\n",
      "Validation Error: 0.6963058635540857\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.47524779223081454\n",
      "Validation Error: 0.6169625168916922\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 1000, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6840239875276554\n",
      "Validation Error: 0.708668378563387\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 1000, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5883616702456608\n",
      "Validation Error: 0.6456573269501363\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 1000, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.3945263767180505\n",
      "Validation Error: 0.5883451835989083\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 2000, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6207078584437424\n",
      "Validation Error: 0.6580050611387002\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 2000, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.520265334484549\n",
      "Validation Error: 0.6081044312378009\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 2000, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.31576463665444865\n",
      "Validation Error: 0.5691880078101187\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8477734920732338\n",
      "Validation Error: 0.8635943746474168\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7749300669574346\n",
      "Validation Error: 0.7981384547602882\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6167290560220933\n",
      "Validation Error: 0.6956828488938885\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7882345587966298\n",
      "Validation Error: 0.8054653617464994\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7006842326345166\n",
      "Validation Error: 0.7302868619337366\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5249558345103099\n",
      "Validation Error: 0.6427159067885031\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7203132538824941\n",
      "Validation Error: 0.7408813852056034\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6265104438792016\n",
      "Validation Error: 0.6727893088001192\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4416964683999406\n",
      "Validation Error: 0.6056129960291073\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.633835093681103\n",
      "Validation Error: 0.6679129018559286\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.535126030404487\n",
      "Validation Error: 0.6147146349358291\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.33484181551582953\n",
      "Validation Error: 0.5781107129023814\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 1000, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5734279248759746\n",
      "Validation Error: 0.6253798643746618\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 1000, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4646703381592451\n",
      "Validation Error: 0.5849421595422992\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 1000, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.2582097930287942\n",
      "Validation Error: 0.5684419865565185\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 2000, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5126593867047677\n",
      "Validation Error: 0.5965906853543875\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 2000, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.3902803677299543\n",
      "Validation Error: 0.5710136122022752\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 2000, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.17732587253593382\n",
      "Validation Error: 0.5690646667309286\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7411732383827632\n",
      "Validation Error: 0.759707375282048\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6505469321804074\n",
      "Validation Error: 0.6895700971241407\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4727977921151798\n",
      "Validation Error: 0.6307768432871251\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6735473495300585\n",
      "Validation Error: 0.6989564952069758\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5822821205901875\n",
      "Validation Error: 0.6445336971168617\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.39029499049662936\n",
      "Validation Error: 0.6072173975217089\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6117261411566616\n",
      "Validation Error: 0.6496854485676338\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5124212346375444\n",
      "Validation Error: 0.6101837914618298\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.3095451376107483\n",
      "Validation Error: 0.5915776409381389\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.532571029776479\n",
      "Validation Error: 0.6050473770308531\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.41584513873021584\n",
      "Validation Error: 0.5837684399296836\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error: 0.20162587407172136\n",
      "Validation Error: 0.5881889589189081\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 1000, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.46988545890059963\n",
      "Validation Error: 0.5847003633397732\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 1000, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.3362558793070724\n",
      "Validation Error: 0.5794432857811497\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 1000, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.12375197406176025\n",
      "Validation Error: 0.5985382001059424\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 2000, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.39958957988091076\n",
      "Validation Error: 0.5768256291512879\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 2000, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.25034101761429917\n",
      "Validation Error: 0.5806002474770103\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 2000, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.06019494312452797\n",
      "Validation Error: 0.6085860952332176\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6886184967370189\n",
      "Validation Error: 0.709207693413248\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.605120127446229\n",
      "Validation Error: 0.6623091375899822\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4232876498633171\n",
      "Validation Error: 0.6313728919279659\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6265128749324141\n",
      "Validation Error: 0.6659953986186704\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5391947706513114\n",
      "Validation Error: 0.629572021160769\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.3411706318602605\n",
      "Validation Error: 0.6190735936530634\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5685132945124897\n",
      "Validation Error: 0.625856054874359\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4686430066136844\n",
      "Validation Error: 0.6084846310427406\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.25741324962437\n",
      "Validation Error: 0.6254098415238337\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.48800358806423483\n",
      "Validation Error: 0.5947882639535944\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.3621446359275361\n",
      "Validation Error: 0.6009731684638782\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.142511742186234\n",
      "Validation Error: 0.6411311209017816\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 1000, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4196509881598354\n",
      "Validation Error: 0.5857653200183447\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 1000, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.2727586485507299\n",
      "Validation Error: 0.6041429285982238\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 1000, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.07213499372611183\n",
      "Validation Error: 0.6529643132811481\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 2000, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.3405460738755207\n",
      "Validation Error: 0.589919631885785\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 2000, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.18287273684008926\n",
      "Validation Error: 0.6204527249413128\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 2000, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.024671270151059365\n",
      "Validation Error: 0.6671292855882747\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.03, 0.1, 0.3, 0.5]\n",
    "estimators = [50, 100, 200, 500, 1000, 2000]\n",
    "depths = [1, 2, 5]\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for estimator in estimators:\n",
    "        for depth in depths:\n",
    "            print(\"Learning Rate: {0}, # Estimators: {1}, Depth: {2}\".format(learning_rate, estimator, depth))\n",
    "            print(\"--------------------------------------------------\")\n",
    "            xg_reg = XGBRegressor(\n",
    "                learning_rate=learning_rate, max_depth=depth, n_estimators=estimator)\n",
    "            xg_reg.fit(X_train_cv_reg3, y_train3)\n",
    "            print(\"Training Error: {}\".format(calculate_MSE(y_train3, vthreshold_rating(xg_reg.predict(X_train_cv_reg3)))))\n",
    "            print(\"Validation Error: {}\".format(calculate_MSE(y_val3, vthreshold_rating(xg_reg.predict(X_val_cv_reg3)))))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not any better than the pure language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csc2515-env-3.8",
   "language": "python",
   "name": "csc2515-env-3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
