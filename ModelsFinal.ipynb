{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Models\n",
    "\n",
    "In this notebook we look at some final candidate models and assess their performance on a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "random.seed(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = None\n",
    "with open(os.path.join('data', 'train.json'), 'r') as train_file:\n",
    "    data = [json.loads(row) for row in train_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>category</th>\n",
       "      <th>price</th>\n",
       "      <th>itemID</th>\n",
       "      <th>reviewHash</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>08 24, 2010</td>\n",
       "      <td>u04428712</td>\n",
       "      <td>So is Katy Perry's new album \"Teenage Dream\" c...</td>\n",
       "      <td>Amazing that I Actually Bought This...More Ama...</td>\n",
       "      <td>1282608000</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$35.93</td>\n",
       "      <td>p70761125</td>\n",
       "      <td>85559980</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>10 31, 2009</td>\n",
       "      <td>u06946603</td>\n",
       "      <td>I got this CD almost 10 years ago, and given t...</td>\n",
       "      <td>Excellent album</td>\n",
       "      <td>1256947200</td>\n",
       "      <td>Alternative Rock</td>\n",
       "      <td>$11.28</td>\n",
       "      <td>p85427891</td>\n",
       "      <td>41699565</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>10 13, 2015</td>\n",
       "      <td>u92735614</td>\n",
       "      <td>I REALLY enjoy this pairing of Anderson and Po...</td>\n",
       "      <td>Love the Music, Hate the Light Show</td>\n",
       "      <td>1444694400</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$89.86</td>\n",
       "      <td>p82172532</td>\n",
       "      <td>24751194</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>06 28, 2017</td>\n",
       "      <td>u35112935</td>\n",
       "      <td>Finally got it . It was everything thought it ...</td>\n",
       "      <td>Great</td>\n",
       "      <td>1498608000</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$11.89</td>\n",
       "      <td>p15255251</td>\n",
       "      <td>22820631</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>10 12, 2015</td>\n",
       "      <td>u07141505</td>\n",
       "      <td>Look at all star cast.  Outstanding record, pl...</td>\n",
       "      <td>Love these guys.</td>\n",
       "      <td>1444608000</td>\n",
       "      <td>Jazz</td>\n",
       "      <td>$15.24</td>\n",
       "      <td>p82618188</td>\n",
       "      <td>53377470</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>5.0</td>\n",
       "      <td>07 1, 2005</td>\n",
       "      <td>u35885825</td>\n",
       "      <td>This DVD rates a strong five stars with me.  T...</td>\n",
       "      <td>Absolutely Wonderful</td>\n",
       "      <td>1120176000</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$11.91</td>\n",
       "      <td>p00193036</td>\n",
       "      <td>43343710</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>5.0</td>\n",
       "      <td>04 5, 2017</td>\n",
       "      <td>u10979151</td>\n",
       "      <td>No wonder this sold 32 million copies it is an...</td>\n",
       "      <td>... wonder this sold 32 million copies it is a...</td>\n",
       "      <td>1491350400</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$8.78</td>\n",
       "      <td>p21938211</td>\n",
       "      <td>03563170</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>5.0</td>\n",
       "      <td>06 13, 2005</td>\n",
       "      <td>u50404725</td>\n",
       "      <td>Learning to Breathe is one of my favorite albu...</td>\n",
       "      <td>Another must have from Switchfoot</td>\n",
       "      <td>1118620800</td>\n",
       "      <td>Alternative Rock</td>\n",
       "      <td>$9.99</td>\n",
       "      <td>p78687570</td>\n",
       "      <td>61979023</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>5.0</td>\n",
       "      <td>03 17, 2016</td>\n",
       "      <td>u53979205</td>\n",
       "      <td>I remember first hearing this band when they c...</td>\n",
       "      <td>giving back to the band the love and joy that ...</td>\n",
       "      <td>1458172800</td>\n",
       "      <td>Jazz</td>\n",
       "      <td>$57.37</td>\n",
       "      <td>p95260169</td>\n",
       "      <td>52021524</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>5.0</td>\n",
       "      <td>01 28, 2007</td>\n",
       "      <td>u90545314</td>\n",
       "      <td>cool. They had nothing to say but kind words w...</td>\n",
       "      <td>these WoMen Are</td>\n",
       "      <td>1169942400</td>\n",
       "      <td>Alternative Rock</td>\n",
       "      <td>$26.55</td>\n",
       "      <td>p17894919</td>\n",
       "      <td>28658562</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       overall   reviewTime reviewerID  \\\n",
       "0          4.0  08 24, 2010  u04428712   \n",
       "1          5.0  10 31, 2009  u06946603   \n",
       "2          4.0  10 13, 2015  u92735614   \n",
       "3          5.0  06 28, 2017  u35112935   \n",
       "4          4.0  10 12, 2015  u07141505   \n",
       "...        ...          ...        ...   \n",
       "49995      5.0   07 1, 2005  u35885825   \n",
       "49996      5.0   04 5, 2017  u10979151   \n",
       "49997      5.0  06 13, 2005  u50404725   \n",
       "49998      5.0  03 17, 2016  u53979205   \n",
       "49999      5.0  01 28, 2007  u90545314   \n",
       "\n",
       "                                              reviewText  \\\n",
       "0      So is Katy Perry's new album \"Teenage Dream\" c...   \n",
       "1      I got this CD almost 10 years ago, and given t...   \n",
       "2      I REALLY enjoy this pairing of Anderson and Po...   \n",
       "3      Finally got it . It was everything thought it ...   \n",
       "4      Look at all star cast.  Outstanding record, pl...   \n",
       "...                                                  ...   \n",
       "49995  This DVD rates a strong five stars with me.  T...   \n",
       "49996  No wonder this sold 32 million copies it is an...   \n",
       "49997  Learning to Breathe is one of my favorite albu...   \n",
       "49998  I remember first hearing this band when they c...   \n",
       "49999  cool. They had nothing to say but kind words w...   \n",
       "\n",
       "                                                 summary  unixReviewTime  \\\n",
       "0      Amazing that I Actually Bought This...More Ama...      1282608000   \n",
       "1                                        Excellent album      1256947200   \n",
       "2                    Love the Music, Hate the Light Show      1444694400   \n",
       "3                                                  Great      1498608000   \n",
       "4                                       Love these guys.      1444608000   \n",
       "...                                                  ...             ...   \n",
       "49995                               Absolutely Wonderful      1120176000   \n",
       "49996  ... wonder this sold 32 million copies it is a...      1491350400   \n",
       "49997                  Another must have from Switchfoot      1118620800   \n",
       "49998  giving back to the band the love and joy that ...      1458172800   \n",
       "49999                                    these WoMen Are      1169942400   \n",
       "\n",
       "               category   price     itemID reviewHash image  \n",
       "0                   Pop  $35.93  p70761125   85559980   NaN  \n",
       "1      Alternative Rock  $11.28  p85427891   41699565   NaN  \n",
       "2                   Pop  $89.86  p82172532   24751194   NaN  \n",
       "3                   Pop  $11.89  p15255251   22820631   NaN  \n",
       "4                  Jazz  $15.24  p82618188   53377470   NaN  \n",
       "...                 ...     ...        ...        ...   ...  \n",
       "49995               Pop  $11.91  p00193036   43343710   NaN  \n",
       "49996               Pop   $8.78  p21938211   03563170   NaN  \n",
       "49997  Alternative Rock   $9.99  p78687570   61979023   NaN  \n",
       "49998              Jazz  $57.37  p95260169   52021524   NaN  \n",
       "49999  Alternative Rock  $26.55  p17894919   28658562   NaN  \n",
       "\n",
       "[50000 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.DataFrame(data)\n",
    "data_df = data_df[0:50000]\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_price(price):\n",
    "    \"\"\"Trims `price` to remove the $ sign.\n",
    "    \n",
    "    If the price variable does not have the format $x.xx\n",
    "    then the empty string is returned.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    price: str\n",
    "        A string representing a price.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A string representing `price` but with the $ sign removed,\n",
    "        or the empty string if `price` does not have the correct\n",
    "        format.\n",
    "    \n",
    "    \"\"\"\n",
    "    if (not pd.isnull(price) and isinstance(price, str) and\n",
    "        len(price) > 0 and price[0] == '$'):\n",
    "        return price[1:]\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-d3ba8e2e1b50>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_df['cleanedPrice'] = data_df['cleanedPrice'].astype('float')\n",
      "<ipython-input-5-d3ba8e2e1b50>:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_df['fixedReviewText'] = np.where(pd.isnull(data_df['reviewText']), \"\", data_df['reviewText'])\n",
      "<ipython-input-5-d3ba8e2e1b50>:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_df['fixedSummary'] = np.where(pd.isnull(data_df['summary']), \"\", data_df['summary'])\n",
      "<ipython-input-5-d3ba8e2e1b50>:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_df['fullReviewText'] = data_df['fixedSummary'] + \" \" + data_df['fixedReviewText']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>category</th>\n",
       "      <th>price</th>\n",
       "      <th>itemID</th>\n",
       "      <th>reviewHash</th>\n",
       "      <th>...</th>\n",
       "      <th>reviewHour</th>\n",
       "      <th>reviewMonthYear</th>\n",
       "      <th>cleanedPrice</th>\n",
       "      <th>fullReviewText</th>\n",
       "      <th>isPop</th>\n",
       "      <th>isAlternativeRock</th>\n",
       "      <th>isJazz</th>\n",
       "      <th>isClassical</th>\n",
       "      <th>isDanceElectronic</th>\n",
       "      <th>reviewWordCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>08 24, 2010</td>\n",
       "      <td>u04428712</td>\n",
       "      <td>So is Katy Perry's new album \"Teenage Dream\" c...</td>\n",
       "      <td>Amazing that I Actually Bought This...More Ama...</td>\n",
       "      <td>1282608000</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$35.93</td>\n",
       "      <td>p70761125</td>\n",
       "      <td>85559980</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>2010-08</td>\n",
       "      <td>35.93</td>\n",
       "      <td>Amazing that I Actually Bought This...More Ama...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>10 31, 2009</td>\n",
       "      <td>u06946603</td>\n",
       "      <td>I got this CD almost 10 years ago, and given t...</td>\n",
       "      <td>Excellent album</td>\n",
       "      <td>1256947200</td>\n",
       "      <td>Alternative Rock</td>\n",
       "      <td>$11.28</td>\n",
       "      <td>p85427891</td>\n",
       "      <td>41699565</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>2009-10</td>\n",
       "      <td>11.28</td>\n",
       "      <td>Excellent album I got this CD almost 10 years ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>10 13, 2015</td>\n",
       "      <td>u92735614</td>\n",
       "      <td>I REALLY enjoy this pairing of Anderson and Po...</td>\n",
       "      <td>Love the Music, Hate the Light Show</td>\n",
       "      <td>1444694400</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$89.86</td>\n",
       "      <td>p82172532</td>\n",
       "      <td>24751194</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>2015-10</td>\n",
       "      <td>89.86</td>\n",
       "      <td>Love the Music, Hate the Light Show I REALLY e...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>06 28, 2017</td>\n",
       "      <td>u35112935</td>\n",
       "      <td>Finally got it . It was everything thought it ...</td>\n",
       "      <td>Great</td>\n",
       "      <td>1498608000</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$11.89</td>\n",
       "      <td>p15255251</td>\n",
       "      <td>22820631</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>2017-06</td>\n",
       "      <td>11.89</td>\n",
       "      <td>Great Finally got it . It was everything thoug...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>10 12, 2015</td>\n",
       "      <td>u07141505</td>\n",
       "      <td>Look at all star cast.  Outstanding record, pl...</td>\n",
       "      <td>Love these guys.</td>\n",
       "      <td>1444608000</td>\n",
       "      <td>Jazz</td>\n",
       "      <td>$15.24</td>\n",
       "      <td>p82618188</td>\n",
       "      <td>53377470</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>2015-10</td>\n",
       "      <td>15.24</td>\n",
       "      <td>Love these guys. Look at all star cast.  Outst...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>5.0</td>\n",
       "      <td>07 1, 2005</td>\n",
       "      <td>u35885825</td>\n",
       "      <td>This DVD rates a strong five stars with me.  T...</td>\n",
       "      <td>Absolutely Wonderful</td>\n",
       "      <td>1120176000</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$11.91</td>\n",
       "      <td>p00193036</td>\n",
       "      <td>43343710</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>2005-07</td>\n",
       "      <td>11.91</td>\n",
       "      <td>Absolutely Wonderful This DVD rates a strong f...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>5.0</td>\n",
       "      <td>04 5, 2017</td>\n",
       "      <td>u10979151</td>\n",
       "      <td>No wonder this sold 32 million copies it is an...</td>\n",
       "      <td>... wonder this sold 32 million copies it is a...</td>\n",
       "      <td>1491350400</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$8.78</td>\n",
       "      <td>p21938211</td>\n",
       "      <td>03563170</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>2017-04</td>\n",
       "      <td>8.78</td>\n",
       "      <td>... wonder this sold 32 million copies it is a...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>5.0</td>\n",
       "      <td>06 13, 2005</td>\n",
       "      <td>u50404725</td>\n",
       "      <td>Learning to Breathe is one of my favorite albu...</td>\n",
       "      <td>Another must have from Switchfoot</td>\n",
       "      <td>1118620800</td>\n",
       "      <td>Alternative Rock</td>\n",
       "      <td>$9.99</td>\n",
       "      <td>p78687570</td>\n",
       "      <td>61979023</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>2005-06</td>\n",
       "      <td>9.99</td>\n",
       "      <td>Another must have from Switchfoot Learning to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>5.0</td>\n",
       "      <td>03 17, 2016</td>\n",
       "      <td>u53979205</td>\n",
       "      <td>I remember first hearing this band when they c...</td>\n",
       "      <td>giving back to the band the love and joy that ...</td>\n",
       "      <td>1458172800</td>\n",
       "      <td>Jazz</td>\n",
       "      <td>$57.37</td>\n",
       "      <td>p95260169</td>\n",
       "      <td>52021524</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>2016-03</td>\n",
       "      <td>57.37</td>\n",
       "      <td>giving back to the band the love and joy that ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>5.0</td>\n",
       "      <td>01 28, 2007</td>\n",
       "      <td>u90545314</td>\n",
       "      <td>cool. They had nothing to say but kind words w...</td>\n",
       "      <td>these WoMen Are</td>\n",
       "      <td>1169942400</td>\n",
       "      <td>Alternative Rock</td>\n",
       "      <td>$26.55</td>\n",
       "      <td>p17894919</td>\n",
       "      <td>28658562</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>2007-01</td>\n",
       "      <td>26.55</td>\n",
       "      <td>these WoMen Are cool. They had nothing to say ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49345 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       overall   reviewTime reviewerID  \\\n",
       "0          4.0  08 24, 2010  u04428712   \n",
       "1          5.0  10 31, 2009  u06946603   \n",
       "2          4.0  10 13, 2015  u92735614   \n",
       "3          5.0  06 28, 2017  u35112935   \n",
       "4          4.0  10 12, 2015  u07141505   \n",
       "...        ...          ...        ...   \n",
       "49995      5.0   07 1, 2005  u35885825   \n",
       "49996      5.0   04 5, 2017  u10979151   \n",
       "49997      5.0  06 13, 2005  u50404725   \n",
       "49998      5.0  03 17, 2016  u53979205   \n",
       "49999      5.0  01 28, 2007  u90545314   \n",
       "\n",
       "                                              reviewText  \\\n",
       "0      So is Katy Perry's new album \"Teenage Dream\" c...   \n",
       "1      I got this CD almost 10 years ago, and given t...   \n",
       "2      I REALLY enjoy this pairing of Anderson and Po...   \n",
       "3      Finally got it . It was everything thought it ...   \n",
       "4      Look at all star cast.  Outstanding record, pl...   \n",
       "...                                                  ...   \n",
       "49995  This DVD rates a strong five stars with me.  T...   \n",
       "49996  No wonder this sold 32 million copies it is an...   \n",
       "49997  Learning to Breathe is one of my favorite albu...   \n",
       "49998  I remember first hearing this band when they c...   \n",
       "49999  cool. They had nothing to say but kind words w...   \n",
       "\n",
       "                                                 summary  unixReviewTime  \\\n",
       "0      Amazing that I Actually Bought This...More Ama...      1282608000   \n",
       "1                                        Excellent album      1256947200   \n",
       "2                    Love the Music, Hate the Light Show      1444694400   \n",
       "3                                                  Great      1498608000   \n",
       "4                                       Love these guys.      1444608000   \n",
       "...                                                  ...             ...   \n",
       "49995                               Absolutely Wonderful      1120176000   \n",
       "49996  ... wonder this sold 32 million copies it is a...      1491350400   \n",
       "49997                  Another must have from Switchfoot      1118620800   \n",
       "49998  giving back to the band the love and joy that ...      1458172800   \n",
       "49999                                    these WoMen Are      1169942400   \n",
       "\n",
       "               category   price     itemID reviewHash  ... reviewHour  \\\n",
       "0                   Pop  $35.93  p70761125   85559980  ...         20   \n",
       "1      Alternative Rock  $11.28  p85427891   41699565  ...         20   \n",
       "2                   Pop  $89.86  p82172532   24751194  ...         20   \n",
       "3                   Pop  $11.89  p15255251   22820631  ...         20   \n",
       "4                  Jazz  $15.24  p82618188   53377470  ...         20   \n",
       "...                 ...     ...        ...        ...  ...        ...   \n",
       "49995               Pop  $11.91  p00193036   43343710  ...         20   \n",
       "49996               Pop   $8.78  p21938211   03563170  ...         20   \n",
       "49997  Alternative Rock   $9.99  p78687570   61979023  ...         20   \n",
       "49998              Jazz  $57.37  p95260169   52021524  ...         20   \n",
       "49999  Alternative Rock  $26.55  p17894919   28658562  ...         19   \n",
       "\n",
       "      reviewMonthYear cleanedPrice  \\\n",
       "0             2010-08        35.93   \n",
       "1             2009-10        11.28   \n",
       "2             2015-10        89.86   \n",
       "3             2017-06        11.89   \n",
       "4             2015-10        15.24   \n",
       "...               ...          ...   \n",
       "49995         2005-07        11.91   \n",
       "49996         2017-04         8.78   \n",
       "49997         2005-06         9.99   \n",
       "49998         2016-03        57.37   \n",
       "49999         2007-01        26.55   \n",
       "\n",
       "                                          fullReviewText isPop  \\\n",
       "0      Amazing that I Actually Bought This...More Ama...     1   \n",
       "1      Excellent album I got this CD almost 10 years ...     0   \n",
       "2      Love the Music, Hate the Light Show I REALLY e...     1   \n",
       "3      Great Finally got it . It was everything thoug...     1   \n",
       "4      Love these guys. Look at all star cast.  Outst...     0   \n",
       "...                                                  ...   ...   \n",
       "49995  Absolutely Wonderful This DVD rates a strong f...     1   \n",
       "49996  ... wonder this sold 32 million copies it is a...     1   \n",
       "49997  Another must have from Switchfoot Learning to ...     0   \n",
       "49998  giving back to the band the love and joy that ...     0   \n",
       "49999  these WoMen Are cool. They had nothing to say ...     0   \n",
       "\n",
       "       isAlternativeRock isJazz  isClassical  isDanceElectronic  \\\n",
       "0                      0      0            0                  0   \n",
       "1                      1      0            0                  0   \n",
       "2                      0      0            0                  0   \n",
       "3                      0      0            0                  0   \n",
       "4                      0      1            0                  0   \n",
       "...                  ...    ...          ...                ...   \n",
       "49995                  0      0            0                  0   \n",
       "49996                  0      0            0                  0   \n",
       "49997                  1      0            0                  0   \n",
       "49998                  0      1            0                  0   \n",
       "49999                  1      0            0                  0   \n",
       "\n",
       "       reviewWordCount  \n",
       "0                  277  \n",
       "1                  125  \n",
       "2                  133  \n",
       "3                   15  \n",
       "4                   21  \n",
       "...                ...  \n",
       "49995              130  \n",
       "49996               24  \n",
       "49997              720  \n",
       "49998              137  \n",
       "49999               60  \n",
       "\n",
       "[49345 rows x 23 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "data_df['reviewMonth'] = data_df['reviewTime'].apply(lambda x: x.split(' ')[0])\n",
    "data_df['reviewYear'] = data_df['reviewTime'].apply(lambda x: x.split(' ')[2])\n",
    "data_df['reviewHour'] = data_df['unixReviewTime'].apply(lambda x: datetime.fromtimestamp(x).hour)\n",
    "data_df['reviewMonthYear'] = data_df['reviewYear'] + '-' + data_df['reviewMonth']\n",
    "\n",
    "data_df['cleanedPrice'] = data_df['price'].apply(lambda x: trim_price(x))\n",
    "data_df = data_df[data_df['cleanedPrice'] != \"\"]\n",
    "data_df['cleanedPrice'] = data_df['cleanedPrice'].astype('float')\n",
    "\n",
    "data_df['fixedReviewText'] = np.where(pd.isnull(data_df['reviewText']), \"\", data_df['reviewText'])\n",
    "data_df['fixedSummary'] = np.where(pd.isnull(data_df['summary']), \"\", data_df['summary'])\n",
    "data_df['fullReviewText'] = data_df['fixedSummary'] + \" \" + data_df['fixedReviewText']\n",
    "\n",
    "data_df = data_df.drop(columns=['fixedReviewText', 'fixedSummary'])\n",
    "\n",
    "genres = data_df['category'].unique()\n",
    "\n",
    "for genre in genres:\n",
    "    genre_col = \"is\" + genre.replace(\" \", \"\").replace(\"&\", \"\")\n",
    "    data_df[genre_col] = data_df['category'].apply(lambda x: 1 if x == genre else 0)\n",
    "\n",
    "data_df['reviewWordCount'] = data_df['fullReviewText'].apply(lambda x: len(x.split()))\n",
    "\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_MSE(actuals, predicteds):\n",
    "    \"\"\"Calculates the Mean Squared Error between `actuals` and `predicteds`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    actuals: np.array\n",
    "        A numpy array of the actual values.\n",
    "    predicteds: np.array\n",
    "        A numpy array of the predicted values.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        A float representing the Mean Squared Error between `actuals` and\n",
    "        `predicteds`.\n",
    "    \n",
    "    \"\"\"\n",
    "    return (((actuals - predicteds)**2).sum()) / (len(actuals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "genres = data_df['category'].unique()\n",
    "X_train_set = []\n",
    "X_val_set = []\n",
    "y_train_set = []\n",
    "y_val_set = []\n",
    "\n",
    "for genre in genres:\n",
    "    genre_df = data_df[data_df['category'] == genre]\n",
    "    targets = genre_df['overall']\n",
    "    feature_data = genre_df.drop(columns=['overall'])\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        feature_data, targets, shuffle=True, test_size=0.2, random_state=17)\n",
    "    X_train_set.append(X_train)\n",
    "    X_val_set.append(X_val)\n",
    "    y_train_set.append(y_train)\n",
    "    y_val_set.append(y_val)\n",
    "\n",
    "X_train = pd.concat(X_train_set)\n",
    "X_val = pd.concat(X_val_set)\n",
    "y_train = pd.concat(y_train_set)\n",
    "y_val = pd.concat(y_val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_item_matrix(df, rating_col, user_col, item_col):\n",
    "    return sp.csr_matrix(df[rating_col], (df[user_col], df[item_col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.concat([X_train, pd.DataFrame(y_train, columns=['overall'])], axis=1)\n",
    "val_data = pd.concat([X_val, pd.DataFrame(y_val, columns=['overall'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "item_matrix = train_data.pivot(index='itemID', columns='reviewerID', values='overall')\n",
    "item_matrix = item_matrix.fillna(0)\n",
    "user_item_train_matrix = sp.csr_matrix(item_matrix.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_average = train_data['overall'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "model_knn = NearestNeighbors(metric='cosine', algorithm='brute', n_neighbors=5)\n",
    "model_knn.fit(user_item_train_matrix)\n",
    "item_neighbors = np.asarray(model_knn.kneighbors(user_item_train_matrix, return_distance=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_matrix = train_data.pivot(index='reviewerID', columns='itemID', values='overall')\n",
    "user_matrix = user_matrix.fillna(0)\n",
    "user_item_train_matrix = sp.csr_matrix(user_matrix.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_knn = NearestNeighbors(metric='cosine', algorithm='brute', n_neighbors=5)\n",
    "model_knn.fit(user_item_train_matrix)\n",
    "user_neighbors = np.asarray(model_knn.kneighbors(user_item_train_matrix, return_distance=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user_avg = train_data.groupby(train_data['reviewerID'], as_index=False)['overall'].mean()\n",
    "train_item_avg = train_data.groupby(train_data['itemID'], as_index=False)['overall'].mean()\n",
    "train_user_avg.columns = ['reviewerID', 'userAverage']\n",
    "train_item_avg.columns = ['itemID', 'itemAverage']\n",
    "train_user_avg = train_user_avg.set_index('reviewerID')\n",
    "train_item_avg = train_item_avg.set_index('itemID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_avgs = []\n",
    "for i in range(len(item_neighbors)):\n",
    "    item_avgs.append(train_item_avg['itemAverage'][item_matrix.index[item_neighbors[i]]].mean())\n",
    "\n",
    "item_avgs = pd.concat([pd.DataFrame(item_matrix.index, columns=['itemID']), pd.DataFrame(item_avgs, columns=['itemRating'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_avgs = []\n",
    "for i in range(len(user_neighbors)):\n",
    "    user_avgs.append(train_user_avg['userAverage'][user_matrix.index[user_neighbors[i]]].mean())\n",
    "\n",
    "user_avgs = pd.concat([pd.DataFrame(user_matrix.index, columns=['reviewerID']), pd.DataFrame(user_avgs, columns=['userRating'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average_data(X, total_avg, user_avgs, item_avgs):\n",
    "    \"\"\"Calculates the error based on the weighted average prediction.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: pd.DataFrame\n",
    "        The DataFrame of features.\n",
    "    y: np.array\n",
    "        A numpy array containing the targets\n",
    "    total_avg: float\n",
    "        The average across all users/items.\n",
    "    user_avgs: pd.DataFrame\n",
    "        A DataFrame containing the average rating for each user.\n",
    "    item_avgs: pd.DataFrame\n",
    "        A DataFrame containing the average rating for each item.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        A float representing the mean squared error of the predictions.\n",
    "    \n",
    "    \"\"\"\n",
    "    df_user = pd.merge(X, user_avgs, how='left', on=['reviewerID'])\n",
    "    df_final = pd.merge(df_user, item_avgs, how='left', on=['itemID'])\n",
    "    df_final = df_final[['userRating', 'itemRating']]\n",
    "    df_final.fillna(total_avg)\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_aug = weighted_average_data(X_train, global_average, user_avgs, item_avgs)\n",
    "X_val_aug = weighted_average_data(X_val, global_average, user_avgs, item_avgs)\n",
    "X_train_mod = pd.merge(X_train, X_train_aug, how='left', left_index=True, right_index=True)\n",
    "X_val_mod = pd.merge(X_val, X_val_aug, how='left', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_rating(rating):\n",
    "    \"\"\"Thresholds `rating` to lie in the range [1, 5].\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rating: float\n",
    "        The rating to be thresholded.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        A float representing the thresholded rating.\n",
    "    \n",
    "    \"\"\"\n",
    "    if rating < 1:\n",
    "        return 1\n",
    "    if rating > 5:\n",
    "        return 5\n",
    "    return rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.8908996846476707\n",
      "Validation MSE: 0.08718386896678605\n"
     ]
    }
   ],
   "source": [
    "X_train_mod['pred'] = (0.5 * X_train_mod['userRating']) + (0.5 * X_train_mod['itemRating'])\n",
    "X_train_mod['pred'].apply(lambda x: threshold_rating(x))\n",
    "print(\"Training MSE: {}\".format(calculate_MSE(y_train, X_train_mod['pred'])))\n",
    "\n",
    "X_val_mod['pred'] = (0.5 * X_val_mod['userRating']) + (0.5 * X_val_mod['itemRating'])\n",
    "X_val_mod['pred'].apply(lambda x: threshold_rating(x))\n",
    "print(\"Validation MSE: {}\".format(calculate_MSE(y_val, X_val_mod['pred'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['cleanedPrice', 'isPop', 'isAlternativeRock', 'isJazz', 'isClassical', 'isDanceElectronic', 'reviewWordCount']\n",
    "X_train_reg1 = X_train[columns_to_keep]\n",
    "X_val_reg1 = X_val[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-2b5d7d7878ed>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train_reg1['reviewWordCount'] = X_train_reg1['reviewWordCount'].apply(lambda x: np.log(x))\n",
      "<ipython-input-24-2b5d7d7878ed>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val_reg1['reviewWordCount'] = X_val_reg1['reviewWordCount'].apply(lambda x: np.log(x))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_train_reg1['reviewWordCount'] = X_train_reg1['reviewWordCount'].apply(lambda x: np.log(x))\n",
    "X_val_reg1['reviewWordCount'] = X_val_reg1['reviewWordCount'].apply(lambda x: np.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-550e8da2fe0c>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.dropna(inplace=True)\n",
      "<ipython-input-26-550e8da2fe0c>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.dropna(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "def clean_dataset(df):\n",
    "    df.dropna(inplace=True)\n",
    "    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "    return df[indices_to_keep].astype(np.float64)\n",
    "\n",
    "X_train_reg1 = clean_dataset(X_train_reg1)\n",
    "y_train1 = y_train[y_train.index.isin(X_train_reg1.index)]\n",
    "X_train1 = X_train[X_train.index.isin(X_train_reg1.index)]\n",
    "\n",
    "X_val_reg1 = clean_dataset(X_val_reg1)\n",
    "y_val1 = y_val[y_val.index.isin(X_val_reg1.index)]\n",
    "X_val1 = X_val[X_val.index.isin(X_val_reg1.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reg1 = min_max_scaler.fit_transform(X_train_reg1)\n",
    "X_val_reg1 = min_max_scaler.transform(X_val_reg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha = 0.0\n",
      "------------\n",
      "Training Error: 0.9616192418446101\n",
      "Validation Error: 0.9227567987732785\n",
      "\n",
      "Alpha = 0.01\n",
      "------------\n",
      "Training Error: 0.9616194518528689\n",
      "Validation Error: 0.9227570277991929\n",
      "\n",
      "Alpha = 0.03\n",
      "------------\n",
      "Training Error: 0.9616198715719257\n",
      "Validation Error: 0.9227574853932203\n",
      "\n",
      "Alpha = 0.1\n",
      "------------\n",
      "Training Error: 0.9616213374549035\n",
      "Validation Error: 0.9227590821637957\n",
      "\n",
      "Alpha = 0.3\n",
      "------------\n",
      "Training Error: 0.9616254985913107\n",
      "Validation Error: 0.9227636031245654\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "vthreshold_rating = np.vectorize(threshold_rating)\n",
    "\n",
    "alphas = [0.0, 0.01, 0.03, 0.1, 0.3]\n",
    "for alpha in alphas:\n",
    "    print(\"Alpha = {}\".format(alpha))\n",
    "    print(\"------------\")\n",
    "    reg_model = Ridge(alpha=alpha)\n",
    "    reg_model.fit(X_train_reg1, y_train1)\n",
    "    print(\"Training Error: {}\".format(calculate_MSE(y_train1, vthreshold_rating(reg_model.predict(X_train_reg1)))))\n",
    "    print(\"Validation Error: {}\".format(calculate_MSE(y_val1, vthreshold_rating(reg_model.predict(X_val_reg1)))))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Matthew/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def process_review_text(review_text, exclude_text, ps):\n",
    "    \"\"\"Pre-processes the text given by `review_text`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    review_text: str\n",
    "        The review text to be processed.\n",
    "    exclude_text: collection\n",
    "        A collection of words to be excluded.\n",
    "    ps: PorterStemmer\n",
    "        The PorterStemmer used to perform word stemming.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A string representing the processed version of `review_text`.\n",
    "    \n",
    "    \"\"\"\n",
    "    review = re.sub('[^a-zA-Z0-9]', ' ', review_text).lower().split()\n",
    "    review = [ps.stem(word) for word in review if not word in exclude_text]\n",
    "    return ' '.join(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-35-0d22f415ca83>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train1['processedReview'] = X_train1['fullReviewText'].apply(lambda x: process_review_text(x, exclude_english, ps))\n"
     ]
    }
   ],
   "source": [
    "exclude_english = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "X_train1['processedReview'] = X_train1['fullReviewText'].apply(lambda x: process_review_text(x, exclude_english, ps))\n",
    "X_val1['processedReview'] = X_val1['fullReviewText'].apply(lambda x: process_review_text(x, exclude_english, ps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=1500)\n",
    "X_train_cv1 = cv.fit_transform(X_train1['processedReview'])\n",
    "X_val_cv1 = cv.transform(X_val1['processedReview'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "X_train_reg1_sp = sp.csr_matrix(X_train_reg1)\n",
    "X_train_cv_reg1 = sp.hstack((X_train_cv1, X_train_reg1_sp), format='csr')\n",
    "\n",
    "X_val_reg1_sp = sp.csr_matrix(X_val_reg1)\n",
    "X_val_cv_reg1 = sp.hstack((X_val_cv1, X_val_reg1_sp), format='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.01, # Estimators: 10, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 12.593585326307256\n",
      "Validation Error: 12.67473662884927\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 10, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 12.593585326307256\n",
      "Validation Error: 12.67473662884927\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 10, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 12.593585326307256\n",
      "Validation Error: 12.67473662884927\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 10, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 12.593585326307256\n",
      "Validation Error: 12.67473662884927\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 6.550438196243737\n",
      "Validation Error: 6.5880611764144055\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 6.52725632182597\n",
      "Validation Error: 6.5632085015484405\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 6.474950815812452\n",
      "Validation Error: 6.515235130242674\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 6.4049085627239455\n",
      "Validation Error: 6.477203041090882\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 2.9909864425056485\n",
      "Validation Error: 2.997231572241453\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 2.9564379683192237\n",
      "Validation Error: 2.9637166045040493\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 2.873040358066219\n",
      "Validation Error: 2.8916097337214017\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 2.7534970883270704\n",
      "Validation Error: 2.830574463811401\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 1.1937534112945924\n",
      "Validation Error: 1.1718490640807513\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 1.1441831520686658\n",
      "Validation Error: 1.1263715481680487\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 1.0252407078650987\n",
      "Validation Error: 1.0321461501430618\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.852664873225294\n",
      "Validation Error: 0.963022979505327\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8703233418106068\n",
      "Validation Error: 0.8367534444691025\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7947228009449323\n",
      "Validation Error: 0.7710466290913105\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6360947270578214\n",
      "Validation Error: 0.6642658509658224\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4161352145181226\n",
      "Validation Error: 0.611486775855196\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 10, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 9.271438544936347\n",
      "Validation Error: 9.326933996866611\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 10, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 9.255338634619985\n",
      "Validation Error: 9.308521049507627\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 10, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 9.218447255178416\n",
      "Validation Error: 9.275306293649876\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 10, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 9.170216935333631\n",
      "Validation Error: 9.246704046420033\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 1.6570505918247893\n",
      "Validation Error: 1.6454092509129405\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 1.615126670679494\n",
      "Validation Error: 1.6058904579429243\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 1.509834054256618\n",
      "Validation Error: 1.5196953278003371\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 1.356986368033661\n",
      "Validation Error: 1.4492351294798704\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.9341432160528668\n",
      "Validation Error: 0.9026902615825739\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8727823901413795\n",
      "Validation Error: 0.8484195301581637\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7330257732508165\n",
      "Validation Error: 0.7446059868470096\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5368512274330656\n",
      "Validation Error: 0.6788133793977833\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8565662159617881\n",
      "Validation Error: 0.8240333250046581\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.776987463511858\n",
      "Validation Error: 0.7550957086739946\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6123600752696048\n",
      "Validation Error: 0.6492103408307265\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.38246395708167485\n",
      "Validation Error: 0.6017717985809833\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7858319123583294\n",
      "Validation Error: 0.7604087049139929\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6917749175550089\n",
      "Validation Error: 0.6841563291014993\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5082321401658175\n",
      "Validation Error: 0.6010547539084455\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.23993775201305986\n",
      "Validation Error: 0.578250682220027\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 10, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 2.800249774712073\n",
      "Validation Error: 2.8040603021238137\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 10, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 2.764864956515802\n",
      "Validation Error: 2.7700744000737845\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 10, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 2.681838410200421\n",
      "Validation Error: 2.6997213323235636\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 10, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 2.5619665378306773\n",
      "Validation Error: 2.639599636206951\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.867148493248728\n",
      "Validation Error: 0.8336839756217429\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.791391856772724\n",
      "Validation Error: 0.7694129941430407\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6332699947423606\n",
      "Validation Error: 0.6620020255147467\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.41317734951233004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: 0.6136955626194984\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8155976600043131\n",
      "Validation Error: 0.7875032466405151\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.726521861767224\n",
      "Validation Error: 0.7121344775180954\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5527333729149018\n",
      "Validation Error: 0.6203616906595741\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.30108368671816804\n",
      "Validation Error: 0.5902296716747013\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7603381182947971\n",
      "Validation Error: 0.7376392402745808\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6648155092944541\n",
      "Validation Error: 0.6638582645749459\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.47396974475635406\n",
      "Validation Error: 0.5945472381691608\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.20543200991565497\n",
      "Validation Error: 0.5827043504658799\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6971206480310448\n",
      "Validation Error: 0.6846994953320112\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.60007015475517\n",
      "Validation Error: 0.6218760045905573\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.3654822775933115\n",
      "Validation Error: 0.5757436571266696\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.10483945766867236\n",
      "Validation Error: 0.582446464864197\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 10, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.90630787055686\n",
      "Validation Error: 0.8703424341976994\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 10, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8470989262277072\n",
      "Validation Error: 0.8191118046829554\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 10, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7106758839756667\n",
      "Validation Error: 0.7186487333999634\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 10, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5164023832497643\n",
      "Validation Error: 0.6749275315829953\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7746449490504231\n",
      "Validation Error: 0.7514757489764072\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6847754627555952\n",
      "Validation Error: 0.6779093578458835\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5131316034291978\n",
      "Validation Error: 0.6102139158017111\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.25519782595256785\n",
      "Validation Error: 0.6173926426621129\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7228255305273165\n",
      "Validation Error: 0.7061105834087219\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.631835413366388\n",
      "Validation Error: 0.6402295038736551\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.43232484033800106\n",
      "Validation Error: 0.5940919488580755\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.16402267913303564\n",
      "Validation Error: 0.6226955755786499\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.681322026788915\n",
      "Validation Error: 0.6714017452417257\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5841600554522332\n",
      "Validation Error: 0.6142663970560962\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.35062358909827873\n",
      "Validation Error: 0.586114673051449\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.08483914548700379\n",
      "Validation Error: 0.630589504293489\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6398188580529599\n",
      "Validation Error: 0.6442813681258396\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5210280146836813\n",
      "Validation Error: 0.5889768598483542\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.23684211930430282\n",
      "Validation Error: 0.5936169865564452\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.0175730486042508\n",
      "Validation Error: 0.6424307892035772\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 10, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8614753955445565\n",
      "Validation Error: 0.8263920827448844\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 10, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7868724873501688\n",
      "Validation Error: 0.761233734104035\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 10, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6357116330975928\n",
      "Validation Error: 0.6629179293582605\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 10, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4359348830501427\n",
      "Validation Error: 0.6642361834451783\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7272273248859753\n",
      "Validation Error: 0.7102072582689162\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6435634129757966\n",
      "Validation Error: 0.6490353286822046\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4602870744958097\n",
      "Validation Error: 0.616673934157829\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.19232066209548598\n",
      "Validation Error: 0.6739816465717698\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6854570175303519\n",
      "Validation Error: 0.674423587059751\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5999071526738016\n",
      "Validation Error: 0.6237167805395513\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.3808621059591676\n",
      "Validation Error: 0.6152528650488003\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.09856997710601134\n",
      "Validation Error: 0.6927743763550213\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6523956387854117\n",
      "Validation Error: 0.6510570146909723\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5564683130931524\n",
      "Validation Error: 0.6081003620355961\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.2924400527963914\n",
      "Validation Error: 0.6209486484816141\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error: 0.04102141880108724\n",
      "Validation Error: 0.7051522031851937\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6162994432119324\n",
      "Validation Error: 0.6326113331540619\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4906136669008255\n",
      "Validation Error: 0.5963565134729887\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.17578364274699398\n",
      "Validation Error: 0.6432141219299883\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.0030042775739243374\n",
      "Validation Error: 0.7145314862685995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "learning_rates = [0.01, 0.03, 0.1, 0.3, 0.5]\n",
    "estimators = [10, 50, 100, 200, 500]\n",
    "depths = [1, 2, 5, 10]\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for estimator in estimators:\n",
    "        for depth in depths:\n",
    "            print(\"Learning Rate: {0}, # Estimators: {1}, Depth: {2}\".format(learning_rate, estimator, depth))\n",
    "            print(\"--------------------------------------------------\")\n",
    "            xg_reg = XGBRegressor(\n",
    "                learning_rate=learning_rate, max_depth=depth, n_estimators=estimator)\n",
    "            xg_reg.fit(X_train_cv_reg1, y_train1)\n",
    "            print(\"Training Error: {}\".format(calculate_MSE(y_train1, vthreshold_rating(xg_reg.predict(X_train_cv_reg1)))))\n",
    "            print(\"Validation Error: {}\".format(calculate_MSE(y_val1, vthreshold_rating(xg_reg.predict(X_val_cv_reg1)))))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that `learning_rate=0.3`, `n_estimators=500`, and `max_depth=2` provides a very good model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error based on XGBoost CountVectorizer prediction: 0.521\n",
      "Validation error based on XGBoost CountVectorizer prediction: 0.589\n"
     ]
    }
   ],
   "source": [
    "xg_reg = XGBRegressor(learning_rate=0.3, n_estimators=500, max_depth=2)\n",
    "xg_reg.fit(X_train_cv_reg1, y_train1)\n",
    "\n",
    "train_MSE = calculate_MSE(y_train1, vthreshold_rating(xg_reg.predict(X_train_cv_reg1)))\n",
    "val_MSE = calculate_MSE(y_val1, vthreshold_rating(xg_reg.predict(X_val_cv_reg1)))\n",
    "\n",
    "print(\"Training error based on XGBoost CountVectorizer prediction: %.3f\" % train_MSE)\n",
    "print(\"Validation error based on XGBoost CountVectorizer prediction: %.3f\" % val_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['cleanedPrice', 'isPop', 'isAlternativeRock', 'isJazz', 'isClassical', 'isDanceElectronic', 'reviewWordCount', 'userRating', 'itemRating']\n",
    "X_train_reg2 = X_train_mod[columns_to_keep]\n",
    "X_val_reg2 = X_val_mod[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-42-d8a296a7aa94>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train_reg2['reviewWordCount'] = X_train_reg2['reviewWordCount'].apply(lambda x: np.log(x))\n",
      "<ipython-input-42-d8a296a7aa94>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val_reg2['reviewWordCount'] = X_val_reg2['reviewWordCount'].apply(lambda x: np.log(x))\n"
     ]
    }
   ],
   "source": [
    "min_max_scaler = MinMaxScaler()\n",
    "X_train_reg2['reviewWordCount'] = X_train_reg2['reviewWordCount'].apply(lambda x: np.log(x))\n",
    "X_val_reg2['reviewWordCount'] = X_val_reg2['reviewWordCount'].apply(lambda x: np.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-550e8da2fe0c>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.dropna(inplace=True)\n",
      "<ipython-input-26-550e8da2fe0c>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.dropna(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "X_train_reg2 = clean_dataset(X_train_reg2)\n",
    "y_train2 = y_train[y_train.index.isin(X_train_reg2.index)]\n",
    "X_train2 = X_train[X_train.index.isin(X_train_reg2.index)]\n",
    "\n",
    "X_val_reg2 = clean_dataset(X_val_reg2)\n",
    "y_val2 = y_val[y_val.index.isin(X_val_reg2.index)]\n",
    "X_val2 = X_val[X_val.index.isin(X_val_reg2.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reg2 = min_max_scaler.fit_transform(X_train_reg2)\n",
    "X_val_reg2 = min_max_scaler.transform(X_val_reg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha = 0.0\n",
      "------------\n",
      "Training Error: 0.9668155670902776\n",
      "Validation Error: 1.0283431498886901\n",
      "\n",
      "Alpha = 0.01\n",
      "------------\n",
      "Training Error: 0.9665439405257655\n",
      "Validation Error: 1.0277700821753468\n",
      "\n",
      "Alpha = 0.03\n",
      "------------\n",
      "Training Error: 0.9665445701690942\n",
      "Validation Error: 1.0277694869048901\n",
      "\n",
      "Alpha = 0.1\n",
      "------------\n",
      "Training Error: 0.9665467810522257\n",
      "Validation Error: 1.0277674698594539\n",
      "\n",
      "Alpha = 0.3\n",
      "------------\n",
      "Training Error: 0.9665531504372282\n",
      "Validation Error: 1.0277622454507893\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.0, 0.01, 0.03, 0.1, 0.3]\n",
    "for alpha in alphas:\n",
    "    print(\"Alpha = {}\".format(alpha))\n",
    "    print(\"------------\")\n",
    "    reg_model = Ridge(alpha=alpha)\n",
    "    reg_model.fit(X_train_reg2, y_train2)\n",
    "    print(\"Training Error: {}\".format(calculate_MSE(y_train2, vthreshold_rating(reg_model.predict(X_train_reg2)))))\n",
    "    print(\"Validation Error: {}\".format(calculate_MSE(y_val2, vthreshold_rating(reg_model.predict(X_val_reg2)))))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-46-25810bf4f6a4>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train2['processedReview'] = X_train2['fullReviewText'].apply(lambda x: process_review_text(x, exclude_english, ps))\n",
      "<ipython-input-46-25810bf4f6a4>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val2['processedReview'] = X_val2['fullReviewText'].apply(lambda x: process_review_text(x, exclude_english, ps))\n"
     ]
    }
   ],
   "source": [
    "exclude_english = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "X_train2['processedReview'] = X_train2['fullReviewText'].apply(lambda x: process_review_text(x, exclude_english, ps))\n",
    "X_val2['processedReview'] = X_val2['fullReviewText'].apply(lambda x: process_review_text(x, exclude_english, ps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_features=1500)\n",
    "X_train_cv2 = cv.fit_transform(X_train2['processedReview'])\n",
    "X_val_cv2 = cv.transform(X_val2['processedReview'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reg2_sp = sp.csr_matrix(X_train_reg2)\n",
    "X_train_cv_reg2 = sp.hstack((X_train_cv2, X_train_reg2_sp), format='csr')\n",
    "\n",
    "X_val_reg2_sp = sp.csr_matrix(X_val_reg2)\n",
    "X_val_cv_reg2 = sp.hstack((X_val_cv2, X_val_reg2_sp), format='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.01, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 6.550438196243737\n",
      "Validation Error: 6.5880611764144055\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 6.52725632182597\n",
      "Validation Error: 6.5632085015484405\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 6.474950815812452\n",
      "Validation Error: 6.515235130242674\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 6.4049085627239455\n",
      "Validation Error: 6.477203041090882\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 2.9909864425056485\n",
      "Validation Error: 2.997231572241453\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 2.9564379683192237\n",
      "Validation Error: 2.9637166045040493\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 2.873040358066219\n",
      "Validation Error: 2.8916097337214017\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 2.7534970883270704\n",
      "Validation Error: 2.830574463811401\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 1.1937534112945924\n",
      "Validation Error: 1.1718490640807513\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 1.1441831520686658\n",
      "Validation Error: 1.1263715481680487\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 1.0252407078650987\n",
      "Validation Error: 1.0321461501430618\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.852664873225294\n",
      "Validation Error: 0.963022979505327\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8703233418106068\n",
      "Validation Error: 0.8367534444691025\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7947228009449323\n",
      "Validation Error: 0.7710466290913105\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6360947270578214\n",
      "Validation Error: 0.6642658509658224\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4161352145181226\n",
      "Validation Error: 0.611486775855196\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 1.6570505918247893\n",
      "Validation Error: 1.6454092509129405\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 1.615126670679494\n",
      "Validation Error: 1.6058904579429243\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 1.509834054256618\n",
      "Validation Error: 1.5196953278003371\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 1.356986368033661\n",
      "Validation Error: 1.4492351294798704\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.9341432160528668\n",
      "Validation Error: 0.9026902615825739\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8727823901413795\n",
      "Validation Error: 0.8484195301581637\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7330257732508165\n",
      "Validation Error: 0.7446059868470096\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5368512274330656\n",
      "Validation Error: 0.6788133793977833\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8565662159617881\n",
      "Validation Error: 0.8240333250046581\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.776987463511858\n",
      "Validation Error: 0.7550957086739946\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6123600752696048\n",
      "Validation Error: 0.6492103408307265\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.38246395708167485\n",
      "Validation Error: 0.6017717985809833\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7858319123583294\n",
      "Validation Error: 0.7604087049139929\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6917749175550089\n",
      "Validation Error: 0.6841563291014993\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5082321401658175\n",
      "Validation Error: 0.6010547539084455\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.23993775201305986\n",
      "Validation Error: 0.578250682220027\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.867148493248728\n",
      "Validation Error: 0.8336839756217429\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.791391856772724\n",
      "Validation Error: 0.7694129941430407\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6332699947423606\n",
      "Validation Error: 0.6620020255147467\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.41317734951233004\n",
      "Validation Error: 0.6136955626194984\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8155976600043131\n",
      "Validation Error: 0.7875032466405151\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.726521861767224\n",
      "Validation Error: 0.7121344775180954\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5527333729149018\n",
      "Validation Error: 0.6203616906595741\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.30108368671816804\n",
      "Validation Error: 0.5902296716747013\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7603381182947971\n",
      "Validation Error: 0.7376392402745808\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6648155092944541\n",
      "Validation Error: 0.6638582645749459\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.47396974475635406\n",
      "Validation Error: 0.5945472381691608\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.20543200991565497\n",
      "Validation Error: 0.5827043504658799\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6971206480310448\n",
      "Validation Error: 0.6846994953320112\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.60007015475517\n",
      "Validation Error: 0.6218760045905573\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.3654822775933115\n",
      "Validation Error: 0.5757436571266696\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error: 0.10483945766867236\n",
      "Validation Error: 0.582446464864197\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7746449490504231\n",
      "Validation Error: 0.7514757489764072\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6847754627555952\n",
      "Validation Error: 0.6779093578458835\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5131316034291978\n",
      "Validation Error: 0.6102139158017111\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.25519782595256785\n",
      "Validation Error: 0.6173926426621129\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7228255305273165\n",
      "Validation Error: 0.7061105834087219\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.631835413366388\n",
      "Validation Error: 0.6402295038736551\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.43232484033800106\n",
      "Validation Error: 0.5940919488580755\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.16402267913303564\n",
      "Validation Error: 0.6226955755786499\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.681322026788915\n",
      "Validation Error: 0.6714017452417257\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5841600554522332\n",
      "Validation Error: 0.6142663970560962\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.35062358909827873\n",
      "Validation Error: 0.586114673051449\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.08483914548700379\n",
      "Validation Error: 0.630589504293489\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6398188580529599\n",
      "Validation Error: 0.6442813681258396\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5210280146836813\n",
      "Validation Error: 0.5889768598483542\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.23684211930430282\n",
      "Validation Error: 0.5936169865564452\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.0175730486042508\n",
      "Validation Error: 0.6424307892035772\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7272273248859753\n",
      "Validation Error: 0.7102072582689162\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6435634129757966\n",
      "Validation Error: 0.6490353286822046\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4602870744958097\n",
      "Validation Error: 0.616673934157829\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.19232066209548598\n",
      "Validation Error: 0.6739816465717698\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6854570175303519\n",
      "Validation Error: 0.674423587059751\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5999071526738016\n",
      "Validation Error: 0.6237167805395513\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.3808621059591676\n",
      "Validation Error: 0.6152528650488003\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.09856997710601134\n",
      "Validation Error: 0.6927743763550213\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6523956387854117\n",
      "Validation Error: 0.6510570146909723\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5564683130931524\n",
      "Validation Error: 0.6081003620355961\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.2924400527963914\n",
      "Validation Error: 0.6209486484816141\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.04102141880108724\n",
      "Validation Error: 0.7051522031851937\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6162994432119324\n",
      "Validation Error: 0.6326113331540619\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4906136669008255\n",
      "Validation Error: 0.5963565134729887\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.17578364274699398\n",
      "Validation Error: 0.6432141219299883\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.0030042775739243374\n",
      "Validation Error: 0.7145314862685995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.01, 0.03, 0.1, 0.3, 0.5]\n",
    "estimators = [50, 100, 200, 500]\n",
    "depths = [1, 2, 5, 10]\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for estimator in estimators:\n",
    "        for depth in depths:\n",
    "            print(\"Learning Rate: {0}, # Estimators: {1}, Depth: {2}\".format(learning_rate, estimator, depth))\n",
    "            print(\"--------------------------------------------------\")\n",
    "            xg_reg = XGBRegressor(\n",
    "                learning_rate=learning_rate, max_depth=depth, n_estimators=estimator)\n",
    "            xg_reg.fit(X_train_cv_reg1, y_train1)\n",
    "            print(\"Training Error: {}\".format(calculate_MSE(y_train1, vthreshold_rating(xg_reg.predict(X_train_cv_reg1)))))\n",
    "            print(\"Validation Error: {}\".format(calculate_MSE(y_val1, vthreshold_rating(xg_reg.predict(X_val_cv_reg1)))))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems `learning_rate=0.3`, `n_estimators=500`, `max_depth=2` performs the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error based on XGBoost CountVectorizer prediction: 0.459\n",
      "Validation error based on XGBoost CountVectorizer prediction: 0.651\n"
     ]
    }
   ],
   "source": [
    "xg_reg = XGBRegressor(learning_rate=0.3, n_estimators=1000, max_depth=2)\n",
    "xg_reg.fit(X_train_cv_reg2, y_train2)\n",
    "\n",
    "train_MSE = calculate_MSE(y_train2, vthreshold_rating(xg_reg.predict(X_train_cv_reg2)))\n",
    "val_MSE = calculate_MSE(y_val2, vthreshold_rating(xg_reg.predict(X_val_cv_reg2)))\n",
    "\n",
    "print(\"Training error based on XGBoost CountVectorizer prediction: %.3f\" % train_MSE)\n",
    "print(\"Validation error based on XGBoost CountVectorizer prediction: %.3f\" % val_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csc2515-env-3.8",
   "language": "python",
   "name": "csc2515-env-3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
