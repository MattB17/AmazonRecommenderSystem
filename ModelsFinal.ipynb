{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Models\n",
    "\n",
    "In this notebook we look at some final candidate models and assess their performance on a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "random.seed(17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "\n",
    "We will subsample 50,000 of the 200,000 records for training by sampling 25% of the data from each of the five music categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = None\n",
    "with open(os.path.join('data', 'train.json'), 'r') as train_file:\n",
    "    data = [json.loads(row) for row in train_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame(data).drop(columns=['image'])\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>category</th>\n",
       "      <th>price</th>\n",
       "      <th>itemID</th>\n",
       "      <th>reviewHash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>10 13, 2015</td>\n",
       "      <td>u92735614</td>\n",
       "      <td>I REALLY enjoy this pairing of Anderson and Po...</td>\n",
       "      <td>Love the Music, Hate the Light Show</td>\n",
       "      <td>1444694400</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$89.86</td>\n",
       "      <td>p82172532</td>\n",
       "      <td>24751194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>06 28, 2017</td>\n",
       "      <td>u35112935</td>\n",
       "      <td>Finally got it . It was everything thought it ...</td>\n",
       "      <td>Great</td>\n",
       "      <td>1498608000</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$11.89</td>\n",
       "      <td>p15255251</td>\n",
       "      <td>22820631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.0</td>\n",
       "      <td>04 3, 2002</td>\n",
       "      <td>u25030850</td>\n",
       "      <td>Me personally I am not a big fan of Pearl Jam ...</td>\n",
       "      <td>Hmmmmm...........</td>\n",
       "      <td>1017792000</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$6.81</td>\n",
       "      <td>p99659606</td>\n",
       "      <td>41124728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.0</td>\n",
       "      <td>11 16, 2006</td>\n",
       "      <td>u40719083</td>\n",
       "      <td>Katchen's performace throughout this collectio...</td>\n",
       "      <td>Superb interpretations by Katchen</td>\n",
       "      <td>1163635200</td>\n",
       "      <td>Classical</td>\n",
       "      <td>$31.04</td>\n",
       "      <td>p63362921</td>\n",
       "      <td>40704096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.0</td>\n",
       "      <td>09 19, 2016</td>\n",
       "      <td>u55172429</td>\n",
       "      <td>Wow, I had this in the late 60s, I saw Deaf Ge...</td>\n",
       "      <td>Hello Darkness my old friend......</td>\n",
       "      <td>1474243200</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$5.89</td>\n",
       "      <td>p72166335</td>\n",
       "      <td>75947625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199983</th>\n",
       "      <td>5.0</td>\n",
       "      <td>02 9, 2017</td>\n",
       "      <td>u57558427</td>\n",
       "      <td>Those old crooners like Hartman really had a w...</td>\n",
       "      <td>Hartman and Wine Get Better With Age</td>\n",
       "      <td>1486598400</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$1.98</td>\n",
       "      <td>p33434439</td>\n",
       "      <td>24285962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199986</th>\n",
       "      <td>5.0</td>\n",
       "      <td>11 8, 2015</td>\n",
       "      <td>u85136324</td>\n",
       "      <td>Love it</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1446940800</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$11.88</td>\n",
       "      <td>p02978017</td>\n",
       "      <td>27368058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199987</th>\n",
       "      <td>4.0</td>\n",
       "      <td>03 23, 2014</td>\n",
       "      <td>u32710934</td>\n",
       "      <td>Love priest, halford one of the best live show...</td>\n",
       "      <td>Remaster ?</td>\n",
       "      <td>1395532800</td>\n",
       "      <td>Alternative Rock</td>\n",
       "      <td>$11.89</td>\n",
       "      <td>p75893713</td>\n",
       "      <td>97935046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199988</th>\n",
       "      <td>5.0</td>\n",
       "      <td>10 20, 2014</td>\n",
       "      <td>u37750462</td>\n",
       "      <td>Great</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1413763200</td>\n",
       "      <td>Alternative Rock</td>\n",
       "      <td>$18.98</td>\n",
       "      <td>p47415569</td>\n",
       "      <td>75099110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>4.0</td>\n",
       "      <td>05 1, 2004</td>\n",
       "      <td>u68902609</td>\n",
       "      <td>With this, Mariah's third album, Mariah proved...</td>\n",
       "      <td>Well Done Mariah! You Show 'Em!</td>\n",
       "      <td>1083369600</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$7.98</td>\n",
       "      <td>p84118731</td>\n",
       "      <td>35077372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50001 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        overall   reviewTime reviewerID  \\\n",
       "2           4.0  10 13, 2015  u92735614   \n",
       "3           5.0  06 28, 2017  u35112935   \n",
       "10          3.0   04 3, 2002  u25030850   \n",
       "11          5.0  11 16, 2006  u40719083   \n",
       "16          5.0  09 19, 2016  u55172429   \n",
       "...         ...          ...        ...   \n",
       "199983      5.0   02 9, 2017  u57558427   \n",
       "199986      5.0   11 8, 2015  u85136324   \n",
       "199987      4.0  03 23, 2014  u32710934   \n",
       "199988      5.0  10 20, 2014  u37750462   \n",
       "199995      4.0   05 1, 2004  u68902609   \n",
       "\n",
       "                                               reviewText  \\\n",
       "2       I REALLY enjoy this pairing of Anderson and Po...   \n",
       "3       Finally got it . It was everything thought it ...   \n",
       "10      Me personally I am not a big fan of Pearl Jam ...   \n",
       "11      Katchen's performace throughout this collectio...   \n",
       "16      Wow, I had this in the late 60s, I saw Deaf Ge...   \n",
       "...                                                   ...   \n",
       "199983  Those old crooners like Hartman really had a w...   \n",
       "199986                                            Love it   \n",
       "199987  Love priest, halford one of the best live show...   \n",
       "199988                                              Great   \n",
       "199995  With this, Mariah's third album, Mariah proved...   \n",
       "\n",
       "                                     summary  unixReviewTime  \\\n",
       "2        Love the Music, Hate the Light Show      1444694400   \n",
       "3                                      Great      1498608000   \n",
       "10                         Hmmmmm...........      1017792000   \n",
       "11         Superb interpretations by Katchen      1163635200   \n",
       "16        Hello Darkness my old friend......      1474243200   \n",
       "...                                      ...             ...   \n",
       "199983  Hartman and Wine Get Better With Age      1486598400   \n",
       "199986                            Five Stars      1446940800   \n",
       "199987                            Remaster ?      1395532800   \n",
       "199988                            Five Stars      1413763200   \n",
       "199995       Well Done Mariah! You Show 'Em!      1083369600   \n",
       "\n",
       "                category   price     itemID reviewHash  \n",
       "2                    Pop  $89.86  p82172532   24751194  \n",
       "3                    Pop  $11.89  p15255251   22820631  \n",
       "10                   Pop   $6.81  p99659606   41124728  \n",
       "11             Classical  $31.04  p63362921   40704096  \n",
       "16                   Pop   $5.89  p72166335   75947625  \n",
       "...                  ...     ...        ...        ...  \n",
       "199983               Pop   $1.98  p33434439   24285962  \n",
       "199986               Pop  $11.88  p02978017   27368058  \n",
       "199987  Alternative Rock  $11.89  p75893713   97935046  \n",
       "199988  Alternative Rock  $18.98  p47415569   75099110  \n",
       "199995               Pop   $7.98  p84118731   35077372  \n",
       "\n",
       "[50001 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = data_df['category'].unique()\n",
    "dfs = []\n",
    "for category in categories:\n",
    "    dfs.append(data_df[data_df['category'] == category].sample(frac=0.25))\n",
    "data_df = pd.concat(dfs, axis=0)\n",
    "data_df = data_df.sort_index()\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing\n",
    "\n",
    "We apply feature cleaning as prototyped before and then split into a training and validation set, ensuring that the proportion of data points in training vs validation is consistent for each music category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_price(price):\n",
    "    \"\"\"Trims `price` to remove the $ sign.\n",
    "    \n",
    "    If the price variable does not have the format $x.xx\n",
    "    then the empty string is returned.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    price: str\n",
    "        A string representing a price.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A string representing `price` but with the $ sign removed,\n",
    "        or the empty string if `price` does not have the correct\n",
    "        format.\n",
    "    \n",
    "    \"\"\"\n",
    "    if (not pd.isnull(price) and isinstance(price, str) and\n",
    "        len(price) > 0 and price[0] == '$'):\n",
    "        return price[1:]\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-d3ba8e2e1b50>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_df['cleanedPrice'] = data_df['cleanedPrice'].astype('float')\n",
      "<ipython-input-7-d3ba8e2e1b50>:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_df['fixedReviewText'] = np.where(pd.isnull(data_df['reviewText']), \"\", data_df['reviewText'])\n",
      "<ipython-input-7-d3ba8e2e1b50>:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_df['fixedSummary'] = np.where(pd.isnull(data_df['summary']), \"\", data_df['summary'])\n",
      "<ipython-input-7-d3ba8e2e1b50>:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_df['fullReviewText'] = data_df['fixedSummary'] + \" \" + data_df['fixedReviewText']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>category</th>\n",
       "      <th>price</th>\n",
       "      <th>itemID</th>\n",
       "      <th>reviewHash</th>\n",
       "      <th>...</th>\n",
       "      <th>reviewHour</th>\n",
       "      <th>reviewMonthYear</th>\n",
       "      <th>cleanedPrice</th>\n",
       "      <th>fullReviewText</th>\n",
       "      <th>isPop</th>\n",
       "      <th>isClassical</th>\n",
       "      <th>isAlternativeRock</th>\n",
       "      <th>isJazz</th>\n",
       "      <th>isDanceElectronic</th>\n",
       "      <th>reviewWordCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>10 13, 2015</td>\n",
       "      <td>u92735614</td>\n",
       "      <td>I REALLY enjoy this pairing of Anderson and Po...</td>\n",
       "      <td>Love the Music, Hate the Light Show</td>\n",
       "      <td>1444694400</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$89.86</td>\n",
       "      <td>p82172532</td>\n",
       "      <td>24751194</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>2015-10</td>\n",
       "      <td>89.86</td>\n",
       "      <td>Love the Music, Hate the Light Show I REALLY e...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>06 28, 2017</td>\n",
       "      <td>u35112935</td>\n",
       "      <td>Finally got it . It was everything thought it ...</td>\n",
       "      <td>Great</td>\n",
       "      <td>1498608000</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$11.89</td>\n",
       "      <td>p15255251</td>\n",
       "      <td>22820631</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>2017-06</td>\n",
       "      <td>11.89</td>\n",
       "      <td>Great Finally got it . It was everything thoug...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.0</td>\n",
       "      <td>04 3, 2002</td>\n",
       "      <td>u25030850</td>\n",
       "      <td>Me personally I am not a big fan of Pearl Jam ...</td>\n",
       "      <td>Hmmmmm...........</td>\n",
       "      <td>1017792000</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$6.81</td>\n",
       "      <td>p99659606</td>\n",
       "      <td>41124728</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>2002-04</td>\n",
       "      <td>6.81</td>\n",
       "      <td>Hmmmmm........... Me personally I am not a big...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.0</td>\n",
       "      <td>11 16, 2006</td>\n",
       "      <td>u40719083</td>\n",
       "      <td>Katchen's performace throughout this collectio...</td>\n",
       "      <td>Superb interpretations by Katchen</td>\n",
       "      <td>1163635200</td>\n",
       "      <td>Classical</td>\n",
       "      <td>$31.04</td>\n",
       "      <td>p63362921</td>\n",
       "      <td>40704096</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>2006-11</td>\n",
       "      <td>31.04</td>\n",
       "      <td>Superb interpretations by Katchen Katchen's pe...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.0</td>\n",
       "      <td>09 19, 2016</td>\n",
       "      <td>u55172429</td>\n",
       "      <td>Wow, I had this in the late 60s, I saw Deaf Ge...</td>\n",
       "      <td>Hello Darkness my old friend......</td>\n",
       "      <td>1474243200</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$5.89</td>\n",
       "      <td>p72166335</td>\n",
       "      <td>75947625</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>2016-09</td>\n",
       "      <td>5.89</td>\n",
       "      <td>Hello Darkness my old friend...... Wow, I had ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199983</th>\n",
       "      <td>5.0</td>\n",
       "      <td>02 9, 2017</td>\n",
       "      <td>u57558427</td>\n",
       "      <td>Those old crooners like Hartman really had a w...</td>\n",
       "      <td>Hartman and Wine Get Better With Age</td>\n",
       "      <td>1486598400</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$1.98</td>\n",
       "      <td>p33434439</td>\n",
       "      <td>24285962</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>2017-02</td>\n",
       "      <td>1.98</td>\n",
       "      <td>Hartman and Wine Get Better With Age Those old...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199986</th>\n",
       "      <td>5.0</td>\n",
       "      <td>11 8, 2015</td>\n",
       "      <td>u85136324</td>\n",
       "      <td>Love it</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1446940800</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$11.88</td>\n",
       "      <td>p02978017</td>\n",
       "      <td>27368058</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>2015-11</td>\n",
       "      <td>11.88</td>\n",
       "      <td>Five Stars Love it</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199987</th>\n",
       "      <td>4.0</td>\n",
       "      <td>03 23, 2014</td>\n",
       "      <td>u32710934</td>\n",
       "      <td>Love priest, halford one of the best live show...</td>\n",
       "      <td>Remaster ?</td>\n",
       "      <td>1395532800</td>\n",
       "      <td>Alternative Rock</td>\n",
       "      <td>$11.89</td>\n",
       "      <td>p75893713</td>\n",
       "      <td>97935046</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>2014-03</td>\n",
       "      <td>11.89</td>\n",
       "      <td>Remaster ? Love priest, halford one of the bes...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199988</th>\n",
       "      <td>5.0</td>\n",
       "      <td>10 20, 2014</td>\n",
       "      <td>u37750462</td>\n",
       "      <td>Great</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1413763200</td>\n",
       "      <td>Alternative Rock</td>\n",
       "      <td>$18.98</td>\n",
       "      <td>p47415569</td>\n",
       "      <td>75099110</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>2014-10</td>\n",
       "      <td>18.98</td>\n",
       "      <td>Five Stars Great</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>4.0</td>\n",
       "      <td>05 1, 2004</td>\n",
       "      <td>u68902609</td>\n",
       "      <td>With this, Mariah's third album, Mariah proved...</td>\n",
       "      <td>Well Done Mariah! You Show 'Em!</td>\n",
       "      <td>1083369600</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$7.98</td>\n",
       "      <td>p84118731</td>\n",
       "      <td>35077372</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>2004-05</td>\n",
       "      <td>7.98</td>\n",
       "      <td>Well Done Mariah! You Show 'Em! With this, Mar...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49340 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        overall   reviewTime reviewerID  \\\n",
       "2           4.0  10 13, 2015  u92735614   \n",
       "3           5.0  06 28, 2017  u35112935   \n",
       "10          3.0   04 3, 2002  u25030850   \n",
       "11          5.0  11 16, 2006  u40719083   \n",
       "16          5.0  09 19, 2016  u55172429   \n",
       "...         ...          ...        ...   \n",
       "199983      5.0   02 9, 2017  u57558427   \n",
       "199986      5.0   11 8, 2015  u85136324   \n",
       "199987      4.0  03 23, 2014  u32710934   \n",
       "199988      5.0  10 20, 2014  u37750462   \n",
       "199995      4.0   05 1, 2004  u68902609   \n",
       "\n",
       "                                               reviewText  \\\n",
       "2       I REALLY enjoy this pairing of Anderson and Po...   \n",
       "3       Finally got it . It was everything thought it ...   \n",
       "10      Me personally I am not a big fan of Pearl Jam ...   \n",
       "11      Katchen's performace throughout this collectio...   \n",
       "16      Wow, I had this in the late 60s, I saw Deaf Ge...   \n",
       "...                                                   ...   \n",
       "199983  Those old crooners like Hartman really had a w...   \n",
       "199986                                            Love it   \n",
       "199987  Love priest, halford one of the best live show...   \n",
       "199988                                              Great   \n",
       "199995  With this, Mariah's third album, Mariah proved...   \n",
       "\n",
       "                                     summary  unixReviewTime  \\\n",
       "2        Love the Music, Hate the Light Show      1444694400   \n",
       "3                                      Great      1498608000   \n",
       "10                         Hmmmmm...........      1017792000   \n",
       "11         Superb interpretations by Katchen      1163635200   \n",
       "16        Hello Darkness my old friend......      1474243200   \n",
       "...                                      ...             ...   \n",
       "199983  Hartman and Wine Get Better With Age      1486598400   \n",
       "199986                            Five Stars      1446940800   \n",
       "199987                            Remaster ?      1395532800   \n",
       "199988                            Five Stars      1413763200   \n",
       "199995       Well Done Mariah! You Show 'Em!      1083369600   \n",
       "\n",
       "                category   price     itemID reviewHash  ... reviewHour  \\\n",
       "2                    Pop  $89.86  p82172532   24751194  ...         20   \n",
       "3                    Pop  $11.89  p15255251   22820631  ...         20   \n",
       "10                   Pop   $6.81  p99659606   41124728  ...         19   \n",
       "11             Classical  $31.04  p63362921   40704096  ...         19   \n",
       "16                   Pop   $5.89  p72166335   75947625  ...         20   \n",
       "...                  ...     ...        ...        ...  ...        ...   \n",
       "199983               Pop   $1.98  p33434439   24285962  ...         19   \n",
       "199986               Pop  $11.88  p02978017   27368058  ...         19   \n",
       "199987  Alternative Rock  $11.89  p75893713   97935046  ...         20   \n",
       "199988  Alternative Rock  $18.98  p47415569   75099110  ...         20   \n",
       "199995               Pop   $7.98  p84118731   35077372  ...         20   \n",
       "\n",
       "       reviewMonthYear  cleanedPrice  \\\n",
       "2              2015-10         89.86   \n",
       "3              2017-06         11.89   \n",
       "10             2002-04          6.81   \n",
       "11             2006-11         31.04   \n",
       "16             2016-09          5.89   \n",
       "...                ...           ...   \n",
       "199983         2017-02          1.98   \n",
       "199986         2015-11         11.88   \n",
       "199987         2014-03         11.89   \n",
       "199988         2014-10         18.98   \n",
       "199995         2004-05          7.98   \n",
       "\n",
       "                                           fullReviewText  isPop isClassical  \\\n",
       "2       Love the Music, Hate the Light Show I REALLY e...      1           0   \n",
       "3       Great Finally got it . It was everything thoug...      1           0   \n",
       "10      Hmmmmm........... Me personally I am not a big...      1           0   \n",
       "11      Superb interpretations by Katchen Katchen's pe...      0           1   \n",
       "16      Hello Darkness my old friend...... Wow, I had ...      1           0   \n",
       "...                                                   ...    ...         ...   \n",
       "199983  Hartman and Wine Get Better With Age Those old...      1           0   \n",
       "199986                                 Five Stars Love it      1           0   \n",
       "199987  Remaster ? Love priest, halford one of the bes...      0           0   \n",
       "199988                                   Five Stars Great      0           0   \n",
       "199995  Well Done Mariah! You Show 'Em! With this, Mar...      1           0   \n",
       "\n",
       "        isAlternativeRock  isJazz  isDanceElectronic  reviewWordCount  \n",
       "2                       0       0                  0              133  \n",
       "3                       0       0                  0               15  \n",
       "10                      0       0                  0               77  \n",
       "11                      0       0                  0              226  \n",
       "16                      0       0                  0               84  \n",
       "...                   ...     ...                ...              ...  \n",
       "199983                  0       0                  0               25  \n",
       "199986                  0       0                  0                4  \n",
       "199987                  1       0                  0               22  \n",
       "199988                  1       0                  0                3  \n",
       "199995                  0       0                  0              172  \n",
       "\n",
       "[49340 rows x 22 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "data_df['reviewMonth'] = data_df['reviewTime'].apply(lambda x: x.split(' ')[0])\n",
    "data_df['reviewYear'] = data_df['reviewTime'].apply(lambda x: x.split(' ')[2])\n",
    "data_df['reviewHour'] = data_df['unixReviewTime'].apply(lambda x: datetime.fromtimestamp(x).hour)\n",
    "data_df['reviewMonthYear'] = data_df['reviewYear'] + '-' + data_df['reviewMonth']\n",
    "\n",
    "data_df['cleanedPrice'] = data_df['price'].apply(lambda x: trim_price(x))\n",
    "data_df = data_df[data_df['cleanedPrice'] != \"\"]\n",
    "data_df['cleanedPrice'] = data_df['cleanedPrice'].astype('float')\n",
    "\n",
    "data_df['fixedReviewText'] = np.where(pd.isnull(data_df['reviewText']), \"\", data_df['reviewText'])\n",
    "data_df['fixedSummary'] = np.where(pd.isnull(data_df['summary']), \"\", data_df['summary'])\n",
    "data_df['fullReviewText'] = data_df['fixedSummary'] + \" \" + data_df['fixedReviewText']\n",
    "\n",
    "data_df = data_df.drop(columns=['fixedReviewText', 'fixedSummary'])\n",
    "\n",
    "genres = data_df['category'].unique()\n",
    "\n",
    "for genre in genres:\n",
    "    genre_col = \"is\" + genre.replace(\" \", \"\").replace(\"&\", \"\")\n",
    "    data_df[genre_col] = data_df['category'].apply(lambda x: 1 if x == genre else 0)\n",
    "\n",
    "data_df['reviewWordCount'] = data_df['fullReviewText'].apply(lambda x: len(x.split()))\n",
    "\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_MSE(actuals, predicteds):\n",
    "    \"\"\"Calculates the Mean Squared Error between `actuals` and `predicteds`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    actuals: np.array\n",
    "        A numpy array of the actual values.\n",
    "    predicteds: np.array\n",
    "        A numpy array of the predicted values.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        A float representing the Mean Squared Error between `actuals` and\n",
    "        `predicteds`.\n",
    "    \n",
    "    \"\"\"\n",
    "    return (((actuals - predicteds)**2).sum()) / (len(actuals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "genres = data_df['category'].unique()\n",
    "X_train_set = []\n",
    "X_val_set = []\n",
    "y_train_set = []\n",
    "y_val_set = []\n",
    "\n",
    "for genre in genres:\n",
    "    genre_df = data_df[data_df['category'] == genre]\n",
    "    targets = genre_df['overall']\n",
    "    feature_data = genre_df.drop(columns=['overall'])\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        feature_data, targets, shuffle=True, test_size=0.2, random_state=17)\n",
    "    X_train_set.append(X_train)\n",
    "    X_val_set.append(X_val)\n",
    "    y_train_set.append(y_train)\n",
    "    y_val_set.append(y_val)\n",
    "\n",
    "X_train = pd.concat(X_train_set)\n",
    "X_val = pd.concat(X_val_set)\n",
    "y_train = pd.concat(y_train_set)\n",
    "y_val = pd.concat(y_val_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative Filtering\n",
    "\n",
    "In this model we only need a users ID, the items ID, and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_item_matrix(df, rating_col, user_col, item_col):\n",
    "    return sp.csr_matrix(df[rating_col], (df[user_col], df[item_col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.concat([X_train, pd.DataFrame(y_train, columns=['overall'])], axis=1)\n",
    "val_data = pd.concat([X_val, pd.DataFrame(y_val, columns=['overall'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "item_matrix = train_data.pivot(index='itemID', columns='reviewerID', values='overall')\n",
    "item_matrix = item_matrix.fillna(0)\n",
    "user_item_train_matrix = sp.csr_matrix(item_matrix.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_average = train_data['overall'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "model_knn = NearestNeighbors(metric='cosine', algorithm='brute', n_neighbors=5)\n",
    "model_knn.fit(user_item_train_matrix)\n",
    "item_neighbors = np.asarray(model_knn.kneighbors(user_item_train_matrix, return_distance=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_matrix = train_data.pivot(index='reviewerID', columns='itemID', values='overall')\n",
    "user_matrix = user_matrix.fillna(0)\n",
    "user_item_train_matrix = sp.csr_matrix(user_matrix.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_knn = NearestNeighbors(metric='cosine', algorithm='brute', n_neighbors=5)\n",
    "model_knn.fit(user_item_train_matrix)\n",
    "user_neighbors = np.asarray(model_knn.kneighbors(user_item_train_matrix, return_distance=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user_avg = train_data.groupby(train_data['reviewerID'], as_index=False)['overall'].mean()\n",
    "train_item_avg = train_data.groupby(train_data['itemID'], as_index=False)['overall'].mean()\n",
    "train_user_avg.columns = ['reviewerID', 'userAverage']\n",
    "train_item_avg.columns = ['itemID', 'itemAverage']\n",
    "train_user_avg = train_user_avg.set_index('reviewerID')\n",
    "train_item_avg = train_item_avg.set_index('itemID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_avgs = []\n",
    "for i in range(len(item_neighbors)):\n",
    "    item_avgs.append(train_item_avg['itemAverage'][item_matrix.index[item_neighbors[i]]].mean())\n",
    "\n",
    "item_avgs = pd.concat([pd.DataFrame(item_matrix.index, columns=['itemID']), pd.DataFrame(item_avgs, columns=['itemRating'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_avgs = []\n",
    "for i in range(len(user_neighbors)):\n",
    "    user_avgs.append(train_user_avg['userAverage'][user_matrix.index[user_neighbors[i]]].mean())\n",
    "\n",
    "user_avgs = pd.concat([pd.DataFrame(user_matrix.index, columns=['reviewerID']), pd.DataFrame(user_avgs, columns=['userRating'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average_data(X, total_avg, user_avgs, item_avgs):\n",
    "    \"\"\"Calculates the error based on the weighted average prediction.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: pd.DataFrame\n",
    "        The DataFrame of features.\n",
    "    y: np.array\n",
    "        A numpy array containing the targets\n",
    "    total_avg: float\n",
    "        The average across all users/items.\n",
    "    user_avgs: pd.DataFrame\n",
    "        A DataFrame containing the average rating for each user.\n",
    "    item_avgs: pd.DataFrame\n",
    "        A DataFrame containing the average rating for each item.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        A float representing the mean squared error of the predictions.\n",
    "    \n",
    "    \"\"\"\n",
    "    df_user = pd.merge(X, user_avgs, how='left', on=['reviewerID'])\n",
    "    df_final = pd.merge(df_user, item_avgs, how='left', on=['itemID'])\n",
    "    df_final = df_final[['userRating', 'itemRating']]\n",
    "    df_final = df_final.fillna(total_avg)\n",
    "    df_final.index = X.index\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_aug = weighted_average_data(X_train, global_average, user_avgs, item_avgs)\n",
    "X_val_aug = weighted_average_data(X_val, global_average, user_avgs, item_avgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mod = pd.concat([X_train, X_train_aug], axis=1)\n",
    "X_val_mod = pd.concat([X_val, X_val_aug], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_rating(rating):\n",
    "    \"\"\"Thresholds `rating` to lie in the range [1, 5].\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rating: float\n",
    "        The rating to be thresholded.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        A float representing the thresholded rating.\n",
    "    \n",
    "    \"\"\"\n",
    "    if rating < 1:\n",
    "        return 1\n",
    "    if rating > 5:\n",
    "        return 5\n",
    "    return rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.8036093923598844\n",
      "Validation MSE: 1.011061051627551\n"
     ]
    }
   ],
   "source": [
    "X_train_mod['pred'] = (0.5 * X_train_mod['userRating']) + (0.5 * X_train_mod['itemRating'])\n",
    "X_train_mod['pred'].apply(lambda x: threshold_rating(x))\n",
    "print(\"Training MSE: {}\".format(calculate_MSE(y_train, X_train_mod['pred'])))\n",
    "\n",
    "X_val_mod['pred'] = (0.5 * X_val_mod['userRating']) + (0.5 * X_val_mod['itemRating'])\n",
    "X_val_mod['pred'].apply(lambda x: threshold_rating(x))\n",
    "print(\"Validation MSE: {}\".format(calculate_MSE(y_val, X_val_mod['pred'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['cleanedPrice', 'isPop', 'isAlternativeRock', 'isJazz', 'isClassical', 'isDanceElectronic', 'reviewWordCount']\n",
    "X_train_reg1 = X_train[columns_to_keep]\n",
    "X_val_reg1 = X_val[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-2b5d7d7878ed>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train_reg1['reviewWordCount'] = X_train_reg1['reviewWordCount'].apply(lambda x: np.log(x))\n",
      "<ipython-input-26-2b5d7d7878ed>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val_reg1['reviewWordCount'] = X_val_reg1['reviewWordCount'].apply(lambda x: np.log(x))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_train_reg1['reviewWordCount'] = X_train_reg1['reviewWordCount'].apply(lambda x: np.log(x))\n",
    "X_val_reg1['reviewWordCount'] = X_val_reg1['reviewWordCount'].apply(lambda x: np.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-550e8da2fe0c>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.dropna(inplace=True)\n",
      "<ipython-input-27-550e8da2fe0c>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.dropna(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "def clean_dataset(df):\n",
    "    df.dropna(inplace=True)\n",
    "    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "    return df[indices_to_keep].astype(np.float64)\n",
    "\n",
    "X_train_reg1 = clean_dataset(X_train_reg1)\n",
    "y_train1 = y_train[y_train.index.isin(X_train_reg1.index)]\n",
    "X_train1 = X_train[X_train.index.isin(X_train_reg1.index)]\n",
    "\n",
    "X_val_reg1 = clean_dataset(X_val_reg1)\n",
    "y_val1 = y_val[y_val.index.isin(X_val_reg1.index)]\n",
    "X_val1 = X_val[X_val.index.isin(X_val_reg1.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mod = X_train_mod[X_train_mod.index.isin(X_train_reg1.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reg1 = min_max_scaler.fit_transform(X_train_reg1)\n",
    "X_val_reg1 = min_max_scaler.transform(X_val_reg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha = 0.0\n",
      "------------\n",
      "Training Error: 0.9528565786877277\n",
      "Validation Error: 0.9782989806075709\n",
      "\n",
      "Alpha = 0.01\n",
      "------------\n",
      "Training Error: 0.9527874581451989\n",
      "Validation Error: 0.9780706174350472\n",
      "\n",
      "Alpha = 0.03\n",
      "------------\n",
      "Training Error: 0.9527879364316555\n",
      "Validation Error: 0.9780715543169359\n",
      "\n",
      "Alpha = 0.1\n",
      "------------\n",
      "Training Error: 0.9527896104574072\n",
      "Validation Error: 0.9780748237435576\n",
      "\n",
      "Alpha = 0.3\n",
      "------------\n",
      "Training Error: 0.9527943917923807\n",
      "Validation Error: 0.9780840821773407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "vthreshold_rating = np.vectorize(threshold_rating)\n",
    "\n",
    "alphas = [0.0, 0.01, 0.03, 0.1, 0.3]\n",
    "for alpha in alphas:\n",
    "    print(\"Alpha = {}\".format(alpha))\n",
    "    print(\"------------\")\n",
    "    reg_model = Ridge(alpha=alpha)\n",
    "    reg_model.fit(X_train_reg1, y_train1)\n",
    "    print(\"Training Error: {}\".format(calculate_MSE(y_train1, vthreshold_rating(reg_model.predict(X_train_reg1)))))\n",
    "    print(\"Validation Error: {}\".format(calculate_MSE(y_val1, vthreshold_rating(reg_model.predict(X_val_reg1)))))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Matthew/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def process_review_text(review_text, exclude_text, ps):\n",
    "    \"\"\"Pre-processes the text given by `review_text`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    review_text: str\n",
    "        The review text to be processed.\n",
    "    exclude_text: collection\n",
    "        A collection of words to be excluded.\n",
    "    ps: PorterStemmer\n",
    "        The PorterStemmer used to perform word stemming.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A string representing the processed version of `review_text`.\n",
    "    \n",
    "    \"\"\"\n",
    "    review = re.sub('[^a-zA-Z0-9]', ' ', review_text).lower().split()\n",
    "    review = [ps.stem(word) for word in review if not word in exclude_text]\n",
    "    return ' '.join(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-33-0d22f415ca83>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train1['processedReview'] = X_train1['fullReviewText'].apply(lambda x: process_review_text(x, exclude_english, ps))\n"
     ]
    }
   ],
   "source": [
    "exclude_english = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "X_train1['processedReview'] = X_train1['fullReviewText'].apply(lambda x: process_review_text(x, exclude_english, ps))\n",
    "X_val1['processedReview'] = X_val1['fullReviewText'].apply(lambda x: process_review_text(x, exclude_english, ps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=1500)\n",
    "X_train_cv1 = cv.fit_transform(X_train1['processedReview'])\n",
    "X_val_cv1 = cv.transform(X_val1['processedReview'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "X_train_reg1_sp = sp.csr_matrix(X_train_reg1)\n",
    "X_train_cv_reg1 = sp.hstack((X_train_cv1, X_train_reg1_sp), format='csr')\n",
    "\n",
    "X_val_reg1_sp = sp.csr_matrix(X_val_reg1)\n",
    "X_val_cv_reg1 = sp.hstack((X_val_cv1, X_val_reg1_sp), format='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.01, # Estimators: 10, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 12.593585326307256\n",
      "Validation Error: 12.67473662884927\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 10, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 12.593585326307256\n",
      "Validation Error: 12.67473662884927\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 10, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 12.593585326307256\n",
      "Validation Error: 12.67473662884927\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 10, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 12.593585326307256\n",
      "Validation Error: 12.67473662884927\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 6.550438196243737\n",
      "Validation Error: 6.5880611764144055\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 6.52725632182597\n",
      "Validation Error: 6.5632085015484405\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 6.474950815812452\n",
      "Validation Error: 6.515235130242674\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 6.4049085627239455\n",
      "Validation Error: 6.477203041090882\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 2.9909864425056485\n",
      "Validation Error: 2.997231572241453\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 2.9564379683192237\n",
      "Validation Error: 2.9637166045040493\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 2.873040358066219\n",
      "Validation Error: 2.8916097337214017\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 2.7534970883270704\n",
      "Validation Error: 2.830574463811401\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 1.1937534112945924\n",
      "Validation Error: 1.1718490640807513\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 1.1441831520686658\n",
      "Validation Error: 1.1263715481680487\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 1.0252407078650987\n",
      "Validation Error: 1.0321461501430618\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.852664873225294\n",
      "Validation Error: 0.963022979505327\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8703233418106068\n",
      "Validation Error: 0.8367534444691025\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7947228009449323\n",
      "Validation Error: 0.7710466290913105\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6360947270578214\n",
      "Validation Error: 0.6642658509658224\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4161352145181226\n",
      "Validation Error: 0.611486775855196\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 10, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 9.271438544936347\n",
      "Validation Error: 9.326933996866611\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 10, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 9.255338634619985\n",
      "Validation Error: 9.308521049507627\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 10, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 9.218447255178416\n",
      "Validation Error: 9.275306293649876\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 10, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 9.170216935333631\n",
      "Validation Error: 9.246704046420033\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 1.6570505918247893\n",
      "Validation Error: 1.6454092509129405\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 1.615126670679494\n",
      "Validation Error: 1.6058904579429243\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 1.509834054256618\n",
      "Validation Error: 1.5196953278003371\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 1.356986368033661\n",
      "Validation Error: 1.4492351294798704\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.9341432160528668\n",
      "Validation Error: 0.9026902615825739\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8727823901413795\n",
      "Validation Error: 0.8484195301581637\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7330257732508165\n",
      "Validation Error: 0.7446059868470096\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5368512274330656\n",
      "Validation Error: 0.6788133793977833\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8565662159617881\n",
      "Validation Error: 0.8240333250046581\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.776987463511858\n",
      "Validation Error: 0.7550957086739946\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6123600752696048\n",
      "Validation Error: 0.6492103408307265\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.38246395708167485\n",
      "Validation Error: 0.6017717985809833\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7858319123583294\n",
      "Validation Error: 0.7604087049139929\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6917749175550089\n",
      "Validation Error: 0.6841563291014993\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5082321401658175\n",
      "Validation Error: 0.6010547539084455\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.23993775201305986\n",
      "Validation Error: 0.578250682220027\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 10, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 2.800249774712073\n",
      "Validation Error: 2.8040603021238137\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 10, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 2.764864956515802\n",
      "Validation Error: 2.7700744000737845\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 10, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 2.681838410200421\n",
      "Validation Error: 2.6997213323235636\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 10, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 2.5619665378306773\n",
      "Validation Error: 2.639599636206951\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.867148493248728\n",
      "Validation Error: 0.8336839756217429\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.791391856772724\n",
      "Validation Error: 0.7694129941430407\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6332699947423606\n",
      "Validation Error: 0.6620020255147467\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.41317734951233004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: 0.6136955626194984\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8155976600043131\n",
      "Validation Error: 0.7875032466405151\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.726521861767224\n",
      "Validation Error: 0.7121344775180954\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5527333729149018\n",
      "Validation Error: 0.6203616906595741\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.30108368671816804\n",
      "Validation Error: 0.5902296716747013\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7603381182947971\n",
      "Validation Error: 0.7376392402745808\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6648155092944541\n",
      "Validation Error: 0.6638582645749459\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.47396974475635406\n",
      "Validation Error: 0.5945472381691608\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.20543200991565497\n",
      "Validation Error: 0.5827043504658799\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6971206480310448\n",
      "Validation Error: 0.6846994953320112\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.60007015475517\n",
      "Validation Error: 0.6218760045905573\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.3654822775933115\n",
      "Validation Error: 0.5757436571266696\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.10483945766867236\n",
      "Validation Error: 0.582446464864197\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 10, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.90630787055686\n",
      "Validation Error: 0.8703424341976994\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 10, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8470989262277072\n",
      "Validation Error: 0.8191118046829554\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 10, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7106758839756667\n",
      "Validation Error: 0.7186487333999634\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 10, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5164023832497643\n",
      "Validation Error: 0.6749275315829953\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7746449490504231\n",
      "Validation Error: 0.7514757489764072\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6847754627555952\n",
      "Validation Error: 0.6779093578458835\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5131316034291978\n",
      "Validation Error: 0.6102139158017111\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.25519782595256785\n",
      "Validation Error: 0.6173926426621129\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7228255305273165\n",
      "Validation Error: 0.7061105834087219\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.631835413366388\n",
      "Validation Error: 0.6402295038736551\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.43232484033800106\n",
      "Validation Error: 0.5940919488580755\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.16402267913303564\n",
      "Validation Error: 0.6226955755786499\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.681322026788915\n",
      "Validation Error: 0.6714017452417257\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5841600554522332\n",
      "Validation Error: 0.6142663970560962\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.35062358909827873\n",
      "Validation Error: 0.586114673051449\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.08483914548700379\n",
      "Validation Error: 0.630589504293489\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6398188580529599\n",
      "Validation Error: 0.6442813681258396\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5210280146836813\n",
      "Validation Error: 0.5889768598483542\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.23684211930430282\n",
      "Validation Error: 0.5936169865564452\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.0175730486042508\n",
      "Validation Error: 0.6424307892035772\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 10, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8614753955445565\n",
      "Validation Error: 0.8263920827448844\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 10, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7868724873501688\n",
      "Validation Error: 0.761233734104035\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 10, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6357116330975928\n",
      "Validation Error: 0.6629179293582605\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 10, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4359348830501427\n",
      "Validation Error: 0.6642361834451783\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7272273248859753\n",
      "Validation Error: 0.7102072582689162\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6435634129757966\n",
      "Validation Error: 0.6490353286822046\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4602870744958097\n",
      "Validation Error: 0.616673934157829\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.19232066209548598\n",
      "Validation Error: 0.6739816465717698\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6854570175303519\n",
      "Validation Error: 0.674423587059751\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5999071526738016\n",
      "Validation Error: 0.6237167805395513\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.3808621059591676\n",
      "Validation Error: 0.6152528650488003\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.09856997710601134\n",
      "Validation Error: 0.6927743763550213\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6523956387854117\n",
      "Validation Error: 0.6510570146909723\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5564683130931524\n",
      "Validation Error: 0.6081003620355961\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.2924400527963914\n",
      "Validation Error: 0.6209486484816141\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error: 0.04102141880108724\n",
      "Validation Error: 0.7051522031851937\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6162994432119324\n",
      "Validation Error: 0.6326113331540619\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4906136669008255\n",
      "Validation Error: 0.5963565134729887\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.17578364274699398\n",
      "Validation Error: 0.6432141219299883\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.0030042775739243374\n",
      "Validation Error: 0.7145314862685995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "learning_rates = [0.01, 0.03, 0.1, 0.3, 0.5]\n",
    "estimators = [10, 50, 100, 200, 500]\n",
    "depths = [1, 2, 5, 10]\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for estimator in estimators:\n",
    "        for depth in depths:\n",
    "            print(\"Learning Rate: {0}, # Estimators: {1}, Depth: {2}\".format(learning_rate, estimator, depth))\n",
    "            print(\"--------------------------------------------------\")\n",
    "            xg_reg = XGBRegressor(\n",
    "                learning_rate=learning_rate, max_depth=depth, n_estimators=estimator)\n",
    "            xg_reg.fit(X_train_cv_reg1, y_train1)\n",
    "            print(\"Training Error: {}\".format(calculate_MSE(y_train1, vthreshold_rating(xg_reg.predict(X_train_cv_reg1)))))\n",
    "            print(\"Validation Error: {}\".format(calculate_MSE(y_val1, vthreshold_rating(xg_reg.predict(X_val_cv_reg1)))))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that `learning_rate=0.3`, `n_estimators=500`, and `max_depth=2` provides a very good model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'XGBRegressor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-2f965e68c39b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxg_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mxg_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_cv_reg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_MSE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_MSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvthreshold_rating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxg_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_cv_reg1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mval_MSE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_MSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvthreshold_rating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxg_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_cv_reg1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'XGBRegressor' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "xg_reg = XGBRegressor(learning_rate=0.3, n_estimators=500, max_depth=2)\n",
    "xg_reg.fit(X_train_cv_reg1, y_train1)\n",
    "\n",
    "train_MSE = calculate_MSE(y_train1, vthreshold_rating(xg_reg.predict(X_train_cv_reg1)))\n",
    "val_MSE = calculate_MSE(y_val1, vthreshold_rating(xg_reg.predict(X_val_cv_reg1)))\n",
    "\n",
    "print(\"Training error based on XGBoost CountVectorizer prediction: %.3f\" % train_MSE)\n",
    "print(\"Validation error based on XGBoost CountVectorizer prediction: %.3f\" % val_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta Models\n",
    "\n",
    "In this section we look at models that combine both collaborative filtering and language models.\n",
    "\n",
    "We start by using the predictions of the collaborative filtering as features in our language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['cleanedPrice', 'isPop', 'isAlternativeRock', 'isJazz', 'isClassical', 'isDanceElectronic', 'reviewWordCount', 'userRating', 'itemRating']\n",
    "X_train_reg2 = X_train_mod[columns_to_keep]\n",
    "X_val_reg2 = X_val_mod[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = MinMaxScaler()\n",
    "X_train_reg2['reviewWordCount'] = X_train_reg2['reviewWordCount'].apply(lambda x: np.log(x))\n",
    "X_val_reg2['reviewWordCount'] = X_val_reg2['reviewWordCount'].apply(lambda x: np.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reg2 = clean_dataset(X_train_reg2)\n",
    "y_train2 = y_train[y_train.index.isin(X_train_reg2.index)]\n",
    "X_train2 = X_train[X_train.index.isin(X_train_reg2.index)]\n",
    "\n",
    "X_val_reg2 = clean_dataset(X_val_reg2)\n",
    "y_val2 = y_val[y_val.index.isin(X_val_reg2.index)]\n",
    "X_val2 = X_val[X_val.index.isin(X_val_reg2.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reg2 = min_max_scaler.fit_transform(X_train_reg2)\n",
    "X_val_reg2 = min_max_scaler.transform(X_val_reg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.0, 0.01, 0.03, 0.1, 0.3]\n",
    "for alpha in alphas:\n",
    "    print(\"Alpha = {}\".format(alpha))\n",
    "    print(\"------------\")\n",
    "    reg_model = Ridge(alpha=alpha)\n",
    "    reg_model.fit(X_train_reg2, y_train2)\n",
    "    print(\"Training Error: {}\".format(calculate_MSE(y_train2, vthreshold_rating(reg_model.predict(X_train_reg2)))))\n",
    "    print(\"Validation Error: {}\".format(calculate_MSE(y_val2, vthreshold_rating(reg_model.predict(X_val_reg2)))))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_english = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "X_train2['processedReview'] = X_train2['fullReviewText'].apply(lambda x: process_review_text(x, exclude_english, ps))\n",
    "X_val2['processedReview'] = X_val2['fullReviewText'].apply(lambda x: process_review_text(x, exclude_english, ps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_features=1500)\n",
    "X_train_cv2 = cv.fit_transform(X_train2['processedReview'])\n",
    "X_val_cv2 = cv.transform(X_val2['processedReview'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reg2_sp = sp.csr_matrix(X_train_reg2)\n",
    "X_train_cv_reg2 = sp.hstack((X_train_cv2, X_train_reg2_sp), format='csr')\n",
    "\n",
    "X_val_reg2_sp = sp.csr_matrix(X_val_reg2)\n",
    "X_val_cv_reg2 = sp.hstack((X_val_cv2, X_val_reg2_sp), format='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.01, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 6.491714957506724\n",
      "Validation Error: 6.595133463794399\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 6.461911787213483\n",
      "Validation Error: 6.581446631609598\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 6.40314233843784\n",
      "Validation Error: 6.494756910963362\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 6.322468623454848\n",
      "Validation Error: 6.449049836055851\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 2.9063349781748853\n",
      "Validation Error: 3.0541690432646975\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 2.864983122568773\n",
      "Validation Error: 3.0248490602683447\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 2.7746577219828428\n",
      "Validation Error: 2.9329276144421748\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 2.641873378434369\n",
      "Validation Error: 2.8661844197284942\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 1.0887502706083245\n",
      "Validation Error: 1.24856173557696\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 1.0375145603395417\n",
      "Validation Error: 1.2099057245529476\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.9181829565520647\n",
      "Validation Error: 1.1137822557339343\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7336193280144047\n",
      "Validation Error: 1.0424370485054306\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7552105318458843\n",
      "Validation Error: 0.9110352688546176\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6873842797112717\n",
      "Validation Error: 0.8515998842555135\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5389879918642386\n",
      "Validation Error: 0.7497130130876134\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.3198095149618316\n",
      "Validation Error: 0.6911225758226082\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 1.559264211405952\n",
      "Validation Error: 1.7152704606516513\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 1.5120695762631462\n",
      "Validation Error: 1.6838742549115513\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 1.4039985997079618\n",
      "Validation Error: 1.5899770772431716\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 1.2373879760527946\n",
      "Validation Error: 1.518689481305781\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.821718140398159\n",
      "Validation Error: 0.97865131218862\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7641372185810101\n",
      "Validation Error: 0.9317374818521682\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6303251445028609\n",
      "Validation Error: 0.8317017652930294\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4259622414099379\n",
      "Validation Error: 0.7667885274073404\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7418270489128119\n",
      "Validation Error: 0.8969849849993025\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6699634556223704\n",
      "Validation Error: 0.8325262461192058\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5178793440103493\n",
      "Validation Error: 0.7288676543394199\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.2921712247748835\n",
      "Validation Error: 0.6810106856122266\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6770420206500362\n",
      "Validation Error: 0.8257219611533172\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5943064877344827\n",
      "Validation Error: 0.7487179925197569\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4211542045423864\n",
      "Validation Error: 1.0404747413268411\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.17962945540889735\n",
      "Validation Error: 1.0337796713329275\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7521297862328494\n",
      "Validation Error: 0.9052569561815387\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6847293784701177\n",
      "Validation Error: 0.850394492470902\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5378531677402704\n",
      "Validation Error: 0.7464448941720473\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.3174466296476431\n",
      "Validation Error: 0.6944014688963156\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7037679342512072\n",
      "Validation Error: 0.8555458263079659\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6262514690875688\n",
      "Validation Error: 0.7880618523491495\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4623858291013403\n",
      "Validation Error: 0.6886055037864045\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.22401227452969538\n",
      "Validation Error: 0.6620071148108415\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6549295133624683\n",
      "Validation Error: 0.8000364587665448\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5714213664549379\n",
      "Validation Error: 0.725358651885929\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.39162576570577257\n",
      "Validation Error: 1.0196794481639277\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.1460545471991196\n",
      "Validation Error: 1.0388516940555894\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6010377757480155\n",
      "Validation Error: 0.7375441373561658\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5146625934734538\n",
      "Validation Error: 0.6724604690718077\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.29857770743742845\n",
      "Validation Error: 0.9973625481842159\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error: 0.06969777971966795\n",
      "Validation Error: 1.0286062081558125\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6692930402447763\n",
      "Validation Error: 0.8200044562755053\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5884008659699668\n",
      "Validation Error: 0.7383251494368801\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.42549690960893155\n",
      "Validation Error: 0.6699183704615137\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.18309838036827866\n",
      "Validation Error: 1.0646175694867113\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6252972247435069\n",
      "Validation Error: 0.7682903278136153\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5442217612780595\n",
      "Validation Error: 0.690639615176997\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.3548163579514815\n",
      "Validation Error: 1.0364171231487118\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.11192888875637015\n",
      "Validation Error: 1.0634002840332724\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5895733475490498\n",
      "Validation Error: 0.7246703453292379\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5044313370884319\n",
      "Validation Error: 0.6595179070706937\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.2780941236192296\n",
      "Validation Error: 1.02759180361128\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.058101636143119736\n",
      "Validation Error: 1.0571109758571717\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5536430963440929\n",
      "Validation Error: 0.687754755888004\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4468381958063219\n",
      "Validation Error: 0.9861026577399067\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.17748816559608316\n",
      "Validation Error: 1.0228241022519782\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.010992911544215032\n",
      "Validation Error: 1.059748427672956\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6350153989380218\n",
      "Validation Error: 0.7792832120098409\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5598281897207122\n",
      "Validation Error: 0.7063470845219258\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.3848791889436608\n",
      "Validation Error: 1.038344491783323\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.14462507167251373\n",
      "Validation Error: 1.10752688172043\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5973726654714011\n",
      "Validation Error: 0.7301162047089381\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5208177893064524\n",
      "Validation Error: 0.6747534419067588\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.3132570992575132\n",
      "Validation Error: 1.038648813146683\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.07221863985763591\n",
      "Validation Error: 0.734013743814687\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5667887285605565\n",
      "Validation Error: 0.6957672956389194\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4751378674546625\n",
      "Validation Error: 1.0022316899979713\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.23868361530122834\n",
      "Validation Error: 1.040068979509028\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.515020043639316\n",
      "Validation Error: 1.1071211199026172\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.534089808488277\n",
      "Validation Error: 0.6692697628321458\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4089931583144231\n",
      "Validation Error: 0.9825522418340434\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5767240066981276\n",
      "Validation Error: 1.044735240413877\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4938600497285229\n",
      "Validation Error: 1.1003246094542503\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.01, 0.03, 0.1, 0.3, 0.5]\n",
    "estimators = [50, 100, 200, 500]\n",
    "depths = [1, 2, 5, 10]\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for estimator in estimators:\n",
    "        for depth in depths:\n",
    "            print(\"Learning Rate: {0}, # Estimators: {1}, Depth: {2}\".format(learning_rate, estimator, depth))\n",
    "            print(\"--------------------------------------------------\")\n",
    "            xg_reg = XGBRegressor(\n",
    "                learning_rate=learning_rate, max_depth=depth, n_estimators=estimator)\n",
    "            xg_reg.fit(X_train_cv_reg2, y_train2)\n",
    "            print(\"Training Error: {}\".format(calculate_MSE(y_train2, vthreshold_rating(xg_reg.predict(X_train_cv_reg2)))))\n",
    "            print(\"Validation Error: {}\".format(calculate_MSE(y_val2, vthreshold_rating(xg_reg.predict(X_val_cv_reg2)))))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems `learning_rate=0.3`, `n_estimators=200`, `max_depth=2` performs the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_reg = XGBRegressor(learning_rate=0.3, n_estimators=200, max_depth=2)\n",
    "xg_reg.fit(X_train_cv_reg2, y_train2)\n",
    "\n",
    "train_MSE = calculate_MSE(y_train2, vthreshold_rating(xg_reg.predict(X_train_cv_reg2)))\n",
    "val_MSE = calculate_MSE(y_val2, vthreshold_rating(xg_reg.predict(X_val_cv_reg2)))\n",
    "\n",
    "print(\"Training error based on XGBoost CountVectorizer prediction: %.3f\" % train_MSE)\n",
    "print(\"Validation error based on XGBoost CountVectorizer prediction: %.3f\" % val_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is actually much worse compared to the pure language model.\n",
    "\n",
    "However, we could also create a meta model by taking a weighted average of predictions from collaborative filtering and the pure language model. We now try this for a few candidate weightings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight: 0.0\n",
      "------------\n",
      "Training error: 0.811\n",
      "Validation error: 0.979\n",
      "\n",
      "Weight: 0.1\n",
      "------------\n",
      "Training error: 0.755\n",
      "Validation error: 0.910\n",
      "\n",
      "Weight: 0.3\n",
      "------------\n",
      "Training error: 0.660\n",
      "Validation error: 0.793\n",
      "\n",
      "Weight: 0.5\n",
      "------------\n",
      "Training error: 0.590\n",
      "Validation error: 0.702\n",
      "\n",
      "Weight: 0.7\n",
      "------------\n",
      "Training error: 0.545\n",
      "Validation error: 0.639\n",
      "\n",
      "Weight: 0.9\n",
      "------------\n",
      "Training error: 0.524\n",
      "Validation error: 0.602\n",
      "\n",
      "Weight: 1.0\n",
      "------------\n",
      "Training error: 0.522\n",
      "Validation error: 0.594\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weights = [0.0, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0]\n",
    "\n",
    "xg_reg = XGBRegressor(learning_rate=0.3, n_estimators=500, max_depth=2)\n",
    "xg_reg.fit(X_train_cv_reg1, y_train1)\n",
    "\n",
    "cf_train_preds = vthreshold_rating(X_train_mod['pred'])\n",
    "cf_val_preds = vthreshold_rating(X_val_mod['pred'])\n",
    "\n",
    "for weight in weights:\n",
    "    print(\"Weight: %.1f\" % weight)\n",
    "    print(\"------------\")\n",
    "    train_MSE = calculate_MSE(y_train1, ((weight*reg_train_preds) + ((1.0 - weight)*cf_train_preds)))\n",
    "    val_MSE = calculate_MSE(y_val1, ((weight*reg_val_preds) + ((1.0 - weight)*cf_val_preds)))\n",
    "    print(\"Training error: %.3f\" % train_MSE)\n",
    "    print(\"Validation error: %.3f\" % val_MSE)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csc2515-env-3.8",
   "language": "python",
   "name": "csc2515-env-3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
