{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Recommender System\n",
    "\n",
    "In this notebook we prototype a set of models to be used in order to build a Recommender System for the Amazon music data.\n",
    "\n",
    "This is done on a small subset of the data (10%) corresponding to 20% rows. In this version we use a single validation set for a wide range of models in order to restrict to a few candidate models which will be trained on the full dataset using cross validation.\n",
    "\n",
    "We first establish a few simple baselines and then progress to implementing two different classes of models:\n",
    "*  Collaborative filtering models based only on user and item data (no text)\n",
    "*  A textual based model\n",
    "\n",
    "We then finally combine the best model from each class into a meta model and evaluate it's performance.\n",
    "\n",
    "We start with some necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "random.seed(17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data from json into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = None\n",
    "with open(os.path.join('data', 'train.json'), 'r') as train_file:\n",
    "    data = [json.loads(row) for row in train_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>category</th>\n",
       "      <th>price</th>\n",
       "      <th>itemID</th>\n",
       "      <th>reviewHash</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>08 24, 2010</td>\n",
       "      <td>u04428712</td>\n",
       "      <td>So is Katy Perry's new album \"Teenage Dream\" c...</td>\n",
       "      <td>Amazing that I Actually Bought This...More Ama...</td>\n",
       "      <td>1282608000</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$35.93</td>\n",
       "      <td>p70761125</td>\n",
       "      <td>85559980</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>10 31, 2009</td>\n",
       "      <td>u06946603</td>\n",
       "      <td>I got this CD almost 10 years ago, and given t...</td>\n",
       "      <td>Excellent album</td>\n",
       "      <td>1256947200</td>\n",
       "      <td>Alternative Rock</td>\n",
       "      <td>$11.28</td>\n",
       "      <td>p85427891</td>\n",
       "      <td>41699565</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>10 13, 2015</td>\n",
       "      <td>u92735614</td>\n",
       "      <td>I REALLY enjoy this pairing of Anderson and Po...</td>\n",
       "      <td>Love the Music, Hate the Light Show</td>\n",
       "      <td>1444694400</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$89.86</td>\n",
       "      <td>p82172532</td>\n",
       "      <td>24751194</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>06 28, 2017</td>\n",
       "      <td>u35112935</td>\n",
       "      <td>Finally got it . It was everything thought it ...</td>\n",
       "      <td>Great</td>\n",
       "      <td>1498608000</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$11.89</td>\n",
       "      <td>p15255251</td>\n",
       "      <td>22820631</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>10 12, 2015</td>\n",
       "      <td>u07141505</td>\n",
       "      <td>Look at all star cast.  Outstanding record, pl...</td>\n",
       "      <td>Love these guys.</td>\n",
       "      <td>1444608000</td>\n",
       "      <td>Jazz</td>\n",
       "      <td>$15.24</td>\n",
       "      <td>p82618188</td>\n",
       "      <td>53377470</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>5.0</td>\n",
       "      <td>07 1, 2005</td>\n",
       "      <td>u35885825</td>\n",
       "      <td>This DVD rates a strong five stars with me.  T...</td>\n",
       "      <td>Absolutely Wonderful</td>\n",
       "      <td>1120176000</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$11.91</td>\n",
       "      <td>p00193036</td>\n",
       "      <td>43343710</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>5.0</td>\n",
       "      <td>04 5, 2017</td>\n",
       "      <td>u10979151</td>\n",
       "      <td>No wonder this sold 32 million copies it is an...</td>\n",
       "      <td>... wonder this sold 32 million copies it is a...</td>\n",
       "      <td>1491350400</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$8.78</td>\n",
       "      <td>p21938211</td>\n",
       "      <td>03563170</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>5.0</td>\n",
       "      <td>06 13, 2005</td>\n",
       "      <td>u50404725</td>\n",
       "      <td>Learning to Breathe is one of my favorite albu...</td>\n",
       "      <td>Another must have from Switchfoot</td>\n",
       "      <td>1118620800</td>\n",
       "      <td>Alternative Rock</td>\n",
       "      <td>$9.99</td>\n",
       "      <td>p78687570</td>\n",
       "      <td>61979023</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>5.0</td>\n",
       "      <td>03 17, 2016</td>\n",
       "      <td>u53979205</td>\n",
       "      <td>I remember first hearing this band when they c...</td>\n",
       "      <td>giving back to the band the love and joy that ...</td>\n",
       "      <td>1458172800</td>\n",
       "      <td>Jazz</td>\n",
       "      <td>$57.37</td>\n",
       "      <td>p95260169</td>\n",
       "      <td>52021524</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>5.0</td>\n",
       "      <td>01 28, 2007</td>\n",
       "      <td>u90545314</td>\n",
       "      <td>cool. They had nothing to say but kind words w...</td>\n",
       "      <td>these WoMen Are</td>\n",
       "      <td>1169942400</td>\n",
       "      <td>Alternative Rock</td>\n",
       "      <td>$26.55</td>\n",
       "      <td>p17894919</td>\n",
       "      <td>28658562</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       overall   reviewTime reviewerID  \\\n",
       "0          4.0  08 24, 2010  u04428712   \n",
       "1          5.0  10 31, 2009  u06946603   \n",
       "2          4.0  10 13, 2015  u92735614   \n",
       "3          5.0  06 28, 2017  u35112935   \n",
       "4          4.0  10 12, 2015  u07141505   \n",
       "...        ...          ...        ...   \n",
       "49995      5.0   07 1, 2005  u35885825   \n",
       "49996      5.0   04 5, 2017  u10979151   \n",
       "49997      5.0  06 13, 2005  u50404725   \n",
       "49998      5.0  03 17, 2016  u53979205   \n",
       "49999      5.0  01 28, 2007  u90545314   \n",
       "\n",
       "                                              reviewText  \\\n",
       "0      So is Katy Perry's new album \"Teenage Dream\" c...   \n",
       "1      I got this CD almost 10 years ago, and given t...   \n",
       "2      I REALLY enjoy this pairing of Anderson and Po...   \n",
       "3      Finally got it . It was everything thought it ...   \n",
       "4      Look at all star cast.  Outstanding record, pl...   \n",
       "...                                                  ...   \n",
       "49995  This DVD rates a strong five stars with me.  T...   \n",
       "49996  No wonder this sold 32 million copies it is an...   \n",
       "49997  Learning to Breathe is one of my favorite albu...   \n",
       "49998  I remember first hearing this band when they c...   \n",
       "49999  cool. They had nothing to say but kind words w...   \n",
       "\n",
       "                                                 summary  unixReviewTime  \\\n",
       "0      Amazing that I Actually Bought This...More Ama...      1282608000   \n",
       "1                                        Excellent album      1256947200   \n",
       "2                    Love the Music, Hate the Light Show      1444694400   \n",
       "3                                                  Great      1498608000   \n",
       "4                                       Love these guys.      1444608000   \n",
       "...                                                  ...             ...   \n",
       "49995                               Absolutely Wonderful      1120176000   \n",
       "49996  ... wonder this sold 32 million copies it is a...      1491350400   \n",
       "49997                  Another must have from Switchfoot      1118620800   \n",
       "49998  giving back to the band the love and joy that ...      1458172800   \n",
       "49999                                    these WoMen Are      1169942400   \n",
       "\n",
       "               category   price     itemID reviewHash image  \n",
       "0                   Pop  $35.93  p70761125   85559980   NaN  \n",
       "1      Alternative Rock  $11.28  p85427891   41699565   NaN  \n",
       "2                   Pop  $89.86  p82172532   24751194   NaN  \n",
       "3                   Pop  $11.89  p15255251   22820631   NaN  \n",
       "4                  Jazz  $15.24  p82618188   53377470   NaN  \n",
       "...                 ...     ...        ...        ...   ...  \n",
       "49995               Pop  $11.91  p00193036   43343710   NaN  \n",
       "49996               Pop   $8.78  p21938211   03563170   NaN  \n",
       "49997  Alternative Rock   $9.99  p78687570   61979023   NaN  \n",
       "49998              Jazz  $57.37  p95260169   52021524   NaN  \n",
       "49999  Alternative Rock  $26.55  p17894919   28658562   NaN  \n",
       "\n",
       "[50000 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.DataFrame(data)\n",
    "data_df = data_df[0:50000]\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function to handle price so that it can be converted to a float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_price(price):\n",
    "    \"\"\"Trims `price` to remove the $ sign.\n",
    "    \n",
    "    If the price variable does not have the format $x.xx\n",
    "    then the empty string is returned.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    price: str\n",
    "        A string representing a price.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A string representing `price` but with the $ sign removed,\n",
    "        or the empty string if `price` does not have the correct\n",
    "        format.\n",
    "    \n",
    "    \"\"\"\n",
    "    if (not pd.isnull(price) and isinstance(price, str) and\n",
    "        len(price) > 0 and price[0] == '$'):\n",
    "        return price[1:]\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "We add some additional features:\n",
    "*  reviewMonth - the month in which the review was done.\n",
    "*  reviewYear - the year in which the review was done.\n",
    "*  reviewHour - the hour in which the review was done\n",
    "*  cleanedPrice - a numeric version of the price column. We only keep this column if the price is correctly formatted.\n",
    "*  fullReviewText - a column that combines the summary followed by reviewText\n",
    "*  reviewWordCount - indicates whether the record has an associated review based on the fullReviewText column\n",
    "\n",
    "We also add an indicator variable for each music category to indicate if the record is in that category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-d3ba8e2e1b50>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_df['cleanedPrice'] = data_df['cleanedPrice'].astype('float')\n",
      "<ipython-input-5-d3ba8e2e1b50>:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_df['fixedReviewText'] = np.where(pd.isnull(data_df['reviewText']), \"\", data_df['reviewText'])\n",
      "<ipython-input-5-d3ba8e2e1b50>:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_df['fixedSummary'] = np.where(pd.isnull(data_df['summary']), \"\", data_df['summary'])\n",
      "<ipython-input-5-d3ba8e2e1b50>:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_df['fullReviewText'] = data_df['fixedSummary'] + \" \" + data_df['fixedReviewText']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>category</th>\n",
       "      <th>price</th>\n",
       "      <th>itemID</th>\n",
       "      <th>reviewHash</th>\n",
       "      <th>...</th>\n",
       "      <th>reviewHour</th>\n",
       "      <th>reviewMonthYear</th>\n",
       "      <th>cleanedPrice</th>\n",
       "      <th>fullReviewText</th>\n",
       "      <th>isPop</th>\n",
       "      <th>isAlternativeRock</th>\n",
       "      <th>isJazz</th>\n",
       "      <th>isClassical</th>\n",
       "      <th>isDanceElectronic</th>\n",
       "      <th>reviewWordCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>08 24, 2010</td>\n",
       "      <td>u04428712</td>\n",
       "      <td>So is Katy Perry's new album \"Teenage Dream\" c...</td>\n",
       "      <td>Amazing that I Actually Bought This...More Ama...</td>\n",
       "      <td>1282608000</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$35.93</td>\n",
       "      <td>p70761125</td>\n",
       "      <td>85559980</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>2010-08</td>\n",
       "      <td>35.93</td>\n",
       "      <td>Amazing that I Actually Bought This...More Ama...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>10 31, 2009</td>\n",
       "      <td>u06946603</td>\n",
       "      <td>I got this CD almost 10 years ago, and given t...</td>\n",
       "      <td>Excellent album</td>\n",
       "      <td>1256947200</td>\n",
       "      <td>Alternative Rock</td>\n",
       "      <td>$11.28</td>\n",
       "      <td>p85427891</td>\n",
       "      <td>41699565</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>2009-10</td>\n",
       "      <td>11.28</td>\n",
       "      <td>Excellent album I got this CD almost 10 years ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>10 13, 2015</td>\n",
       "      <td>u92735614</td>\n",
       "      <td>I REALLY enjoy this pairing of Anderson and Po...</td>\n",
       "      <td>Love the Music, Hate the Light Show</td>\n",
       "      <td>1444694400</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$89.86</td>\n",
       "      <td>p82172532</td>\n",
       "      <td>24751194</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>2015-10</td>\n",
       "      <td>89.86</td>\n",
       "      <td>Love the Music, Hate the Light Show I REALLY e...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>06 28, 2017</td>\n",
       "      <td>u35112935</td>\n",
       "      <td>Finally got it . It was everything thought it ...</td>\n",
       "      <td>Great</td>\n",
       "      <td>1498608000</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$11.89</td>\n",
       "      <td>p15255251</td>\n",
       "      <td>22820631</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>2017-06</td>\n",
       "      <td>11.89</td>\n",
       "      <td>Great Finally got it . It was everything thoug...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>10 12, 2015</td>\n",
       "      <td>u07141505</td>\n",
       "      <td>Look at all star cast.  Outstanding record, pl...</td>\n",
       "      <td>Love these guys.</td>\n",
       "      <td>1444608000</td>\n",
       "      <td>Jazz</td>\n",
       "      <td>$15.24</td>\n",
       "      <td>p82618188</td>\n",
       "      <td>53377470</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>2015-10</td>\n",
       "      <td>15.24</td>\n",
       "      <td>Love these guys. Look at all star cast.  Outst...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>5.0</td>\n",
       "      <td>07 1, 2005</td>\n",
       "      <td>u35885825</td>\n",
       "      <td>This DVD rates a strong five stars with me.  T...</td>\n",
       "      <td>Absolutely Wonderful</td>\n",
       "      <td>1120176000</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$11.91</td>\n",
       "      <td>p00193036</td>\n",
       "      <td>43343710</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>2005-07</td>\n",
       "      <td>11.91</td>\n",
       "      <td>Absolutely Wonderful This DVD rates a strong f...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>5.0</td>\n",
       "      <td>04 5, 2017</td>\n",
       "      <td>u10979151</td>\n",
       "      <td>No wonder this sold 32 million copies it is an...</td>\n",
       "      <td>... wonder this sold 32 million copies it is a...</td>\n",
       "      <td>1491350400</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$8.78</td>\n",
       "      <td>p21938211</td>\n",
       "      <td>03563170</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>2017-04</td>\n",
       "      <td>8.78</td>\n",
       "      <td>... wonder this sold 32 million copies it is a...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>5.0</td>\n",
       "      <td>06 13, 2005</td>\n",
       "      <td>u50404725</td>\n",
       "      <td>Learning to Breathe is one of my favorite albu...</td>\n",
       "      <td>Another must have from Switchfoot</td>\n",
       "      <td>1118620800</td>\n",
       "      <td>Alternative Rock</td>\n",
       "      <td>$9.99</td>\n",
       "      <td>p78687570</td>\n",
       "      <td>61979023</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>2005-06</td>\n",
       "      <td>9.99</td>\n",
       "      <td>Another must have from Switchfoot Learning to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>5.0</td>\n",
       "      <td>03 17, 2016</td>\n",
       "      <td>u53979205</td>\n",
       "      <td>I remember first hearing this band when they c...</td>\n",
       "      <td>giving back to the band the love and joy that ...</td>\n",
       "      <td>1458172800</td>\n",
       "      <td>Jazz</td>\n",
       "      <td>$57.37</td>\n",
       "      <td>p95260169</td>\n",
       "      <td>52021524</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>2016-03</td>\n",
       "      <td>57.37</td>\n",
       "      <td>giving back to the band the love and joy that ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>5.0</td>\n",
       "      <td>01 28, 2007</td>\n",
       "      <td>u90545314</td>\n",
       "      <td>cool. They had nothing to say but kind words w...</td>\n",
       "      <td>these WoMen Are</td>\n",
       "      <td>1169942400</td>\n",
       "      <td>Alternative Rock</td>\n",
       "      <td>$26.55</td>\n",
       "      <td>p17894919</td>\n",
       "      <td>28658562</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>2007-01</td>\n",
       "      <td>26.55</td>\n",
       "      <td>these WoMen Are cool. They had nothing to say ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49345 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       overall   reviewTime reviewerID  \\\n",
       "0          4.0  08 24, 2010  u04428712   \n",
       "1          5.0  10 31, 2009  u06946603   \n",
       "2          4.0  10 13, 2015  u92735614   \n",
       "3          5.0  06 28, 2017  u35112935   \n",
       "4          4.0  10 12, 2015  u07141505   \n",
       "...        ...          ...        ...   \n",
       "49995      5.0   07 1, 2005  u35885825   \n",
       "49996      5.0   04 5, 2017  u10979151   \n",
       "49997      5.0  06 13, 2005  u50404725   \n",
       "49998      5.0  03 17, 2016  u53979205   \n",
       "49999      5.0  01 28, 2007  u90545314   \n",
       "\n",
       "                                              reviewText  \\\n",
       "0      So is Katy Perry's new album \"Teenage Dream\" c...   \n",
       "1      I got this CD almost 10 years ago, and given t...   \n",
       "2      I REALLY enjoy this pairing of Anderson and Po...   \n",
       "3      Finally got it . It was everything thought it ...   \n",
       "4      Look at all star cast.  Outstanding record, pl...   \n",
       "...                                                  ...   \n",
       "49995  This DVD rates a strong five stars with me.  T...   \n",
       "49996  No wonder this sold 32 million copies it is an...   \n",
       "49997  Learning to Breathe is one of my favorite albu...   \n",
       "49998  I remember first hearing this band when they c...   \n",
       "49999  cool. They had nothing to say but kind words w...   \n",
       "\n",
       "                                                 summary  unixReviewTime  \\\n",
       "0      Amazing that I Actually Bought This...More Ama...      1282608000   \n",
       "1                                        Excellent album      1256947200   \n",
       "2                    Love the Music, Hate the Light Show      1444694400   \n",
       "3                                                  Great      1498608000   \n",
       "4                                       Love these guys.      1444608000   \n",
       "...                                                  ...             ...   \n",
       "49995                               Absolutely Wonderful      1120176000   \n",
       "49996  ... wonder this sold 32 million copies it is a...      1491350400   \n",
       "49997                  Another must have from Switchfoot      1118620800   \n",
       "49998  giving back to the band the love and joy that ...      1458172800   \n",
       "49999                                    these WoMen Are      1169942400   \n",
       "\n",
       "               category   price     itemID reviewHash  ... reviewHour  \\\n",
       "0                   Pop  $35.93  p70761125   85559980  ...         20   \n",
       "1      Alternative Rock  $11.28  p85427891   41699565  ...         20   \n",
       "2                   Pop  $89.86  p82172532   24751194  ...         20   \n",
       "3                   Pop  $11.89  p15255251   22820631  ...         20   \n",
       "4                  Jazz  $15.24  p82618188   53377470  ...         20   \n",
       "...                 ...     ...        ...        ...  ...        ...   \n",
       "49995               Pop  $11.91  p00193036   43343710  ...         20   \n",
       "49996               Pop   $8.78  p21938211   03563170  ...         20   \n",
       "49997  Alternative Rock   $9.99  p78687570   61979023  ...         20   \n",
       "49998              Jazz  $57.37  p95260169   52021524  ...         20   \n",
       "49999  Alternative Rock  $26.55  p17894919   28658562  ...         19   \n",
       "\n",
       "      reviewMonthYear cleanedPrice  \\\n",
       "0             2010-08        35.93   \n",
       "1             2009-10        11.28   \n",
       "2             2015-10        89.86   \n",
       "3             2017-06        11.89   \n",
       "4             2015-10        15.24   \n",
       "...               ...          ...   \n",
       "49995         2005-07        11.91   \n",
       "49996         2017-04         8.78   \n",
       "49997         2005-06         9.99   \n",
       "49998         2016-03        57.37   \n",
       "49999         2007-01        26.55   \n",
       "\n",
       "                                          fullReviewText isPop  \\\n",
       "0      Amazing that I Actually Bought This...More Ama...     1   \n",
       "1      Excellent album I got this CD almost 10 years ...     0   \n",
       "2      Love the Music, Hate the Light Show I REALLY e...     1   \n",
       "3      Great Finally got it . It was everything thoug...     1   \n",
       "4      Love these guys. Look at all star cast.  Outst...     0   \n",
       "...                                                  ...   ...   \n",
       "49995  Absolutely Wonderful This DVD rates a strong f...     1   \n",
       "49996  ... wonder this sold 32 million copies it is a...     1   \n",
       "49997  Another must have from Switchfoot Learning to ...     0   \n",
       "49998  giving back to the band the love and joy that ...     0   \n",
       "49999  these WoMen Are cool. They had nothing to say ...     0   \n",
       "\n",
       "       isAlternativeRock isJazz  isClassical  isDanceElectronic  \\\n",
       "0                      0      0            0                  0   \n",
       "1                      1      0            0                  0   \n",
       "2                      0      0            0                  0   \n",
       "3                      0      0            0                  0   \n",
       "4                      0      1            0                  0   \n",
       "...                  ...    ...          ...                ...   \n",
       "49995                  0      0            0                  0   \n",
       "49996                  0      0            0                  0   \n",
       "49997                  1      0            0                  0   \n",
       "49998                  0      1            0                  0   \n",
       "49999                  1      0            0                  0   \n",
       "\n",
       "       reviewWordCount  \n",
       "0                  277  \n",
       "1                  125  \n",
       "2                  133  \n",
       "3                   15  \n",
       "4                   21  \n",
       "...                ...  \n",
       "49995              130  \n",
       "49996               24  \n",
       "49997              720  \n",
       "49998              137  \n",
       "49999               60  \n",
       "\n",
       "[49345 rows x 23 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "data_df['reviewMonth'] = data_df['reviewTime'].apply(lambda x: x.split(' ')[0])\n",
    "data_df['reviewYear'] = data_df['reviewTime'].apply(lambda x: x.split(' ')[2])\n",
    "data_df['reviewHour'] = data_df['unixReviewTime'].apply(lambda x: datetime.fromtimestamp(x).hour)\n",
    "data_df['reviewMonthYear'] = data_df['reviewYear'] + '-' + data_df['reviewMonth']\n",
    "\n",
    "data_df['cleanedPrice'] = data_df['price'].apply(lambda x: trim_price(x))\n",
    "data_df = data_df[data_df['cleanedPrice'] != \"\"]\n",
    "data_df['cleanedPrice'] = data_df['cleanedPrice'].astype('float')\n",
    "\n",
    "data_df['fixedReviewText'] = np.where(pd.isnull(data_df['reviewText']), \"\", data_df['reviewText'])\n",
    "data_df['fixedSummary'] = np.where(pd.isnull(data_df['summary']), \"\", data_df['summary'])\n",
    "data_df['fullReviewText'] = data_df['fixedSummary'] + \" \" + data_df['fixedReviewText']\n",
    "\n",
    "data_df = data_df.drop(columns=['fixedReviewText', 'fixedSummary'])\n",
    "\n",
    "genres = data_df['category'].unique()\n",
    "\n",
    "for genre in genres:\n",
    "    genre_col = \"is\" + genre.replace(\" \", \"\").replace(\"&\", \"\")\n",
    "    data_df[genre_col] = data_df['category'].apply(lambda x: 1 if x == genre else 0)\n",
    "\n",
    "data_df['reviewWordCount'] = data_df['fullReviewText'].apply(lambda x: len(x.split()))\n",
    "\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics\n",
    "\n",
    "Definining a MSE function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_MSE(actuals, predicteds):\n",
    "    \"\"\"Calculates the Mean Squared Error between `actuals` and `predicteds`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    actuals: np.array\n",
    "        A numpy array of the actual values.\n",
    "    predicteds: np.array\n",
    "        A numpy array of the predicted values.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        A float representing the Mean Squared Error between `actuals` and\n",
    "        `predicteds`.\n",
    "    \n",
    "    \"\"\"\n",
    "    return (((actuals - predicteds)**2).sum()) / (len(actuals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate targets and data.\n",
    "\n",
    "Then split into training and validation sets.\n",
    "\n",
    "Note that we split into validation sets for each music genre and then concatenate the data frames so that the proportion of each genre in the train and validation sets is equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "genres = data_df['category'].unique()\n",
    "X_train_set = []\n",
    "X_val_set = []\n",
    "y_train_set = []\n",
    "y_val_set = []\n",
    "\n",
    "for genre in genres:\n",
    "    genre_df = data_df[data_df['category'] == genre]\n",
    "    targets = genre_df['overall']\n",
    "    feature_data = genre_df.drop(columns=['overall'])\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        feature_data, targets, shuffle=True, test_size=0.2, random_state=17)\n",
    "    X_train_set.append(X_train)\n",
    "    X_val_set.append(X_val)\n",
    "    y_train_set.append(y_train)\n",
    "    y_val_set.append(y_val)\n",
    "\n",
    "X_train = pd.concat(X_train_set)\n",
    "X_val = pd.concat(X_val_set)\n",
    "y_train = pd.concat(y_train_set)\n",
    "y_val = pd.concat(y_val_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fitting\n",
    "\n",
    "Throughout the model fitting process we will keep 3 arrays that store the model name, training error, and validation error respectively for all models that we prototype.\n",
    "\n",
    "##### Baselines\n",
    "\n",
    "We will look at two simple baseline models.\n",
    "\n",
    "The first is the same baseline model implemented in `baseline.py`. But we will evaluate its performance on the validation set in order to fit models and compare performance on data that is distinct from the test set.\n",
    "\n",
    "In this model we simply compute the average rating and assign this as our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = []\n",
    "train_errors = []\n",
    "validation_errors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_on_average(targets, avg):\n",
    "    \"\"\"Computers the error based on using average rating as the prediction.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    targets: np.array\n",
    "        The actual ratings.\n",
    "    avg: float\n",
    "        The predicted rating based on an average.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        A float representing the mean squared error from predicting\n",
    "        based on `avg`.\n",
    "    \n",
    "    \"\"\"\n",
    "    return calculate_MSE(targets, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error based on average prediction: 0.994\n",
      "Validation error based on average prediction: 0.950\n"
     ]
    }
   ],
   "source": [
    "train_avg = y_train.mean()\n",
    "\n",
    "model_names.append(\"Average\")\n",
    "train_errors.append(error_on_average(y_train, train_avg))\n",
    "validation_errors.append(error_on_average(y_val, train_avg))\n",
    "\n",
    "print(\"Training error based on average prediction: %.3f\" % train_errors[0])\n",
    "print(\"Validation error based on average prediction: %.3f\" % validation_errors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our second baseline model is slightly more complicated. We will calculate three types of quantities:\n",
    "*  The overall average\n",
    "*  The difference between the average rating for each item and the overall average\n",
    "*  The difference between the average rating for each user and the overall average\n",
    "\n",
    "Our prediction for a particular user and item will then be the sum of these 3 quantities.\n",
    "\n",
    "We will denote this model as Weighted Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "train_avg_total = y_train.mean()\n",
    "train_user_avg = train_df.groupby(train_df['reviewerID'], as_index=False)['overall'].mean()\n",
    "train_item_avg = train_df.groupby(train_df['itemID'], as_index=False)['overall'].mean()\n",
    "train_user_avg.columns = ['reviewerID', 'userAverage']\n",
    "train_item_avg.columns = ['itemID', 'itemAverage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_rating(rating):\n",
    "    \"\"\"Thresholds `rating` to lie in the range [1, 5].\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rating: float\n",
    "        The rating to be thresholded.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        A float representing the thresholded rating.\n",
    "    \n",
    "    \"\"\"\n",
    "    if rating < 1:\n",
    "        return 1\n",
    "    if rating > 5:\n",
    "        return 5\n",
    "    return rating\n",
    "\n",
    "def weighted_average_error(X, y, total_avg, user_avgs, item_avgs):\n",
    "    \"\"\"Calculates the error based on the weighted average prediction.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: pd.DataFrame\n",
    "        The DataFrame of features.\n",
    "    y: np.array\n",
    "        A numpy array containing the targets\n",
    "    total_avg: float\n",
    "        The average across all users/items.\n",
    "    user_avgs: pd.DataFrame\n",
    "        A DataFrame containing the average rating for each user.\n",
    "    item_avgs: pd.DataFrame\n",
    "        A DataFrame containing the average rating for each item.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        A float representing the mean squared error of the predictions.\n",
    "    \n",
    "    \"\"\"\n",
    "    df_user = pd.merge(X, user_avgs, how='left', on=['reviewerID'])\n",
    "    df_final = pd.merge(df_user, item_avgs, how='left', on=['itemID'])\n",
    "    df_final = df_final[['userAverage', 'itemAverage']]\n",
    "    df_final.fillna(total_avg)\n",
    "    df_final['pred'] = df_final['userAverage'] + df_final['itemAverage'] - total_avg\n",
    "    df_final['pred'].apply(lambda x: threshold_rating(x))\n",
    "    return calculate_MSE(y, df_final['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error based on weighted average prediction: 2.234\n",
      "Validation error based on weighted average prediction: 0.173\n"
     ]
    }
   ],
   "source": [
    "train_MSE = weighted_average_error(X_train, y_train, train_avg_total, train_user_avg, train_item_avg)\n",
    "val_MSE = weighted_average_error(X_val, y_val, train_avg_total, train_user_avg, train_item_avg)\n",
    "\n",
    "model_names.append(\"Weighted Average\")\n",
    "train_errors.append(train_MSE)\n",
    "validation_errors.append(val_MSE)\n",
    "\n",
    "print(\"Training error based on weighted average prediction: %.3f\" % train_MSE)\n",
    "print(\"Validation error based on weighted average prediction: %.3f\" % val_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Models\n",
    "\n",
    "In this section we build a set of feature based models to predict ratings on our validation set and compare their performance. These models do not follow the typical recommender system approach of collaborative filtering / matrix factorization\n",
    "\n",
    "We start with a linear regression model. At this stage we do not have a lot of features and so it makes more sense to use the $L_{2}$-norm for regularization (a.k.a. ridge regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['cleanedPrice', 'isPop', 'isAlternativeRock', 'isJazz', 'isClassical', 'isDanceElectronic', 'reviewWordCount']\n",
    "X_train_reg = X_train[columns_to_keep]\n",
    "X_val_reg = X_val[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-324e3f034aed>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train_reg['reviewWordCount'] = X_train_reg['reviewWordCount'].apply(lambda x: np.log(x))\n",
      "<ipython-input-15-324e3f034aed>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val_reg['reviewWordCount'] = X_val_reg['reviewWordCount'].apply(lambda x: np.log(x))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_train_reg['reviewWordCount'] = X_train_reg['reviewWordCount'].apply(lambda x: np.log(x))\n",
    "X_val_reg['reviewWordCount'] = X_val_reg['reviewWordCount'].apply(lambda x: np.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-632f1f5ab49f>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.dropna(inplace=True)\n",
      "<ipython-input-16-632f1f5ab49f>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.dropna(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "def clean_dataset(df):\n",
    "    df.dropna(inplace=True)\n",
    "    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "    return df[indices_to_keep].astype(np.float64)\n",
    "\n",
    "X_train_reg = clean_dataset(X_train_reg)\n",
    "y_train = y_train[y_train.index.isin(X_train_reg.index)]\n",
    "X_train = X_train[X_train.index.isin(X_train_reg.index)]\n",
    "\n",
    "X_val_reg = clean_dataset(X_val_reg)\n",
    "y_val = y_val[y_val.index.isin(X_val_reg.index)]\n",
    "X_val = X_val[X_val.index.isin(X_val_reg.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reg = min_max_scaler.fit_transform(X_train_reg)\n",
    "X_val_reg = min_max_scaler.transform(X_val_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha = 0.0\n",
      "------------\n",
      "Training Error: 0.9616192418446101\n",
      "Validation Error: 0.9227567987732785\n",
      "\n",
      "Alpha = 0.01\n",
      "------------\n",
      "Training Error: 0.9616194518528689\n",
      "Validation Error: 0.9227570277991929\n",
      "\n",
      "Alpha = 0.03\n",
      "------------\n",
      "Training Error: 0.9616198715719257\n",
      "Validation Error: 0.9227574853932203\n",
      "\n",
      "Alpha = 0.1\n",
      "------------\n",
      "Training Error: 0.9616213374549035\n",
      "Validation Error: 0.9227590821637957\n",
      "\n",
      "Alpha = 0.3\n",
      "------------\n",
      "Training Error: 0.9616254985913107\n",
      "Validation Error: 0.9227636031245654\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "vthreshold_rating = np.vectorize(threshold_rating)\n",
    "\n",
    "alphas = [0.0, 0.01, 0.03, 0.1, 0.3]\n",
    "for alpha in alphas:\n",
    "    print(\"Alpha = {}\".format(alpha))\n",
    "    print(\"------------\")\n",
    "    reg_model = Ridge(alpha=alpha)\n",
    "    reg_model.fit(X_train_reg, y_train)\n",
    "    print(\"Training Error: {}\".format(calculate_MSE(y_train, vthreshold_rating(reg_model.predict(X_train_reg)))))\n",
    "    print(\"Validation Error: {}\".format(calculate_MSE(y_val, vthreshold_rating(reg_model.predict(X_val_reg)))))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After $\\alpha = 0.01$ the MSE does not marginally change and so we will use a regularization term of 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error based on L2 regularized regression prediction: 0.962\n",
      "Validation error based on L2 regularized regression prediction: 0.923\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "reg_model = Ridge(alpha=0.01)\n",
    "reg_model.fit(X_train_reg, y_train)\n",
    "\n",
    "train_MSE = calculate_MSE(y_train, vthreshold_rating(reg_model.predict(X_train_reg)))\n",
    "val_MSE = calculate_MSE(y_val, vthreshold_rating(reg_model.predict(X_val_reg)))\n",
    "\n",
    "model_names.append(\"L2-Reg\")\n",
    "train_errors.append(train_MSE)\n",
    "validation_errors.append(val_MSE)\n",
    "\n",
    "print(\"Training error based on L2 regularized regression prediction: %.3f\" % train_MSE)\n",
    "print(\"Validation error based on L2 regularized regression prediction: %.3f\" % val_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now look at some natural language processing models.\n",
    "\n",
    "We start by processing the review column. This involves the following:\n",
    "* Removing all non-alphanumeric characters\n",
    "* converting to lower case\n",
    "* removing a set of exclusion words (the english stopwords)\n",
    "* and stemming (getting the root word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Matthew/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def process_review_text(review_text, exclude_text, ps):\n",
    "    \"\"\"Pre-processes the text given by `review_text`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    review_text: str\n",
    "        The review text to be processed.\n",
    "    exclude_text: collection\n",
    "        A collection of words to be excluded.\n",
    "    ps: PorterStemmer\n",
    "        The PorterStemmer used to perform word stemming.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A string representing the processed version of `review_text`.\n",
    "    \n",
    "    \"\"\"\n",
    "    review = re.sub('[^a-zA-Z0-9]', ' ', review_text).lower().split()\n",
    "    review = [ps.stem(word) for word in review if not word in exclude_text]\n",
    "    return ' '.join(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>category</th>\n",
       "      <th>price</th>\n",
       "      <th>itemID</th>\n",
       "      <th>reviewHash</th>\n",
       "      <th>image</th>\n",
       "      <th>...</th>\n",
       "      <th>reviewMonthYear</th>\n",
       "      <th>cleanedPrice</th>\n",
       "      <th>fullReviewText</th>\n",
       "      <th>isPop</th>\n",
       "      <th>isAlternativeRock</th>\n",
       "      <th>isJazz</th>\n",
       "      <th>isClassical</th>\n",
       "      <th>isDanceElectronic</th>\n",
       "      <th>reviewWordCount</th>\n",
       "      <th>processedReview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32105</th>\n",
       "      <td>01 29, 2017</td>\n",
       "      <td>u67724906</td>\n",
       "      <td>I liked it a lot when I first heard it, and it...</td>\n",
       "      <td>Dependable, authentic, great for those long dr...</td>\n",
       "      <td>1485648000</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$12.51</td>\n",
       "      <td>p07525947</td>\n",
       "      <td>94030061</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2017-01</td>\n",
       "      <td>12.51</td>\n",
       "      <td>Dependable, authentic, great for those long dr...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>depend authent great long drive like lot first...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14490</th>\n",
       "      <td>05 24, 2006</td>\n",
       "      <td>u10309608</td>\n",
       "      <td>The best Chili Peppers albums were Blood Sugar...</td>\n",
       "      <td>I give it 4 stars because it's RHCP...but it's...</td>\n",
       "      <td>1148428800</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$11.89</td>\n",
       "      <td>p36536384</td>\n",
       "      <td>32777266</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006-05</td>\n",
       "      <td>11.89</td>\n",
       "      <td>I give it 4 stars because it's RHCP...but it's...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>515</td>\n",
       "      <td>give 4 star rhcp noth special best chili peppe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16921</th>\n",
       "      <td>05 28, 2013</td>\n",
       "      <td>u04929059</td>\n",
       "      <td>When I saw the review and buy the songs. Expec...</td>\n",
       "      <td>Few songs</td>\n",
       "      <td>1369699200</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$8.99</td>\n",
       "      <td>p03487927</td>\n",
       "      <td>11500344</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-05</td>\n",
       "      <td>8.99</td>\n",
       "      <td>Few songs When I saw the review and buy the so...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>song saw review buy song expect bring action o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39859</th>\n",
       "      <td>06 5, 2017</td>\n",
       "      <td>u20690354</td>\n",
       "      <td>GREAT!!</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1496620800</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$6.96</td>\n",
       "      <td>p36784389</td>\n",
       "      <td>87729782</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2017-06</td>\n",
       "      <td>6.96</td>\n",
       "      <td>Five Stars GREAT!!</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>five star great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25563</th>\n",
       "      <td>12 6, 2013</td>\n",
       "      <td>u63753846</td>\n",
       "      <td>There is very little; this Lady can sing, that...</td>\n",
       "      <td>A GREAT ONE....</td>\n",
       "      <td>1386288000</td>\n",
       "      <td>Pop</td>\n",
       "      <td>$9.10</td>\n",
       "      <td>p28779552</td>\n",
       "      <td>92947167</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-12</td>\n",
       "      <td>9.10</td>\n",
       "      <td>A GREAT ONE.... There is very little; this Lad...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>great one littl ladi sing like immens show man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20877</th>\n",
       "      <td>10 29, 2003</td>\n",
       "      <td>u83502050</td>\n",
       "      <td>I'm undoubtedly in the minority here, but this...</td>\n",
       "      <td>For me personally.....Underworlds finest work!!!</td>\n",
       "      <td>1067385600</td>\n",
       "      <td>Dance &amp; Electronic</td>\n",
       "      <td>$11.97</td>\n",
       "      <td>p73092793</td>\n",
       "      <td>32251137</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2003-10</td>\n",
       "      <td>11.97</td>\n",
       "      <td>For me personally.....Underworlds finest work!...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>177</td>\n",
       "      <td>person underworld finest work undoubtedli mino...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6058</th>\n",
       "      <td>02 17, 2015</td>\n",
       "      <td>u72292741</td>\n",
       "      <td>like</td>\n",
       "      <td>Four Stars</td>\n",
       "      <td>1424131200</td>\n",
       "      <td>Dance &amp; Electronic</td>\n",
       "      <td>$13.96</td>\n",
       "      <td>p87964435</td>\n",
       "      <td>31658850</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2015-02</td>\n",
       "      <td>13.96</td>\n",
       "      <td>Four Stars like</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>four star like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22092</th>\n",
       "      <td>08 31, 2001</td>\n",
       "      <td>u13926491</td>\n",
       "      <td>Bjork is back with \"Vespertine\".\\nA good effor...</td>\n",
       "      <td>VESPERTINE DELIVERS ON COVER ART, BUT DON'T JU...</td>\n",
       "      <td>999216000</td>\n",
       "      <td>Dance &amp; Electronic</td>\n",
       "      <td>$18.29</td>\n",
       "      <td>p15146077</td>\n",
       "      <td>59635469</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2001-08</td>\n",
       "      <td>18.29</td>\n",
       "      <td>VESPERTINE DELIVERS ON COVER ART, BUT DON'T JU...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>233</td>\n",
       "      <td>vespertin deliv cover art judg book bjork back...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34562</th>\n",
       "      <td>06 4, 2003</td>\n",
       "      <td>u11852145</td>\n",
       "      <td>If hip-hop is truly where the mainstream music...</td>\n",
       "      <td>Some of the best hip-hop/R&amp;B yet to be release...</td>\n",
       "      <td>1054684800</td>\n",
       "      <td>Dance &amp; Electronic</td>\n",
       "      <td>$0.57</td>\n",
       "      <td>p22525318</td>\n",
       "      <td>94089341</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2003-06</td>\n",
       "      <td>0.57</td>\n",
       "      <td>Some of the best hip-hop/R&amp;B yet to be release...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>239</td>\n",
       "      <td>best hip hop r b yet releas u hip hop truli ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42069</th>\n",
       "      <td>01 28, 2013</td>\n",
       "      <td>u84727986</td>\n",
       "      <td>This Cd is short &amp; sweet. It is really a quick...</td>\n",
       "      <td>Marvin Gaye: I want you</td>\n",
       "      <td>1359331200</td>\n",
       "      <td>Dance &amp; Electronic</td>\n",
       "      <td>$14.84</td>\n",
       "      <td>p23197423</td>\n",
       "      <td>24019223</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-01</td>\n",
       "      <td>14.84</td>\n",
       "      <td>Marvin Gaye: I want you This Cd is short &amp; swe...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>176</td>\n",
       "      <td>marvin gay want cd short sweet realli quick pl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39472 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        reviewTime reviewerID  \\\n",
       "32105  01 29, 2017  u67724906   \n",
       "14490  05 24, 2006  u10309608   \n",
       "16921  05 28, 2013  u04929059   \n",
       "39859   06 5, 2017  u20690354   \n",
       "25563   12 6, 2013  u63753846   \n",
       "...            ...        ...   \n",
       "20877  10 29, 2003  u83502050   \n",
       "6058   02 17, 2015  u72292741   \n",
       "22092  08 31, 2001  u13926491   \n",
       "34562   06 4, 2003  u11852145   \n",
       "42069  01 28, 2013  u84727986   \n",
       "\n",
       "                                              reviewText  \\\n",
       "32105  I liked it a lot when I first heard it, and it...   \n",
       "14490  The best Chili Peppers albums were Blood Sugar...   \n",
       "16921  When I saw the review and buy the songs. Expec...   \n",
       "39859                                            GREAT!!   \n",
       "25563  There is very little; this Lady can sing, that...   \n",
       "...                                                  ...   \n",
       "20877  I'm undoubtedly in the minority here, but this...   \n",
       "6058                                                like   \n",
       "22092  Bjork is back with \"Vespertine\".\\nA good effor...   \n",
       "34562  If hip-hop is truly where the mainstream music...   \n",
       "42069  This Cd is short & sweet. It is really a quick...   \n",
       "\n",
       "                                                 summary  unixReviewTime  \\\n",
       "32105  Dependable, authentic, great for those long dr...      1485648000   \n",
       "14490  I give it 4 stars because it's RHCP...but it's...      1148428800   \n",
       "16921                                          Few songs      1369699200   \n",
       "39859                                         Five Stars      1496620800   \n",
       "25563                                    A GREAT ONE....      1386288000   \n",
       "...                                                  ...             ...   \n",
       "20877   For me personally.....Underworlds finest work!!!      1067385600   \n",
       "6058                                          Four Stars      1424131200   \n",
       "22092  VESPERTINE DELIVERS ON COVER ART, BUT DON'T JU...       999216000   \n",
       "34562  Some of the best hip-hop/R&B yet to be release...      1054684800   \n",
       "42069                            Marvin Gaye: I want you      1359331200   \n",
       "\n",
       "                 category   price     itemID reviewHash image  ...  \\\n",
       "32105                 Pop  $12.51  p07525947   94030061   NaN  ...   \n",
       "14490                 Pop  $11.89  p36536384   32777266   NaN  ...   \n",
       "16921                 Pop   $8.99  p03487927   11500344   NaN  ...   \n",
       "39859                 Pop   $6.96  p36784389   87729782   NaN  ...   \n",
       "25563                 Pop   $9.10  p28779552   92947167   NaN  ...   \n",
       "...                   ...     ...        ...        ...   ...  ...   \n",
       "20877  Dance & Electronic  $11.97  p73092793   32251137   NaN  ...   \n",
       "6058   Dance & Electronic  $13.96  p87964435   31658850   NaN  ...   \n",
       "22092  Dance & Electronic  $18.29  p15146077   59635469   NaN  ...   \n",
       "34562  Dance & Electronic   $0.57  p22525318   94089341   NaN  ...   \n",
       "42069  Dance & Electronic  $14.84  p23197423   24019223   NaN  ...   \n",
       "\n",
       "      reviewMonthYear cleanedPrice  \\\n",
       "32105         2017-01        12.51   \n",
       "14490         2006-05        11.89   \n",
       "16921         2013-05         8.99   \n",
       "39859         2017-06         6.96   \n",
       "25563         2013-12         9.10   \n",
       "...               ...          ...   \n",
       "20877         2003-10        11.97   \n",
       "6058          2015-02        13.96   \n",
       "22092         2001-08        18.29   \n",
       "34562         2003-06         0.57   \n",
       "42069         2013-01        14.84   \n",
       "\n",
       "                                          fullReviewText isPop  \\\n",
       "32105  Dependable, authentic, great for those long dr...     1   \n",
       "14490  I give it 4 stars because it's RHCP...but it's...     1   \n",
       "16921  Few songs When I saw the review and buy the so...     1   \n",
       "39859                                 Five Stars GREAT!!     1   \n",
       "25563  A GREAT ONE.... There is very little; this Lad...     1   \n",
       "...                                                  ...   ...   \n",
       "20877  For me personally.....Underworlds finest work!...     0   \n",
       "6058                                     Four Stars like     0   \n",
       "22092  VESPERTINE DELIVERS ON COVER ART, BUT DON'T JU...     0   \n",
       "34562  Some of the best hip-hop/R&B yet to be release...     0   \n",
       "42069  Marvin Gaye: I want you This Cd is short & swe...     0   \n",
       "\n",
       "       isAlternativeRock isJazz  isClassical  isDanceElectronic  \\\n",
       "32105                  0      0            0                  0   \n",
       "14490                  0      0            0                  0   \n",
       "16921                  0      0            0                  0   \n",
       "39859                  0      0            0                  0   \n",
       "25563                  0      0            0                  0   \n",
       "...                  ...    ...          ...                ...   \n",
       "20877                  0      0            0                  1   \n",
       "6058                   0      0            0                  1   \n",
       "22092                  0      0            0                  1   \n",
       "34562                  0      0            0                  1   \n",
       "42069                  0      0            0                  1   \n",
       "\n",
       "       reviewWordCount                                    processedReview  \n",
       "32105               31  depend authent great long drive like lot first...  \n",
       "14490              515  give 4 star rhcp noth special best chili peppe...  \n",
       "16921               30  song saw review buy song expect bring action o...  \n",
       "39859                3                                    five star great  \n",
       "25563               25  great one littl ladi sing like immens show man...  \n",
       "...                ...                                                ...  \n",
       "20877              177  person underworld finest work undoubtedli mino...  \n",
       "6058                 3                                     four star like  \n",
       "22092              233  vespertin deliv cover art judg book bjork back...  \n",
       "34562              239  best hip hop r b yet releas u hip hop truli ma...  \n",
       "42069              176  marvin gay want cd short sweet realli quick pl...  \n",
       "\n",
       "[39472 rows x 23 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exclude_english = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "X_train['processedReview'] = X_train['fullReviewText'].apply(lambda x: process_review_text(x, exclude_english, ps))\n",
    "X_val['processedReview'] = X_val['fullReviewText'].apply(lambda x: process_review_text(x, exclude_english, ps))\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use a CountVectorizer to build counts of the 1500 most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=1500)\n",
    "X_train_cv = cv.fit_transform(X_train['processedReview'])\n",
    "X_val_cv = cv.transform(X_val['processedReview'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "X_train_reg_sp = sp.csr_matrix(X_train_reg)\n",
    "X_train_cv_reg = sp.hstack((X_train_cv, X_train_reg_sp), format='csr')\n",
    "\n",
    "X_val_reg_sp = sp.csr_matrix(X_val_reg)\n",
    "X_val_cv_reg = sp.hstack((X_val_cv, X_val_reg_sp), format='csr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will fit a few sample models to this dataset.\n",
    "\n",
    "First we will perform linear regression. In this case we use $L_{1}$ regularization as we have 1507 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha = 0\n",
      "------------\n",
      "Training Error: 0.9702267832256429\n",
      "Validation Error: 0.7343669218867732\n",
      "\n",
      "Alpha = 0.001\n",
      "------------\n",
      "Training Error: 1.0172938046370201\n",
      "Validation Error: 0.7084168824390358\n",
      "\n",
      "Alpha = 0.003\n",
      "------------\n",
      "Training Error: 1.0505511212466743\n",
      "Validation Error: 0.7278164470848788\n",
      "\n",
      "Alpha = 0.01\n",
      "------------\n",
      "Training Error: 0.7878701843021085\n",
      "Validation Error: 0.8028965298964436\n",
      "\n",
      "Alpha = 0.03\n",
      "------------\n",
      "Training Error: 0.9012085275848064\n",
      "Validation Error: 0.9024814384396381\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "\n",
    "print(\"Alpha = 0\")\n",
    "print(\"------------\")\n",
    "reg_model = LinearRegression()\n",
    "reg_model.fit(X_train_cv_reg, y_train)\n",
    "print(\"Training Error: {}\".format(calculate_MSE(y_train, vthreshold_rating(reg_model.predict(X_train_cv_reg)))))\n",
    "print(\"Validation Error: {}\".format(calculate_MSE(y_val, vthreshold_rating(reg_model.predict(X_val_cv_reg)))))\n",
    "print()\n",
    "\n",
    "alphas = [0.001, 0.003, 0.01, 0.03]\n",
    "for alpha in alphas:\n",
    "    print(\"Alpha = {}\".format(alpha))\n",
    "    print(\"------------\")\n",
    "    reg_model = Lasso(alpha=alpha)\n",
    "    reg_model.fit(X_train_cv_reg, y_train)\n",
    "    print(\"Training Error: {}\".format(calculate_MSE(y_train, vthreshold_rating(reg_model.predict(X_train_cv_reg)))))\n",
    "    print(\"Validation Error: {}\".format(calculate_MSE(y_val, vthreshold_rating(reg_model.predict(X_val_cv_reg)))))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A value of $\\alpha = 0.01$ seems to work the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error based on L2 regularized regression prediction: 0.800\n",
      "Validation error based on L2 regularized regression prediction: 0.775\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "reg_model = Lasso(alpha=0.01)\n",
    "reg_model.fit(X_train_cv_reg, y_train)\n",
    "\n",
    "train_MSE = calculate_MSE(y_train, vthreshold_rating(reg_model.predict(X_train_cv_reg)))\n",
    "val_MSE = calculate_MSE(y_val, vthreshold_rating(reg_model.predict(X_val_cv_reg)))\n",
    "\n",
    "model_names.append(\"CV-L1-Reg\")\n",
    "train_errors.append(train_MSE)\n",
    "validation_errors.append(val_MSE)\n",
    "\n",
    "print(\"Training error based on L2 regularized regression prediction: %.3f\" % train_MSE)\n",
    "print(\"Validation error based on L2 regularized regression prediction: %.3f\" % val_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now try a DecisionTreeRegressor and we will try various values for `min_samples_split`. This is the minimum number of samples required to split an internal node. Intuitively, lower values will correspond to higher variance and thus overfitting, whereas higher values will correspond to higher bias and thus overfitting. Obviously, this value show be at least 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples Split = 2\n",
      "-------------------\n",
      "Training Error: 0.0\n",
      "Validation Error: 1.3621169916434541\n",
      "\n",
      "Samples Split = 5\n",
      "-------------------\n",
      "Training Error: 0.061990582372566404\n",
      "Validation Error: 1.2772255986044287\n",
      "\n",
      "Samples Split = 10\n",
      "-------------------\n",
      "Training Error: 0.14434726168857676\n",
      "Validation Error: 1.1985976957818383\n",
      "\n",
      "Samples Split = 20\n",
      "-------------------\n",
      "Training Error: 0.21662396829778507\n",
      "Validation Error: 1.139970892726337\n",
      "\n",
      "Samples Split = 50\n",
      "-------------------\n",
      "Training Error: 0.3487511087905338\n",
      "Validation Error: 1.0140588194819098\n",
      "\n",
      "Samples Split = 100\n",
      "-------------------\n",
      "Training Error: 0.4179916778859212\n",
      "Validation Error: 0.9681598567080321\n",
      "\n",
      "Samples Split = 200\n",
      "-------------------\n",
      "Training Error: 0.4994314849422262\n",
      "Validation Error: 0.9277689865884085\n",
      "\n",
      "Samples Split = 500\n",
      "-------------------\n",
      "Training Error: 0.636519419002298\n",
      "Validation Error: 0.8577306502336626\n",
      "\n",
      "Samples Split = 1000\n",
      "-------------------\n",
      "Training Error: 0.7138154668215363\n",
      "Validation Error: 0.819981661067408\n",
      "\n",
      "Samples Split = 2000\n",
      "-------------------\n",
      "Training Error: 0.7808885440528472\n",
      "Validation Error: 0.8113020475243286\n",
      "\n",
      "Samples Split = 5000\n",
      "-------------------\n",
      "Training Error: 0.8167709375810198\n",
      "Validation Error: 0.831837848458731\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "samples_split_lst = [2, 5, 10, 20, 50, 100, 200, 500, 1000, 2000, 5000]\n",
    "\n",
    "for samples_split in samples_split_lst:\n",
    "    print(\"Samples Split = {}\".format(samples_split))\n",
    "    print(\"-------------------\")\n",
    "    tree_model = DecisionTreeRegressor(criterion=\"mse\", min_samples_split=samples_split)\n",
    "    tree_model.fit(X_train_cv_reg, y_train)\n",
    "    print(\"Training Error: {}\".format(calculate_MSE(y_train, vthreshold_rating(tree_model.predict(X_train_cv_reg)))))\n",
    "    print(\"Validation Error: {}\".format(calculate_MSE(y_val, vthreshold_rating(tree_model.predict(X_val_cv_reg)))))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The low values of `min_samples_split` clearly show overfitting as the training error is very low but with very high validation error. Conversely, once `min_samples_split` is past 1000 the validation error is not changing much and starts to increase. So we will stick with a value of 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error based on L2 regularized regression prediction: 0.666\n",
      "Validation error based on L2 regularized regression prediction: 0.785\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_model = DecisionTreeRegressor(criterion=\"mse\", min_samples_split=1000)\n",
    "tree_model.fit(X_train_cv_reg, y_train)\n",
    "\n",
    "train_MSE = calculate_MSE(y_train, vthreshold_rating(tree_model.predict(X_train_cv_reg)))\n",
    "val_MSE = calculate_MSE(y_val, vthreshold_rating(tree_model.predict(X_val_cv_reg)))\n",
    "\n",
    "model_names.append(\"CV-DecTree\")\n",
    "train_errors.append(train_MSE)\n",
    "validation_errors.append(val_MSE)\n",
    "\n",
    "print(\"Training error based on L2 regularized regression prediction: %.3f\" % train_MSE)\n",
    "print(\"Validation error based on L2 regularized regression prediction: %.3f\" % val_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging is a technique used to lower variance. One decision tree could have high variance if the fit is specific to one dataset. The random forest model is a form of bagging used with decision trees with the additional technique of splitting on a random subset of features. This additional technique is done to decorrelate the predictions.\n",
    "\n",
    "We now look at a random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator: 50, Samples Split: 100\n",
      "-----------------------------------\n",
      "Training Error: 0.4057792553637449\n",
      "Validation Error: 0.7191506672122061\n",
      "\n",
      "Estimator: 50, Samples Split: 200\n",
      "-----------------------------------\n",
      "Training Error: 0.49593356805206995\n",
      "Validation Error: 0.7205042054861818\n",
      "\n",
      "Estimator: 50, Samples Split: 500\n",
      "-----------------------------------\n",
      "Training Error: 0.6101750128150603\n",
      "Validation Error: 0.7439081543549444\n",
      "\n",
      "Estimator: 50, Samples Split: 1000\n",
      "-----------------------------------\n",
      "Training Error: 0.6899881867827375\n",
      "Validation Error: 0.762581111737615\n",
      "\n",
      "Estimator: 50, Samples Split: 2000\n",
      "-----------------------------------\n",
      "Training Error: 0.7681853361350132\n",
      "Validation Error: 0.7928475095683795\n",
      "\n",
      "Estimator: 50, Samples Split: 5000\n",
      "-----------------------------------\n",
      "Training Error: 0.8106714637644119\n",
      "Validation Error: 0.8281190868440167\n",
      "\n",
      "Estimator: 100, Samples Split: 100\n",
      "-----------------------------------\n",
      "Training Error: 0.40068404774153965\n",
      "Validation Error: 0.7104829754890318\n",
      "\n",
      "Estimator: 100, Samples Split: 200\n",
      "-----------------------------------\n",
      "Training Error: 0.4907224911474887\n",
      "Validation Error: 0.7187442338596703\n",
      "\n",
      "Estimator: 100, Samples Split: 500\n",
      "-----------------------------------\n",
      "Training Error: 0.6128223286903284\n",
      "Validation Error: 0.7401019339304276\n",
      "\n",
      "Estimator: 100, Samples Split: 1000\n",
      "-----------------------------------\n",
      "Training Error: 0.6959453669917869\n",
      "Validation Error: 0.7636468672485771\n",
      "\n",
      "Estimator: 100, Samples Split: 2000\n",
      "-----------------------------------\n",
      "Training Error: 0.7661419508532509\n",
      "Validation Error: 0.7924458019283732\n",
      "\n",
      "Estimator: 100, Samples Split: 5000\n",
      "-----------------------------------\n",
      "Training Error: 0.8097431641695165\n",
      "Validation Error: 0.8261113068333923\n",
      "\n",
      "Estimator: 200, Samples Split: 100\n",
      "-----------------------------------\n",
      "Training Error: 0.401822646937913\n",
      "Validation Error: 0.7092779780217924\n",
      "\n",
      "Estimator: 200, Samples Split: 200\n",
      "-----------------------------------\n",
      "Training Error: 0.48991620389254087\n",
      "Validation Error: 0.7168570925000571\n",
      "\n",
      "Estimator: 200, Samples Split: 500\n",
      "-----------------------------------\n",
      "Training Error: 0.6097315845678075\n",
      "Validation Error: 0.7371682272107951\n",
      "\n",
      "Estimator: 200, Samples Split: 1000\n",
      "-----------------------------------\n",
      "Training Error: 0.692667627693239\n",
      "Validation Error: 0.7618353501631314\n",
      "\n",
      "Estimator: 200, Samples Split: 2000\n",
      "-----------------------------------\n",
      "Training Error: 0.7632630588543882\n",
      "Validation Error: 0.7900874312158377\n",
      "\n",
      "Estimator: 200, Samples Split: 5000\n",
      "-----------------------------------\n",
      "Training Error: 0.8108590123286283\n",
      "Validation Error: 0.8261147040969983\n",
      "\n",
      "Estimator: 500, Samples Split: 100\n",
      "-----------------------------------\n",
      "Training Error: 0.40000786851064035\n",
      "Validation Error: 0.7100815485312582\n",
      "\n",
      "Estimator: 500, Samples Split: 200\n",
      "-----------------------------------\n",
      "Training Error: 0.48787299683558694\n",
      "Validation Error: 0.7181599459242168\n",
      "\n",
      "Estimator: 500, Samples Split: 500\n",
      "-----------------------------------\n",
      "Training Error: 0.6111872510648637\n",
      "Validation Error: 0.738590654682426\n",
      "\n",
      "Estimator: 500, Samples Split: 1000\n",
      "-----------------------------------\n",
      "Training Error: 0.6945480109838964\n",
      "Validation Error: 0.762267336514513\n",
      "\n",
      "Estimator: 500, Samples Split: 2000\n",
      "-----------------------------------\n",
      "Training Error: 0.7650835452233102\n",
      "Validation Error: 0.7912893760456803\n",
      "\n",
      "Estimator: 500, Samples Split: 5000\n",
      "-----------------------------------\n",
      "Training Error: 0.8113623863134298\n",
      "Validation Error: 0.8264942277141101\n",
      "\n",
      "Estimator: 1000, Samples Split: 100\n",
      "-----------------------------------\n",
      "Training Error: 0.40047858016807275\n",
      "Validation Error: 0.7105113332584745\n",
      "\n",
      "Estimator: 1000, Samples Split: 200\n",
      "-----------------------------------\n",
      "Training Error: 0.48827605594225154\n",
      "Validation Error: 0.7176040453678342\n",
      "\n",
      "Estimator: 1000, Samples Split: 500\n",
      "-----------------------------------\n",
      "Training Error: 0.6101806855925688\n",
      "Validation Error: 0.7374431390068514\n",
      "\n",
      "Estimator: 1000, Samples Split: 1000\n",
      "-----------------------------------\n",
      "Training Error: 0.6924366929879568\n",
      "Validation Error: 0.7612471729753861\n",
      "\n",
      "Estimator: 1000, Samples Split: 2000\n",
      "-----------------------------------\n",
      "Training Error: 0.7645434197846462\n",
      "Validation Error: 0.7904356952760528\n",
      "\n",
      "Estimator: 1000, Samples Split: 5000\n",
      "-----------------------------------\n",
      "Training Error: 0.8102686266608251\n",
      "Validation Error: 0.8257747460940099\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "estimator_lst = [50, 100, 200, 500, 1000]\n",
    "samples_split_lst = [100, 200, 500, 1000, 2000, 5000]\n",
    "for estimators in estimator_lst:\n",
    "    for samples_split in samples_split_lst:\n",
    "        print(\"Estimator: {0}, Samples Split: {1}\".format(estimators, samples_split))\n",
    "        print(\"-----------------------------------\")\n",
    "        forest_model = RandomForestRegressor(n_estimators=estimators, criterion='mse', min_samples_split=samples_split)\n",
    "        forest_model.fit(X_train_cv_reg, y_train)\n",
    "        print(\"Training Error: {}\".format(calculate_MSE(y_train, vthreshold_rating(forest_model.predict(X_train_cv_reg)))))\n",
    "        print(\"Validation Error: {}\".format(calculate_MSE(y_val, vthreshold_rating(forest_model.predict(X_val_cv_reg)))))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A value of `n_estimators = 200` and `min_samples_split = 200` seems to work quite well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error based on Random Forest Count Vectorizer: 0.455\n",
      "Validation error based on Random Forest Count Vectorizer: 0.672\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "forest_model = RandomForestRegressor(n_estimators=200, criterion=\"mse\", min_samples_split=200)\n",
    "forest_model.fit(X_train_cv_reg, y_train)\n",
    "\n",
    "train_MSE = calculate_MSE(y_train, vthreshold_rating(forest_model.predict(X_train_cv_reg)))\n",
    "val_MSE = calculate_MSE(y_val, vthreshold_rating(forest_model.predict(X_val_cv_reg)))\n",
    "\n",
    "model_names.append(\"CV-RanFor\")\n",
    "train_errors.append(train_MSE)\n",
    "validation_errors.append(val_MSE)\n",
    "\n",
    "print(\"Training error based on Random Forest Count Vectorizer: %.3f\" % train_MSE)\n",
    "print(\"Validation error based on Random Forest Count Vectorizer: %.3f\" % val_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.01, # Estimators: 10, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 12.647472443937666\n",
      "Validation Error: 12.663965560901493\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 10, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 12.647472443937666\n",
      "Validation Error: 12.663965560901493\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 10, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 12.647472443937666\n",
      "Validation Error: 12.663965560901493\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 10, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 12.647472443937666\n",
      "Validation Error: 12.663965560901493\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 6.567148287775521\n",
      "Validation Error: 6.580003963790851\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 6.544153143964236\n",
      "Validation Error: 6.56065481922039\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 6.489009557708315\n",
      "Validation Error: 6.518345559132838\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 6.407496993426535\n",
      "Validation Error: 6.4740687988898005\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 2.9901487626827925\n",
      "Validation Error: 2.997280183295985\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 2.955665559214561\n",
      "Validation Error: 2.969390545285835\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 2.864821186635007\n",
      "Validation Error: 2.9039411160544923\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 2.7266895147746224\n",
      "Validation Error: 2.8388691543135436\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 1.1833405975768483\n",
      "Validation Error: 1.1878432426758425\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 1.1342685473208902\n",
      "Validation Error: 1.148260311984346\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.9994783974420172\n",
      "Validation Error: 1.0568174599652562\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7942267083804493\n",
      "Validation Error: 0.9848332122925881\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8587281381645229\n",
      "Validation Error: 0.8682612795978688\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7826492700804814\n",
      "Validation Error: 0.8043459661000957\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5949771715180141\n",
      "Validation Error: 0.6943214954879069\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.33963908839835744\n",
      "Validation Error: 0.6415441977586364\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 10, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 9.301494382145071\n",
      "Validation Error: 9.317648402009524\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 10, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 9.285176348550658\n",
      "Validation Error: 9.305272759247766\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 10, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 9.245774955180867\n",
      "Validation Error: 9.272157484365914\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 10, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 9.190801162803165\n",
      "Validation Error: 9.23824234270152\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 1.6494084933811632\n",
      "Validation Error: 1.6540385313585833\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 1.6066302161838306\n",
      "Validation Error: 1.6204311636529316\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 1.490003520510351\n",
      "Validation Error: 1.5388617269028835\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 1.311206807415653\n",
      "Validation Error: 1.4648308225272368\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.9221693229361037\n",
      "Validation Error: 0.928834749040847\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8615492145413044\n",
      "Validation Error: 0.8781586422049799\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6997457816133467\n",
      "Validation Error: 0.7738055515253827\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.46978300959377306\n",
      "Validation Error: 0.7065774575467755\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8453871083055507\n",
      "Validation Error: 0.8564578133679281\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7647892214218582\n",
      "Validation Error: 0.7887792797132853\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5676347733638356\n",
      "Validation Error: 0.6784622232145064\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.3076972212644601\n",
      "Validation Error: 0.6337136042090994\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7765862511434555\n",
      "Validation Error: 0.7959215955868171\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6749711359336303\n",
      "Validation Error: 0.7154433576394665\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8513239579374129\n",
      "Validation Error: 0.6332206815170293\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6605219815025972\n",
      "Validation Error: 0.6149367300462054\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 10, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 2.798663898507412\n",
      "Validation Error: 2.805737176589574\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 10, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 2.7626860421374944\n",
      "Validation Error: 2.7761649359296996\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 10, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 2.6697737275019953\n",
      "Validation Error: 2.7108185201963395\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 10, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 2.531827584557739\n",
      "Validation Error: 2.6498053805112494\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8556514403502615\n",
      "Validation Error: 0.865558289879225\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7777855174710313\n",
      "Validation Error: 0.799741043653708\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5890083203410744\n",
      "Validation Error: 0.6927453727648355\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.3340381657554578\n",
      "Validation Error: 0.6470616678745068\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error: 0.8055230137896795\n",
      "Validation Error: 0.8218780722604903\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7115322855995211\n",
      "Validation Error: 0.7448900468672212\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8942100595464335\n",
      "Validation Error: 0.6493338358336637\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6788926897250729\n",
      "Validation Error: 0.6255026057516613\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.751404750011802\n",
      "Validation Error: 0.7742689567006416\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 1.0214747244393767\n",
      "Validation Error: 0.6964735118018198\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8118586088939567\n",
      "Validation Error: 0.6256850425025036\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6304320283795768\n",
      "Validation Error: 0.6192670685888096\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 1.0565057646015457\n",
      "Validation Error: 0.7191019470766012\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.9455213480299\n",
      "Validation Error: 0.6562634406474821\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7182313442290638\n",
      "Validation Error: 0.6200500637401734\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.05778540221775934\n",
      "Validation Error: 0.6268194656265644\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 10, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8933540808286927\n",
      "Validation Error: 0.9007905630211673\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 10, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8338048789987496\n",
      "Validation Error: 0.8493717731933074\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 10, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6744226995165054\n",
      "Validation Error: 0.7468224382673014\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 10, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.44534839003691734\n",
      "Validation Error: 0.7117827914014765\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7666354150150319\n",
      "Validation Error: 0.7864633509556488\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 1.0416825034840997\n",
      "Validation Error: 0.7076303080867425\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.824654757379957\n",
      "Validation Error: 0.6475593649521464\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6462054985430128\n",
      "Validation Error: 0.6684600059308923\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 1.0739262637780311\n",
      "Validation Error: 0.7421655466073782\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.9825161535537819\n",
      "Validation Error: 0.6684754686718043\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7465475737995693\n",
      "Validation Error: 0.6353390877407428\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5898897757506651\n",
      "Validation Error: 0.6712909920538754\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 1.0450399087799316\n",
      "Validation Error: 0.7075951751086953\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.9286076270112759\n",
      "Validation Error: 0.653270698512701\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.2564040229571133\n",
      "Validation Error: 0.6372899424122422\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.03612179246638209\n",
      "Validation Error: 0.6789267780388197\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 1.0081084505257822\n",
      "Validation Error: 0.685499782311178\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8479032053718485\n",
      "Validation Error: 0.6433497914392634\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.1277668513884838\n",
      "Validation Error: 0.6515969371824698\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.003163022918379842\n",
      "Validation Error: 0.6892775202209396\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 10, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8484102083536509\n",
      "Validation Error: 0.8596296491036213\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 10, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7714922391831222\n",
      "Validation Error: 0.7959135745000521\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 10, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5957665935604455\n",
      "Validation Error: 0.7050896544878358\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 10, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.35610057527503886\n",
      "Validation Error: 0.7080803628176755\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 1.077473710883061\n",
      "Validation Error: 0.7451524038624011\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.9883441023691879\n",
      "Validation Error: 0.680776634855113\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7893069808691245\n",
      "Validation Error: 0.6692813156829942\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.11265191655433023\n",
      "Validation Error: 0.7207155827264624\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 1.051754719371595\n",
      "Validation Error: 0.7105783233159061\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.9291144051691372\n",
      "Validation Error: 0.6663538601686161\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.699290510578994\n",
      "Validation Error: 0.6704443719736513\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.04675784657872768\n",
      "Validation Error: 0.7359142247152145\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 1.026669200557456\n",
      "Validation Error: 0.6912857997883357\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.88109717471177\n",
      "Validation Error: 0.6623548893002731\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6168123653870519\n",
      "Validation Error: 0.6827748603003342\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.010117268911033971\n",
      "Validation Error: 1.1197771587743732\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error: 0.978398581021158\n",
      "Validation Error: 0.6803343245180234\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7940580260990752\n",
      "Validation Error: 0.6603559222672145\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5364880273660205\n",
      "Validation Error: 0.699476446541734\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.00011779267994905767\n",
      "Validation Error: 1.1147125854646747\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "learning_rates = [0.01, 0.03, 0.1, 0.3, 0.5]\n",
    "estimators = [10, 50, 100, 200, 500]\n",
    "depths = [1, 2, 5, 10]\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for estimator in estimators:\n",
    "        for depth in depths:\n",
    "            print(\"Learning Rate: {0}, # Estimators: {1}, Depth: {2}\".format(learning_rate, estimator, depth))\n",
    "            print(\"--------------------------------------------------\")\n",
    "            xg_reg = XGBRegressor(\n",
    "                learning_rate=learning_rate, max_depth=depth, n_estimators=estimator)\n",
    "            xg_reg.fit(X_train_cv_reg, y_train)\n",
    "            print(\"Training Error: {}\".format(calculate_MSE(y_train, vthreshold_rating(xg_reg.predict(X_train_cv_reg)))))\n",
    "            print(\"Validation Error: {}\".format(calculate_MSE(y_val, vthreshold_rating(xg_reg.predict(X_val_cv_reg)))))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters `learning_rate=0.03`, `n_estimators=500` and `max_depth=10` seem to give really good performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error based on XGBoost CountVectorizer prediction: 0.240\n",
      "Validation error based on XGBoost CountVectorizer prediction: 0.578\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "xg_reg = XGBRegressor(learning_rate=0.03, n_estimators=500, max_depth=10)\n",
    "xg_reg.fit(X_train_cv_reg, y_train)\n",
    "\n",
    "train_MSE = calculate_MSE(y_train, vthreshold_rating(xg_reg.predict(X_train_cv_reg)))\n",
    "val_MSE = calculate_MSE(y_val, vthreshold_rating(xg_reg.predict(X_val_cv_reg)))\n",
    "\n",
    "model_names.append(\"CV-XGB\")\n",
    "train_errors.append(train_MSE)\n",
    "validation_errors.append(val_MSE)\n",
    "\n",
    "print(\"Training error based on XGBoost CountVectorizer prediction: %.3f\" % train_MSE)\n",
    "print(\"Validation error based on XGBoost CountVectorizer prediction: %.3f\" % val_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bigrams\n",
    "\n",
    "Note that the default for CountVectorizer is `ngram_range=(1,1)` which corresponds to unigrams (single words). We are now going to look at bigrams (collections of 2 words). The reason is we could have a sentence such as \"the song was well done and not generic\" or a sentence like \"the song was not well done and generic\". These two sentences mean the same thing but the first sentence would correspond to liking the song whereas the second would correspond to disliking the song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(2,2))\n",
    "X_train_cv = cv.fit_transform(X_train['processedReview'])\n",
    "X_val_cv = cv.transform(X_val['processedReview'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reg_sp = sp.csr_matrix(X_train_reg)\n",
    "X_train_cv_reg = sp.hstack((X_train_cv, X_train_reg_sp), format='csr')\n",
    "\n",
    "X_val_reg_sp = sp.csr_matrix(X_val_reg)\n",
    "X_val_cv_reg = sp.hstack((X_val_cv, X_val_reg_sp), format='csr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we will start with a linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha = 0\n",
      "------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'LinearRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-aaf687f42409>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Alpha = 0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mreg_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mreg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_cv_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Error: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalculate_MSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvthreshold_rating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_cv_reg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LinearRegression' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Alpha = 0\")\n",
    "print(\"------------\")\n",
    "reg_model = LinearRegression()\n",
    "reg_model.fit(X_train_cv_reg, y_train)\n",
    "print(\"Training Error: {}\".format(calculate_MSE(y_train, vthreshold_rating(reg_model.predict(X_train_cv_reg)))))\n",
    "print(\"Validation Error: {}\".format(calculate_MSE(y_val, vthreshold_rating(reg_model.predict(X_val_cv_reg)))))\n",
    "print()\n",
    "\n",
    "alphas = [0.001, 0.003, 0.01, 0.03]\n",
    "for alpha in alphas:\n",
    "    print(\"Alpha = {}\".format(alpha))\n",
    "    print(\"------------\")\n",
    "    reg_model = Lasso(alpha=alpha)\n",
    "    reg_model.fit(X_train_cv_reg, y_train)\n",
    "    print(\"Training Error: {}\".format(calculate_MSE(y_train, vthreshold_rating(reg_model.predict(X_train_cv_reg)))))\n",
    "    print(\"Validation Error: {}\".format(calculate_MSE(y_val, vthreshold_rating(reg_model.predict(X_val_cv_reg)))))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A parameter of `alpha = 0.003` seems to work best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error based on bigram L1 regularized regression prediction: 0.858\n",
      "Validation error based on bigram L1 regularized regression prediction: 0.824\n"
     ]
    }
   ],
   "source": [
    "reg_model = Lasso(alpha=0.003)\n",
    "reg_model.fit(X_train_cv_reg, y_train)\n",
    "\n",
    "train_MSE = calculate_MSE(y_train, vthreshold_rating(reg_model.predict(X_train_cv_reg)))\n",
    "val_MSE = calculate_MSE(y_val, vthreshold_rating(reg_model.predict(X_val_cv_reg)))\n",
    "\n",
    "model_names.append(\"bigram-L1-Reg\")\n",
    "train_errors.append(train_MSE)\n",
    "validation_errors.append(val_MSE)\n",
    "\n",
    "print(\"Training error based on bigram L1 regularized regression prediction: %.3f\" % train_MSE)\n",
    "print(\"Validation error based on bigram L1 regularized regression prediction: %.3f\" % val_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we look at a DecisionTree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples Split = 2\n",
      "-------------------\n",
      "Training Error: 0.00034840998352970987\n",
      "Validation Error: 1.5720435553304635\n",
      "\n",
      "Samples Split = 5\n",
      "-------------------\n",
      "Training Error: 0.14395667046750285\n",
      "Validation Error: 1.380092709828086\n",
      "\n",
      "Samples Split = 10\n",
      "-------------------\n",
      "Training Error: 0.2517588922472997\n",
      "Validation Error: 1.2500459664418744\n",
      "\n",
      "Samples Split = 20\n",
      "-------------------\n",
      "Training Error: 0.334875317575188\n",
      "Validation Error: 1.2045591036207899\n",
      "\n",
      "Samples Split = 50\n",
      "-------------------\n",
      "Training Error: 0.38170810626141705\n",
      "Validation Error: 1.1547422191619612\n",
      "\n",
      "Samples Split = 100\n",
      "-------------------\n",
      "Training Error: 0.44518864141093517\n",
      "Validation Error: 1.12558213619203\n",
      "\n",
      "Samples Split = 200\n",
      "-------------------\n",
      "Training Error: 0.47686615493204587\n",
      "Validation Error: 1.1039643057616346\n",
      "\n",
      "Samples Split = 500\n",
      "-------------------\n",
      "Training Error: 0.5870212534753286\n",
      "Validation Error: 0.9927061595309912\n",
      "\n",
      "Samples Split = 1000\n",
      "-------------------\n",
      "Training Error: 0.6849479740741126\n",
      "Validation Error: 0.8984856186346457\n",
      "\n",
      "Samples Split = 2000\n",
      "-------------------\n",
      "Training Error: 0.7498397370548485\n",
      "Validation Error: 0.869769100221819\n",
      "\n",
      "Samples Split = 5000\n",
      "-------------------\n",
      "Training Error: 0.8402633451452788\n",
      "Validation Error: 0.8584647854381274\n",
      "\n",
      "Samples Split = 10000\n",
      "-------------------\n",
      "Training Error: 0.8665349492532685\n",
      "Validation Error: 0.876568959513198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samples_split_lst = [2, 5, 10, 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000]\n",
    "\n",
    "for samples_split in samples_split_lst:\n",
    "    print(\"Samples Split = {}\".format(samples_split))\n",
    "    print(\"-------------------\")\n",
    "    tree_model = DecisionTreeRegressor(criterion=\"mse\", min_samples_split=samples_split)\n",
    "    tree_model.fit(X_train_cv_reg, y_train)\n",
    "    print(\"Training Error: {}\".format(calculate_MSE(y_train, vthreshold_rating(tree_model.predict(X_train_cv_reg)))))\n",
    "    print(\"Validation Error: {}\".format(calculate_MSE(y_val, vthreshold_rating(tree_model.predict(X_val_cv_reg)))))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `min_samples_split=5000` seems to work best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error based on bigram decision tree prediction: 0.651\n",
      "Validation error based on bigram decision tree prediction: 0.829\n"
     ]
    }
   ],
   "source": [
    "tree_model = DecisionTreeRegressor(criterion=\"mse\", min_samples_split=5000)\n",
    "tree_model.fit(X_train_cv_reg, y_train)\n",
    "\n",
    "train_MSE = calculate_MSE(y_train, vthreshold_rating(tree_model.predict(X_train_cv_reg)))\n",
    "val_MSE = calculate_MSE(y_val, vthreshold_rating(tree_model.predict(X_val_cv_reg)))\n",
    "\n",
    "model_names.append(\"bigram-DecTree\")\n",
    "train_errors.append(train_MSE)\n",
    "validation_errors.append(val_MSE)\n",
    "\n",
    "print(\"Training error based on bigram decision tree prediction: %.3f\" % train_MSE)\n",
    "print(\"Validation error based on bigram decision tree prediction: %.3f\" % val_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we try RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator: 50, Samples Split: 100\n",
      "-----------------------------------\n",
      "Training Error: 0.44141249668554217\n",
      "Validation Error: 0.8757471183270643\n",
      "\n",
      "Estimator: 50, Samples Split: 200\n",
      "-----------------------------------\n",
      "Training Error: 0.4928900322931837\n",
      "Validation Error: 0.8629796406063882\n",
      "\n",
      "Estimator: 50, Samples Split: 500\n",
      "-----------------------------------\n",
      "Training Error: 0.6075063573811836\n",
      "Validation Error: 0.8386345191945126\n",
      "\n",
      "Estimator: 50, Samples Split: 1000\n",
      "-----------------------------------\n",
      "Training Error: 0.6761420982965163\n",
      "Validation Error: 0.8286892342528405\n",
      "\n",
      "Estimator: 50, Samples Split: 2000\n",
      "-----------------------------------\n",
      "Training Error: 0.7690676434275397\n",
      "Validation Error: 0.8279859307599594\n",
      "\n",
      "Estimator: 50, Samples Split: 5000\n",
      "-----------------------------------\n",
      "Training Error: 0.8431295781666054\n",
      "Validation Error: 0.8595007183466646\n",
      "\n",
      "Estimator: 100, Samples Split: 100\n",
      "-----------------------------------\n",
      "Training Error: 0.43739480862845315\n",
      "Validation Error: 0.8728051764634944\n",
      "\n",
      "Estimator: 100, Samples Split: 200\n",
      "-----------------------------------\n",
      "Training Error: 0.4852944985193507\n",
      "Validation Error: 0.860344690036123\n",
      "\n",
      "Estimator: 100, Samples Split: 500\n",
      "-----------------------------------\n",
      "Training Error: 0.6070769035152972\n",
      "Validation Error: 0.838525807908505\n",
      "\n",
      "Estimator: 100, Samples Split: 1000\n",
      "-----------------------------------\n",
      "Training Error: 0.6765440999953442\n",
      "Validation Error: 0.8283588185603716\n",
      "\n",
      "Estimator: 100, Samples Split: 2000\n",
      "-----------------------------------\n",
      "Training Error: 0.7616894258874312\n",
      "Validation Error: 0.8262797242183522\n",
      "\n",
      "Estimator: 100, Samples Split: 5000\n",
      "-----------------------------------\n",
      "Training Error: 0.8438783965799213\n",
      "Validation Error: 0.8588044856497253\n",
      "\n",
      "Estimator: 200, Samples Split: 100\n",
      "-----------------------------------\n",
      "Training Error: 0.4385655575607059\n",
      "Validation Error: 0.8711848032611039\n",
      "\n",
      "Estimator: 200, Samples Split: 200\n",
      "-----------------------------------\n",
      "Training Error: 0.48926147293297567\n",
      "Validation Error: 0.8567604834341185\n",
      "\n",
      "Estimator: 200, Samples Split: 500\n",
      "-----------------------------------\n",
      "Training Error: 0.6052961455025166\n",
      "Validation Error: 0.8378456027338266\n",
      "\n",
      "Estimator: 200, Samples Split: 1000\n",
      "-----------------------------------\n",
      "Training Error: 0.6738117131593807\n",
      "Validation Error: 0.8251510560333009\n",
      "\n",
      "Estimator: 200, Samples Split: 2000\n",
      "-----------------------------------\n",
      "Training Error: 0.7619620133174893\n",
      "Validation Error: 0.8273633849261925\n",
      "\n",
      "Estimator: 200, Samples Split: 5000\n",
      "-----------------------------------\n",
      "Training Error: 0.8436583885673791\n",
      "Validation Error: 0.8586420745471393\n",
      "\n",
      "Estimator: 500, Samples Split: 100\n",
      "-----------------------------------\n",
      "Training Error: 0.43431474927310293\n",
      "Validation Error: 0.8674254319173978\n",
      "\n",
      "Estimator: 500, Samples Split: 200\n",
      "-----------------------------------\n",
      "Training Error: 0.4854529140450475\n",
      "Validation Error: 0.8559538010361591\n",
      "\n",
      "Estimator: 500, Samples Split: 500\n",
      "-----------------------------------\n",
      "Training Error: 0.6035984472090887\n",
      "Validation Error: 0.8372175103131482\n",
      "\n",
      "Estimator: 500, Samples Split: 1000\n",
      "-----------------------------------\n",
      "Training Error: 0.6747665508380223\n",
      "Validation Error: 0.8260556501137705\n",
      "\n",
      "Estimator: 500, Samples Split: 2000\n",
      "-----------------------------------\n",
      "Training Error: 0.7601881352467567\n",
      "Validation Error: 0.8264982385962116\n",
      "\n",
      "Estimator: 500, Samples Split: 5000\n",
      "-----------------------------------\n",
      "Training Error: 0.8436685368103953\n",
      "Validation Error: 0.8587058432862466\n",
      "\n",
      "Estimator: 1000, Samples Split: 100\n",
      "-----------------------------------\n",
      "Training Error: 0.434175739058689\n",
      "Validation Error: 0.86806118815067\n",
      "\n",
      "Estimator: 1000, Samples Split: 200\n",
      "-----------------------------------\n",
      "Training Error: 0.48573138666858484\n",
      "Validation Error: 0.8550892744326247\n",
      "\n",
      "Estimator: 1000, Samples Split: 500\n",
      "-----------------------------------\n",
      "Training Error: 0.6015717463678933\n",
      "Validation Error: 0.8363747775093775\n",
      "\n",
      "Estimator: 1000, Samples Split: 1000\n",
      "-----------------------------------\n",
      "Training Error: 0.6735740174856352\n",
      "Validation Error: 0.8261994149329676\n",
      "\n",
      "Estimator: 1000, Samples Split: 2000\n",
      "-----------------------------------\n",
      "Training Error: 0.7618498817324086\n",
      "Validation Error: 0.8270195945241967\n",
      "\n",
      "Estimator: 1000, Samples Split: 5000\n",
      "-----------------------------------\n",
      "Training Error: 0.8428364969871297\n",
      "Validation Error: 0.8583613085602138\n",
      "\n"
     ]
    }
   ],
   "source": [
    "estimator_lst = [50, 100, 200, 500, 1000]\n",
    "samples_split_lst = [100, 200, 500, 1000, 2000, 5000]\n",
    "for estimators in estimator_lst:\n",
    "    for samples_split in samples_split_lst:\n",
    "        print(\"Estimator: {0}, Samples Split: {1}\".format(estimators, samples_split))\n",
    "        print(\"-----------------------------------\")\n",
    "        forest_model = RandomForestRegressor(n_estimators=estimators, criterion='mse', min_samples_split=samples_split)\n",
    "        forest_model.fit(X_train_cv_reg, y_train)\n",
    "        print(\"Training Error: {}\".format(calculate_MSE(y_train, vthreshold_rating(forest_model.predict(X_train_cv_reg)))))\n",
    "        print(\"Validation Error: {}\".format(calculate_MSE(y_val, vthreshold_rating(forest_model.predict(X_val_cv_reg)))))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the number of estimators doesn't seem to make much of a difference but `min_samples_split=2000` seems to work best. In order to keep it simple we will use 50 estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_model = RandomForestRegressor(n_estimators=50, criterion=\"mse\", min_samples_split=2000)\n",
    "forest_model.fit(X_train_cv_reg, y_train)\n",
    "\n",
    "train_MSE = calculate_MSE(y_train, vthreshold_rating(forest_model.predict(X_train_cv_reg)))\n",
    "val_MSE = calculate_MSE(y_val, vthreshold_rating(forest_model.predict(X_val_cv_reg)))\n",
    "\n",
    "model_names.append(\"bigram-RanFor\")\n",
    "train_errors.append(train_MSE)\n",
    "validation_errors.append(val_MSE)\n",
    "\n",
    "print(\"Training error based on Random Forest Count Vectorizer: %.3f\" % train_MSE)\n",
    "print(\"Validation error based on Random Forest Count Vectorizer: %.3f\" % val_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we will look at XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.01, # Estimators: 10, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 12.647472443937666\n",
      "Validation Error: 12.663965560901493\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 10, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 12.647472443937666\n",
      "Validation Error: 12.663965560901493\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 10, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 12.647472443937666\n",
      "Validation Error: 12.663965560901493\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 10, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 12.647472443937666\n",
      "Validation Error: 12.663965560901493\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 6.568249772343689\n",
      "Validation Error: 6.57867399079515\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 6.553732014268306\n",
      "Validation Error: 6.56926981656117\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 6.520501861267711\n",
      "Validation Error: 6.537234837588246\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 6.48877814590461\n",
      "Validation Error: 6.519742490565927\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 2.996846949754432\n",
      "Validation Error: 3.001324663432204\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 2.970583932395855\n",
      "Validation Error: 2.9809681656634366\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 2.921089815797185\n",
      "Validation Error: 2.9413764214446037\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 2.8788864145373814\n",
      "Validation Error: 2.9207742088425936\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 1.1961030664892458\n",
      "Validation Error: 1.1964846081575515\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 1.1624511409746818\n",
      "Validation Error: 1.1700775819371108\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 1.0975333787400623\n",
      "Validation Error: 1.1250159191325066\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 1.0196615503539956\n",
      "Validation Error: 1.1071038405256337\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8858874696995472\n",
      "Validation Error: 0.8883024696792284\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8480348783929679\n",
      "Validation Error: 0.8606079772841089\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7650547540739293\n",
      "Validation Error: 0.8200923282122435\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6538328982177833\n",
      "Validation Error: 0.8052240508139764\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 10, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 9.300685639564719\n",
      "Validation Error: 9.31446294659091\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 10, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 9.291473749146041\n",
      "Validation Error: 9.309354793749737\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 10, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 9.267054304370431\n",
      "Validation Error: 9.28481387279064\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 10, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 9.244398243305781\n",
      "Validation Error: 9.26973983946715\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 1.6596652215968315\n",
      "Validation Error: 1.6612141292278304\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 1.6280221406489404\n",
      "Validation Error: 1.6363141889230624\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 1.5689911476652711\n",
      "Validation Error: 1.5927445014733237\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 1.506476407158294\n",
      "Validation Error: 1.5721484764563676\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.9395090212231147\n",
      "Validation Error: 0.9397103358823248\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.9039707143489066\n",
      "Validation Error: 0.9118301976390338\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8309300713345343\n",
      "Validation Error: 0.8710390888448147\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7360238101304339\n",
      "Validation Error: 0.8530677041977491\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8777152605462512\n",
      "Validation Error: 0.8813104878548743\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8384054287503782\n",
      "Validation Error: 0.853370574293734\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7511740688315276\n",
      "Validation Error: 0.8143848429100338\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6330454592559785\n",
      "Validation Error: 0.8007161589740833\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8415482506719454\n",
      "Validation Error: 0.8536661658640461\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.793070637889461\n",
      "Validation Error: 0.8248266230747211\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6793623365762663\n",
      "Validation Error: 0.7910987597539298\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5250057585677567\n",
      "Validation Error: 0.7886760620711442\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 10, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 2.80570422001846\n",
      "Validation Error: 2.809442961866078\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 10, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 2.7791002325211176\n",
      "Validation Error: 2.7887780641705238\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 10, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 2.729972646863467\n",
      "Validation Error: 2.74921866398562\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 10, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 2.6862617071113375\n",
      "Validation Error: 2.7322042642539053\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8838681714175515\n",
      "Validation Error: 0.8861309041470888\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8456278557005781\n",
      "Validation Error: 0.8586624641289545\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7629058244391043\n",
      "Validation Error: 0.8207570524288279\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6465846936887054\n",
      "Validation Error: 0.805920320498846\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error: 0.8562371391738242\n",
      "Validation Error: 0.8646289936763091\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8122424035463263\n",
      "Validation Error: 0.8363016979402929\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7130391586430229\n",
      "Validation Error: 0.8009448747730243\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5694270929171044\n",
      "Validation Error: 0.7913095545411614\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8286290055102565\n",
      "Validation Error: 0.8435263055403349\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7766458375238707\n",
      "Validation Error: 0.8153344332339758\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6497155343269144\n",
      "Validation Error: 0.7880417397078446\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4744682101247222\n",
      "Validation Error: 0.7931897327179802\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7911746904066147\n",
      "Validation Error: 0.815860537847195\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 1.0419358925630304\n",
      "Validation Error: 0.7907374455638793\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.897820853921196\n",
      "Validation Error: 0.7807671177710455\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7462941847206386\n",
      "Validation Error: 0.8067513823360581\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 10, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.9120715067362042\n",
      "Validation Error: 0.9114504241465038\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 10, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8754859528324639\n",
      "Validation Error: 0.8819957496048141\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 10, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8025755392890344\n",
      "Validation Error: 0.8487888079987187\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 10, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7069688262514805\n",
      "Validation Error: 0.8364955187596234\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8362801211537444\n",
      "Validation Error: 0.8489094558749795\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.78848559421501\n",
      "Validation Error: 0.8220467399215405\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.669322233672213\n",
      "Validation Error: 0.7937185301996843\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.505242484934157\n",
      "Validation Error: 0.804080999376571\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8083925003018094\n",
      "Validation Error: 0.828180683198275\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 1.0596731280881795\n",
      "Validation Error: 0.8043062787530002\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5931705403215752\n",
      "Validation Error: 0.7828694404961849\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.39868138785658486\n",
      "Validation Error: 0.8104088708913932\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 1.0879260103889523\n",
      "Validation Error: 0.8091301696350843\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 1.026859242366654\n",
      "Validation Error: 0.791145108075924\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8626631192195616\n",
      "Validation Error: 0.7856274977805955\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.2716816364365224\n",
      "Validation Error: 0.8398959713564337\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 1.0592296972000508\n",
      "Validation Error: 0.7892873911242945\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.9601545673381477\n",
      "Validation Error: 0.7805777126460698\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7457874065627772\n",
      "Validation Error: 0.8112712151047892\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.11315861111202376\n",
      "Validation Error: 0.9065036964982109\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 10, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8760741523421297\n",
      "Validation Error: 0.8820792152673994\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 10, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8379163165809472\n",
      "Validation Error: 0.851980167100397\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 10, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7587971465792839\n",
      "Validation Error: 0.8303414867948182\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 10, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6489489126602492\n",
      "Validation Error: 0.8261409686395439\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8123589297215374\n",
      "Validation Error: 0.8312023385766004\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 1.0707589002913975\n",
      "Validation Error: 0.8104105132276471\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6189340386647613\n",
      "Validation Error: 0.7950063187894878\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.42301087374196855\n",
      "Validation Error: 0.8271937812813082\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7838152678194523\n",
      "Validation Error: 0.8095744866952468\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 1.035031040162169\n",
      "Validation Error: 0.7945302364669834\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8793867984289877\n",
      "Validation Error: 0.8026468682343552\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7293171164322818\n",
      "Validation Error: 0.8445561897338113\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 1.0701254275940706\n",
      "Validation Error: 0.7944208232166641\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.9920815912834157\n",
      "Validation Error: 0.7882180933366503\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7894336754085899\n",
      "Validation Error: 0.815263559470223\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6170657544659826\n",
      "Validation Error: 0.8782669626684744\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error: 1.0441530470036742\n",
      "Validation Error: 0.7848066543164833\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.9222095527682757\n",
      "Validation Error: 0.7876996121689137\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6551374635753199\n",
      "Validation Error: 0.8663779028219787\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.513429621183327\n",
      "Validation Error: 1.2770321600405166\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "learning_rates = [0.01, 0.03, 0.1, 0.3, 0.5]\n",
    "estimators = [10, 50, 100, 200, 500]\n",
    "depths = [1, 2, 5, 10]\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for estimator in estimators:\n",
    "        for depth in depths:\n",
    "            print(\"Learning Rate: {0}, # Estimators: {1}, Depth: {2}\".format(learning_rate, estimator, depth))\n",
    "            print(\"--------------------------------------------------\")\n",
    "            xg_reg = XGBRegressor(\n",
    "                learning_rate=learning_rate, max_depth=depth, n_estimators=estimator)\n",
    "            xg_reg.fit(X_train_cv_reg, y_train)\n",
    "            print(\"Training Error: {}\".format(calculate_MSE(y_train, vthreshold_rating(xg_reg.predict(X_train_cv_reg)))))\n",
    "            print(\"Validation Error: {}\".format(calculate_MSE(y_val, vthreshold_rating(xg_reg.predict(X_val_cv_reg)))))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters `learning_rate=0.1`, `n_estimators=200` and `max_depth=5` seem to work best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error based on XGBoost CountVectorizer prediction: 0.682\n",
      "Validation error based on XGBoost CountVectorizer prediction: 0.726\n"
     ]
    }
   ],
   "source": [
    "xg_reg = XGBRegressor(learning_rate=0.1, n_estimators=200, max_depth=5)\n",
    "xg_reg.fit(X_train_cv_reg, y_train)\n",
    "\n",
    "train_MSE = calculate_MSE(y_train, vthreshold_rating(xg_reg.predict(X_train_cv_reg)))\n",
    "val_MSE = calculate_MSE(y_val, vthreshold_rating(xg_reg.predict(X_val_cv_reg)))\n",
    "\n",
    "model_names.append(\"bigram-XGB\")\n",
    "train_errors.append(train_MSE)\n",
    "validation_errors.append(val_MSE)\n",
    "\n",
    "print(\"Training error based on XGBoost CountVectorizer prediction: %.3f\" % train_MSE)\n",
    "print(\"Validation error based on XGBoost CountVectorizer prediction: %.3f\" % val_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TF-IDF\n",
    "\n",
    "We now look at some Term Frequency, Inverse Document Frequency models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf.fit_transform(X_train['processedReview'])\n",
    "X_val_tfidf = tfidf.transform(X_val['processedReview'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reg_sp = sp.csr_matrix(X_train_reg)\n",
    "X_train_tfidf_reg = sp.hstack((X_train_tfidf, X_train_reg_sp), format='csr')\n",
    "\n",
    "X_val_reg_sp = sp.csr_matrix(X_val_reg)\n",
    "X_val_tfidf_reg = sp.hstack((X_val_tfidf, X_val_reg_sp), format='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha = 0\n",
      "------------\n",
      "Training Error: 0.48764728240212846\n",
      "Validation Error: 4.35882501899215\n",
      "\n",
      "Alpha = 0.0001\n",
      "------------\n",
      "Training Error: 0.4971762425824536\n",
      "Validation Error: 0.5943364603053036\n",
      "\n",
      "Alpha = 0.0003\n",
      "------------\n",
      "Training Error: 0.6132796048231367\n",
      "Validation Error: 0.6367738258983342\n",
      "\n",
      "Alpha = 0.001\n",
      "------------\n",
      "Training Error: 0.7396541286275682\n",
      "Validation Error: 0.7446951232339728\n",
      "\n",
      "Alpha = 0.003\n",
      "------------\n",
      "Training Error: 0.8926590888416531\n",
      "Validation Error: 0.8860759005064034\n",
      "\n",
      "Alpha = 0.01\n",
      "------------\n",
      "Training Error: 0.9449249189212761\n",
      "Validation Error: 0.9401356110678446\n",
      "\n",
      "Alpha = 0.03\n",
      "------------\n",
      "Training Error: 0.9829243379548028\n",
      "Validation Error: 0.9757225376331816\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Alpha = 0\")\n",
    "print(\"------------\")\n",
    "reg_model = LinearRegression()\n",
    "reg_model.fit(X_train_tfidf_reg, y_train)\n",
    "print(\"Training Error: {}\".format(calculate_MSE(y_train, vthreshold_rating(reg_model.predict(X_train_tfidf_reg)))))\n",
    "print(\"Validation Error: {}\".format(calculate_MSE(y_val, vthreshold_rating(reg_model.predict(X_val_tfidf_reg)))))\n",
    "print()\n",
    "\n",
    "alphas = [0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03]\n",
    "for alpha in alphas:\n",
    "    print(\"Alpha = {}\".format(alpha))\n",
    "    print(\"------------\")\n",
    "    reg_model = Lasso(alpha=alpha)\n",
    "    reg_model.fit(X_train_tfidf_reg, y_train)\n",
    "    print(\"Training Error: {}\".format(calculate_MSE(y_train, vthreshold_rating(reg_model.predict(X_train_tfidf_reg)))))\n",
    "    print(\"Validation Error: {}\".format(calculate_MSE(y_val, vthreshold_rating(reg_model.predict(X_val_tfidf_reg)))))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very low values of the regularization parameter seems to correspond to overfitting. So we will use $\\alpha = 0.0003$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model = Lasso(alpha=0.0003)\n",
    "reg_model.fit(X_train_tfidf_reg, y_train)\n",
    "\n",
    "train_MSE = calculate_MSE(y_train, vthreshold_rating(reg_model.predict(X_train_tfidf_reg)))\n",
    "val_MSE = calculate_MSE(y_val, vthreshold_rating(reg_model.predict(X_val_tfidf_reg)))\n",
    "\n",
    "model_names.append(\"TFIDF-L1-Reg\")\n",
    "train_errors.append(train_MSE)\n",
    "validation_errors.append(val_MSE)\n",
    "\n",
    "print(\"Training error based on TF-IDF L1 regularized regression prediction: %.3f\" % train_MSE)\n",
    "print(\"Validation error based on TF-IDF L1 regularized regression prediction: %.3f\" % val_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we look at some decision trees. As very low values of `min_samples_split` corresponded to overfitting in simpler models, low values will also correspond to overfitting in this model. So we will start with higher values. This makes sense as our prediction is in the range [1, 5] and so there shouldn't be a lot of leaf nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples Split = 10\n",
      "-------------------\n",
      "Training Error: 0.09078405317953019\n",
      "Validation Error: 1.1542923660840494\n",
      "\n",
      "Samples Split = 20\n",
      "-------------------\n",
      "Training Error: 0.1481975958452695\n",
      "Validation Error: 1.1519107693805148\n",
      "\n",
      "Samples Split = 50\n",
      "-------------------\n",
      "Training Error: 0.2172065901834366\n",
      "Validation Error: 1.0530955269682027\n",
      "\n",
      "Samples Split = 100\n",
      "-------------------\n",
      "Training Error: 0.2646238230552557\n",
      "Validation Error: 1.0479410075498214\n",
      "\n",
      "Samples Split = 200\n",
      "-------------------\n",
      "Training Error: 0.3291466366217442\n",
      "Validation Error: 1.0350827073757387\n",
      "\n",
      "Samples Split = 500\n",
      "-------------------\n",
      "Training Error: 0.42699768973783386\n",
      "Validation Error: 0.9471850793626092\n",
      "\n",
      "Samples Split = 1000\n",
      "-------------------\n",
      "Training Error: 0.5308267222973465\n",
      "Validation Error: 0.8943621767262115\n",
      "\n",
      "Samples Split = 2000\n",
      "-------------------\n",
      "Training Error: 0.6517068542407606\n",
      "Validation Error: 0.863809577630791\n",
      "\n",
      "Samples Split = 5000\n",
      "-------------------\n",
      "Training Error: 0.7570083021109453\n",
      "Validation Error: 0.8301158747418578\n",
      "\n",
      "Samples Split = 10000\n",
      "-------------------\n",
      "Training Error: 0.8180196272299626\n",
      "Validation Error: 0.8484371575293123\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samples_split_lst = [10, 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000]\n",
    "\n",
    "for samples_split in samples_split_lst:\n",
    "    print(\"Samples Split = {}\".format(samples_split))\n",
    "    print(\"-------------------\")\n",
    "    tree_model = DecisionTreeRegressor(criterion=\"mse\", min_samples_split=samples_split)\n",
    "    tree_model.fit(X_train_tfidf_reg, y_train)\n",
    "    print(\"Training Error: {}\".format(calculate_MSE(y_train, vthreshold_rating(tree_model.predict(X_train_tfidf_reg)))))\n",
    "    print(\"Validation Error: {}\".format(calculate_MSE(y_val, vthreshold_rating(tree_model.predict(X_val_tfidf_reg)))))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems a value of `min_samples_split=5000` works the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model = DecisionTreeRegressor(criterion=\"mse\", min_samples_split=5000)\n",
    "tree_model.fit(X_train_tfidf_reg, y_train)\n",
    "\n",
    "train_MSE = calculate_MSE(y_train, vthreshold_rating(tree_model.predict(X_train_tfidf_reg)))\n",
    "val_MSE = calculate_MSE(y_val, vthreshold_rating(tree_model.predict(X_val_tfidf_reg)))\n",
    "\n",
    "model_names.append(\"TFIDF-DecTree\")\n",
    "train_errors.append(train_MSE)\n",
    "validation_errors.append(val_MSE)\n",
    "\n",
    "print(\"Training error based on TF-IDF decision tree prediction: %.3f\" % train_MSE)\n",
    "print(\"Validation error based on TF-IDF decision tree prediction: %.3f\" % val_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look at some random forest models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator: 50, Samples Split: 100\n",
      "-----------------------------------\n",
      "Training Error: 0.27764912169968925\n",
      "Validation Error: 0.682414274687323\n",
      "\n",
      "Estimator: 50, Samples Split: 200\n",
      "-----------------------------------\n",
      "Training Error: 0.3321947147412355\n",
      "Validation Error: 0.6896463357974781\n",
      "\n",
      "Estimator: 50, Samples Split: 500\n",
      "-----------------------------------\n",
      "Training Error: 0.40365306798451744\n",
      "Validation Error: 0.6999955085365609\n",
      "\n",
      "Estimator: 50, Samples Split: 1000\n",
      "-----------------------------------\n",
      "Training Error: 0.5145458898616719\n",
      "Validation Error: 0.7331253142760055\n",
      "\n",
      "Estimator: 50, Samples Split: 2000\n",
      "-----------------------------------\n",
      "Training Error: 0.5958559191822098\n",
      "Validation Error: 0.7449028000432648\n",
      "\n",
      "Estimator: 50, Samples Split: 5000\n",
      "-----------------------------------\n",
      "Training Error: 0.7522311519697088\n",
      "Validation Error: 0.797617220088879\n",
      "\n",
      "Estimator: 100, Samples Split: 100\n",
      "-----------------------------------\n",
      "Training Error: 0.2786782324210541\n",
      "Validation Error: 0.6791026227302835\n",
      "\n",
      "Estimator: 100, Samples Split: 200\n",
      "-----------------------------------\n",
      "Training Error: 0.3290242892419497\n",
      "Validation Error: 0.6862890004129083\n",
      "\n",
      "Estimator: 100, Samples Split: 500\n",
      "-----------------------------------\n",
      "Training Error: 0.4080171925681312\n",
      "Validation Error: 0.693396065659541\n",
      "\n",
      "Estimator: 100, Samples Split: 1000\n",
      "-----------------------------------\n",
      "Training Error: 0.5086373474322122\n",
      "Validation Error: 0.7309392195826754\n",
      "\n",
      "Estimator: 100, Samples Split: 2000\n",
      "-----------------------------------\n",
      "Training Error: 0.5886329609238974\n",
      "Validation Error: 0.7421595490465268\n",
      "\n",
      "Estimator: 100, Samples Split: 5000\n",
      "-----------------------------------\n",
      "Training Error: 0.7482535793721437\n",
      "Validation Error: 0.7921177467100224\n",
      "\n",
      "Estimator: 200, Samples Split: 100\n",
      "-----------------------------------\n",
      "Training Error: 0.2765106940725526\n",
      "Validation Error: 0.6739228207639246\n",
      "\n",
      "Estimator: 200, Samples Split: 200\n",
      "-----------------------------------\n",
      "Training Error: 0.33221945271160774\n",
      "Validation Error: 0.6862215419872086\n",
      "\n",
      "Estimator: 200, Samples Split: 500\n",
      "-----------------------------------\n",
      "Training Error: 0.40568573266255464\n",
      "Validation Error: 0.6951620722630281\n",
      "\n",
      "Estimator: 200, Samples Split: 1000\n",
      "-----------------------------------\n",
      "Training Error: 0.5121441067292635\n",
      "Validation Error: 0.729975914234379\n",
      "\n",
      "Estimator: 200, Samples Split: 2000\n",
      "-----------------------------------\n",
      "Training Error: 0.5865136258848395\n",
      "Validation Error: 0.7443003376591832\n",
      "\n",
      "Estimator: 200, Samples Split: 5000\n",
      "-----------------------------------\n",
      "Training Error: 0.745069205738039\n",
      "Validation Error: 0.7894732703455828\n",
      "\n",
      "Estimator: 500, Samples Split: 100\n",
      "-----------------------------------\n",
      "Training Error: 0.27565754296711786\n",
      "Validation Error: 0.6748907013071259\n",
      "\n",
      "Estimator: 500, Samples Split: 200\n",
      "-----------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-8e857bed1221>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-----------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mforest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_samples_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msamples_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mforest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tfidf_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Error: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalculate_MSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvthreshold_rating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tfidf_reg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation Error: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalculate_MSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvthreshold_rating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_tfidf_reg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n\u001b[0m\u001b[1;32m    387\u001b[0m                              \u001b[0;34m**\u001b[0m\u001b[0m_joblib_parallel_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'threads'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m                 delayed(_parallel_build_trees)(\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1049\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    864\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 866\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    867\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    166\u001b[0m                                                         indices=indices)\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1240\u001b[0m         \"\"\"\n\u001b[1;32m   1241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1242\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m   1243\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    373\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "estimator_lst = [50, 100, 200, 500, 1000]\n",
    "samples_split_lst = [100, 200, 500, 1000, 2000, 5000]\n",
    "for estimators in estimator_lst:\n",
    "    for samples_split in samples_split_lst:\n",
    "        print(\"Estimator: {0}, Samples Split: {1}\".format(estimators, samples_split))\n",
    "        print(\"-----------------------------------\")\n",
    "        forest_model = RandomForestRegressor(n_estimators=estimators, criterion='mse', min_samples_split=samples_split)\n",
    "        forest_model.fit(X_train_tfidf_reg, y_train)\n",
    "        print(\"Training Error: {}\".format(calculate_MSE(y_train, vthreshold_rating(forest_model.predict(X_train_tfidf_reg)))))\n",
    "        print(\"Validation Error: {}\".format(calculate_MSE(y_val, vthreshold_rating(forest_model.predict(X_val_tfidf_reg)))))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_model = RandomForestRegressor(n_estimators=300, criterion=\"mse\", min_samples_split=2000)\n",
    "forest_model.fit(X_train_cv_reg, y_train)\n",
    "\n",
    "train_MSE = calculate_MSE(y_train, vthreshold_rating(forest_model.predict(X_train_cv_reg)))\n",
    "val_MSE = calculate_MSE(y_val, vthreshold_rating(forest_model.predict(X_val_cv_reg)))\n",
    "\n",
    "model_names.append(\"TFIDF-RanFor\")\n",
    "train_errors.append(train_MSE)\n",
    "validation_errors.append(val_MSE)\n",
    "\n",
    "print(\"Training error based on Random Forest Count Vectorizer: %.3f\" % train_MSE)\n",
    "print(\"Validation error based on Random Forest Count Vectorizer: %.3f\" % val_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.01, # Estimators: 10, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 12.647472443937666\n",
      "Validation Error: 12.663965560901493\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 10, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 12.647472443937666\n",
      "Validation Error: 12.663965560901493\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 10, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 12.647472443937666\n",
      "Validation Error: 12.663965560901493\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 10, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 12.647472443937666\n",
      "Validation Error: 12.663965560901493\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 6.568249772343689\n",
      "Validation Error: 6.57867399079515\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 6.553732014268306\n",
      "Validation Error: 6.56926981656117\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 6.520501861267711\n",
      "Validation Error: 6.537234837588246\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 6.48877814590461\n",
      "Validation Error: 6.519742490565927\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 2.996846949754432\n",
      "Validation Error: 3.001324663432204\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 2.970583932395855\n",
      "Validation Error: 2.9809681656634366\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 2.921089815797185\n",
      "Validation Error: 2.9413764214446037\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 2.8788864145373814\n",
      "Validation Error: 2.9207742088425936\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 1.1961030664892458\n",
      "Validation Error: 1.1964846081575515\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 1.1624511409746818\n",
      "Validation Error: 1.1700775819371108\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 1.0975333787400623\n",
      "Validation Error: 1.1250159191325066\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 1.0196615503539956\n",
      "Validation Error: 1.1071038405256337\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8858874696995472\n",
      "Validation Error: 0.8883024696792284\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8480348783929679\n",
      "Validation Error: 0.8606079772841089\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7650547540739293\n",
      "Validation Error: 0.8200923282122435\n",
      "\n",
      "Learning Rate: 0.01, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6538328982177833\n",
      "Validation Error: 0.8052240508139764\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 10, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 9.300685639564719\n",
      "Validation Error: 9.31446294659091\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 10, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 9.291473749146041\n",
      "Validation Error: 9.309354793749737\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 10, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 9.267054304370431\n",
      "Validation Error: 9.28481387279064\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 10, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 9.244398243305781\n",
      "Validation Error: 9.26973983946715\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 1.6596652215968315\n",
      "Validation Error: 1.6612141292278304\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 1.6280221406489404\n",
      "Validation Error: 1.6363141889230624\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 1.5689911476652711\n",
      "Validation Error: 1.5927445014733237\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 1.506476407158294\n",
      "Validation Error: 1.5721484764563676\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.9395090212231147\n",
      "Validation Error: 0.9397103358823248\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.9039707143489066\n",
      "Validation Error: 0.9118301976390338\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8309300713345343\n",
      "Validation Error: 0.8710390888448147\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7360238101304339\n",
      "Validation Error: 0.8530677041977491\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8777152605462512\n",
      "Validation Error: 0.8813104878548743\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8384054287503782\n",
      "Validation Error: 0.853370574293734\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7511740688315276\n",
      "Validation Error: 0.8143848429100338\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6330454592559785\n",
      "Validation Error: 0.8007161589740833\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8415482506719454\n",
      "Validation Error: 0.8536661658640461\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.793070637889461\n",
      "Validation Error: 0.8248266230747211\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6793623365762663\n",
      "Validation Error: 0.7910987597539298\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5250057585677567\n",
      "Validation Error: 0.7886760620711442\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 10, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 2.80570422001846\n",
      "Validation Error: 2.809442961866078\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 10, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 2.7791002325211176\n",
      "Validation Error: 2.7887780641705238\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 10, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 2.729972646863467\n",
      "Validation Error: 2.74921866398562\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 10, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 2.6862617071113375\n",
      "Validation Error: 2.7322042642539053\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8838681714175515\n",
      "Validation Error: 0.8861309041470888\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8456278557005781\n",
      "Validation Error: 0.8586624641289545\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7629058244391043\n",
      "Validation Error: 0.8207570524288279\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6465846936887054\n",
      "Validation Error: 0.805920320498846\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error: 0.8562371391738242\n",
      "Validation Error: 0.8646289936763091\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8122424035463263\n",
      "Validation Error: 0.8363016979402929\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7130391586430229\n",
      "Validation Error: 0.8009448747730243\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5694270929171044\n",
      "Validation Error: 0.7913095545411614\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8286290055102565\n",
      "Validation Error: 0.8435263055403349\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7766458375238707\n",
      "Validation Error: 0.8153344332339758\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6497155343269144\n",
      "Validation Error: 0.7880417397078446\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.4744682101247222\n",
      "Validation Error: 0.7931897327179802\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7911746904066147\n",
      "Validation Error: 0.815860537847195\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 1.0419358925630304\n",
      "Validation Error: 0.7907374455638793\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.897820853921196\n",
      "Validation Error: 0.7807671177710455\n",
      "\n",
      "Learning Rate: 0.1, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7462941847206386\n",
      "Validation Error: 0.8067513823360581\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 10, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.9120715067362042\n",
      "Validation Error: 0.9114504241465038\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 10, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8754859528324639\n",
      "Validation Error: 0.8819957496048141\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 10, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8025755392890344\n",
      "Validation Error: 0.8487888079987187\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 10, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7069688262514805\n",
      "Validation Error: 0.8364955187596234\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8362801211537444\n",
      "Validation Error: 0.8489094558749795\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.78848559421501\n",
      "Validation Error: 0.8220467399215405\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.669322233672213\n",
      "Validation Error: 0.7937185301996843\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.505242484934157\n",
      "Validation Error: 0.804080999376571\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8083925003018094\n",
      "Validation Error: 0.828180683198275\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 1.0596731280881795\n",
      "Validation Error: 0.8043062787530002\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.5931705403215752\n",
      "Validation Error: 0.7828694404961849\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.39868138785658486\n",
      "Validation Error: 0.8104088708913932\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 1.0879260103889523\n",
      "Validation Error: 0.8091301696350843\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 1.026859242366654\n",
      "Validation Error: 0.791145108075924\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8626631192195616\n",
      "Validation Error: 0.7856274977805955\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.2716816364365224\n",
      "Validation Error: 0.8398959713564337\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 1.0592296972000508\n",
      "Validation Error: 0.7892873911242945\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.9601545673381477\n",
      "Validation Error: 0.7805777126460698\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7457874065627772\n",
      "Validation Error: 0.8112712151047892\n",
      "\n",
      "Learning Rate: 0.3, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.11315861111202376\n",
      "Validation Error: 0.9065036964982109\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 10, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8760741523421297\n",
      "Validation Error: 0.8820792152673994\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 10, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8379163165809472\n",
      "Validation Error: 0.851980167100397\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 10, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7587971465792839\n",
      "Validation Error: 0.8303414867948182\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 10, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6489489126602492\n",
      "Validation Error: 0.8261409686395439\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8123589297215374\n",
      "Validation Error: 0.8312023385766004\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 1.0707589002913975\n",
      "Validation Error: 0.8104105132276471\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6189340386647613\n",
      "Validation Error: 0.7950063187894878\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 50, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.42301087374196855\n",
      "Validation Error: 0.8271937812813082\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7838152678194523\n",
      "Validation Error: 0.8095744866952468\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 1.035031040162169\n",
      "Validation Error: 0.7945302364669834\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.8793867984289877\n",
      "Validation Error: 0.8026468682343552\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 100, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7293171164322818\n",
      "Validation Error: 0.8445561897338113\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 1\n",
      "--------------------------------------------------\n",
      "Training Error: 1.0701254275940706\n",
      "Validation Error: 0.7944208232166641\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.9920815912834157\n",
      "Validation Error: 0.7882180933366503\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.7894336754085899\n",
      "Validation Error: 0.815263559470223\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 200, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6170657544659826\n",
      "Validation Error: 0.8782669626684744\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 1\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error: 1.0441530470036742\n",
      "Validation Error: 0.7848066543164833\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 2\n",
      "--------------------------------------------------\n",
      "Training Error: 0.9222095527682757\n",
      "Validation Error: 0.7876996121689137\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 5\n",
      "--------------------------------------------------\n",
      "Training Error: 0.6551374635753199\n",
      "Validation Error: 0.8663779028219787\n",
      "\n",
      "Learning Rate: 0.5, # Estimators: 500, Depth: 10\n",
      "--------------------------------------------------\n",
      "Training Error: 0.513429621183327\n",
      "Validation Error: 1.2770321600405166\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.01, 0.03, 0.1, 0.3, 0.5]\n",
    "estimators = [10, 50, 100, 200, 500]\n",
    "depths = [1, 2, 5, 10]\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for estimator in estimators:\n",
    "        for depth in depths:\n",
    "            print(\"Learning Rate: {0}, # Estimators: {1}, Depth: {2}\".format(learning_rate, estimator, depth))\n",
    "            print(\"--------------------------------------------------\")\n",
    "            xg_reg = XGBRegressor(\n",
    "                learning_rate=learning_rate, max_depth=depth, n_estimators=estimator)\n",
    "            xg_reg.fit(X_train_cv_reg, y_train)\n",
    "            print(\"Training Error: {}\".format(calculate_MSE(y_train, vthreshold_rating(xg_reg.predict(X_train_cv_reg)))))\n",
    "            print(\"Validation Error: {}\".format(calculate_MSE(y_val, vthreshold_rating(xg_reg.predict(X_val_cv_reg)))))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "xg_reg = XGBRegressor(learning_rate=0.03, n_estimators=500, max_depth=10)\n",
    "xg_reg.fit(X_train_cv_reg, y_train)\n",
    "\n",
    "train_MSE = calculate_MSE(y_train, vthreshold_rating(xg_reg.predict(X_train_cv_reg)))\n",
    "val_MSE = calculate_MSE(y_val, vthreshold_rating(xg_reg.predict(X_val_cv_reg)))\n",
    "\n",
    "model_names.append(\"CV-XGB\")\n",
    "train_errors.append(train_MSE)\n",
    "validation_errors.append(val_MSE)\n",
    "\n",
    "print(\"Training error based on XGBoost CountVectorizer prediction: %.3f\" % train_MSE)\n",
    "print(\"Validation error based on XGBoost CountVectorizer prediction: %.3f\" % val_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_item_matrix(df, ratings, user_col, item_col):\n",
    "    return sp.csr_matrix(ratings, (df[user_col], df[item_col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sparse = user_item_matrix(X_train, y_train, 'reviewerID', 'itemID')\n",
    "X_val_sparse = user_item_matrix(X_val, y_val, 'reviewerID', 'itemID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_rating_total = X_train_sparse.sum() / X_train_sparse.count_nonzero()\n",
    "\n",
    "print(\"Average Rating: {}\".format(average_rating_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csc2515-env-3.8",
   "language": "python",
   "name": "csc2515-env-3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
